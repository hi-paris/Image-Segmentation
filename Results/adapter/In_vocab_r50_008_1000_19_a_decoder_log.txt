[10/08 13:44:53] detectron2 INFO: Rank of current process: 0. World size: 2
[10/08 13:44:54] detectron2 INFO: Environment info:
-------------------------------  -------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.8.19 | packaged by conda-forge | (default, Mar 20 2024, 12:47:35) [GCC 12.3.0]
numpy                            1.24.4
detectron2                       0.6 @/home/ids/gbrison/segmentation/segmentation/detectron2/detectron2
Compiler                         GCC 11.4
CUDA compiler                    CUDA 12.5
detectron2 arch flags            8.9
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.0+cu121 @/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0,1                          NVIDIA L40S (arch=8.9)
Driver version                   555.42.06
CUDA_HOME                        /usr/local/cuda
Pillow                           9.4.0
torchvision                      0.19.0+cu121 @/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torchvision
torchvision arch flags           5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.10.0
-------------------------------  -------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX512
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.0, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/08 13:44:54] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco/panoptic-segmentation/fcclip/r50_exp_1000_a_decoder.yaml', dist_url='tcp://127.0.0.1:51163', eval_only=False, machine_rank=0, num_gpus=2, num_machines=1, opts=[], resume=False)
[10/08 13:44:54] detectron2 INFO: Contents of args.config_file=configs/coco/panoptic-segmentation/fcclip/r50_exp_1000_a_decoder.yaml:
_BASE_: ./fcclip_convnext_large_eval_ade20k_r50.yaml

INPUT:
  MIN_SIZE_TEST: 1024
  MAX_SIZE_TEST: 2560

MODEL:
  SEM_SEG_HEAD:
    NUM_CLASSES: 19
  WEIGHTS: /tsi/hi-paris/GB/segmentation/models/fcclip_cocopan_r50.pth
OUTPUT_DIR: /tsi/hi-paris/GB/segmentation/results/In_vocab/r50_008_1000_19_a_decoder
DATASETS:
  TRAIN: ("openvocab_cityscapes_fine_panoptic_train",)
  TEST: ("openvocab_cityscapes_fine_panoptic_val",)
SOLVER:
  IMS_PER_BATCH: 8
  MAX_ITER: 1000
TEST:
  EVAL_PERIOD: 1000


[10/08 13:44:54] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - openvocab_cityscapes_fine_panoptic_val
  TRAIN:
  - openvocab_cityscapes_fine_panoptic_train
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: false
  CROP:
    ENABLED: false
    MINIMUM_INST_AREA: 1
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  DATASET_MAPPER_NAME: coco_panoptic_lsj
  FORMAT: RGB
  IMAGE_SIZE: 1024
  MASK_FORMAT: polygon
  MAX_SCALE: 2.0
  MAX_SIZE_TEST: 2560
  MAX_SIZE_TRAIN: 1333
  MIN_SCALE: 0.1
  MIN_SIZE_TEST: 1024
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: -1
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 0
    NAME: CLIP
  DEVICE: cuda
  FC_CLIP:
    CLIP_MODEL_NAME: RN50
    CLIP_PRETRAINED_WEIGHTS: openai
    EMBED_DIM: 1024
    ENSEMBLE_ON_VALID_MASK: true
    GEOMETRIC_ENSEMBLE_ALPHA: 0.4
    GEOMETRIC_ENSEMBLE_BETA: 0.8
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_FORMER:
    CLASS_WEIGHT: 2.0
    DEC_LAYERS: 10
    DEEP_SUPERVISION: true
    DICE_WEIGHT: 5.0
    DIM_FEEDFORWARD: 2048
    DROPOUT: 0.0
    ENC_LAYERS: 0
    ENFORCE_INPUT_PROJ: false
    HIDDEN_DIM: 256
    IMPORTANCE_SAMPLE_RATIO: 0.75
    MASK_WEIGHT: 5.0
    NHEADS: 8
    NO_OBJECT_WEIGHT: 0.1
    NUM_OBJECT_QUERIES: 250
    OVERSAMPLE_RATIO: 3.0
    PRE_NORM: false
    SIZE_DIVISIBILITY: 32
    TEST:
      INSTANCE_ON: true
      OBJECT_MASK_THRESHOLD: 0.0
      OVERLAP_THRESHOLD: 0.8
      PANOPTIC_ON: true
      SEMANTIC_ON: true
      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: false
    TRAIN_NUM_POINTS: 12544
    TRANSFORMER_DECODER_NAME: MultiScaleMaskedTransformerDecoder
    TRANSFORMER_IN_FEATURE: multi_scale_pixel_decoder
  MASK_ON: false
  META_ARCHITECTURE: FCCLIP
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 122.7709383
  - 116.7460125
  - 104.09373615
  PIXEL_STD:
  - 68.5005327
  - 66.6321579
  - 70.32316305
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 1
    - 1
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: basic
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 256
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES:
    - res3
    - res4
    - res5
    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8
    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    MASK_DIM: 256
    NAME: FCCLIPHead
    NORM: GN
    NUM_CLASSES: 19
    PIXEL_DECODER_NAME: MSDeformAttnPixelDecoder
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    TRANSFORMER_ENC_LAYERS: 6
    USE_DEPTHWISE_SEPARABLE_CONV: false
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    USE_CHECKPOINT: false
    WINDOW_SIZE: 7
  WEIGHTS: /tsi/hi-paris/GB/segmentation/models/fcclip_cocopan_r50.pth
OUTPUT_DIR: /tsi/hi-paris/GB/segmentation/results/In_vocab/r50_008_1000_19_a_decoder
SEED: -1
SOLVER:
  AMP:
    ENABLED: true
  BACKBONE_MULTIPLIER: 0.1
  BASE_LR: 0.0001
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 5000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.01
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 8
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 1000
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 327778
  - 355092
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 1000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[10/08 13:44:54] detectron2 INFO: Full config saved to /tsi/hi-paris/GB/segmentation/results/In_vocab/r50_008_1000_19_a_decoder/config.yaml
[10/08 13:44:54] d2.utils.env INFO: Using a generated random seed 56023563
[10/08 13:44:59] d2.engine.defaults INFO: Model:
FCCLIP(
  (backbone): CLIP(
    (clip_model): CLIP(
      (visual): ModifiedResNet(
        (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
        (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        (layer1): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
            (downsample): Sequential(
              (-1): AvgPool2d(kernel_size=1, stride=1, padding=0)
              (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
        )
        (layer2): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
            (downsample): Sequential(
              (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (3): Bottleneck(
            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
        )
        (layer3): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
            (downsample): Sequential(
              (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
              (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (3): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (4): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (5): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
        )
        (layer4): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
            (downsample): Sequential(
              (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
              (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
        )
        (attnpool): AttentionPool2d(
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (c_proj): Linear(in_features=2048, out_features=1024, bias=True)
        )
      )
      (transformer): Transformer(
        (resblocks): ModuleList(
          (0-11): 12 x ResidualAttentionBlock(
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ls_2): Identity()
          )
        )
      )
      (token_embedding): Embedding(49408, 512)
      (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (decoder_adapter): DecoderAdapter(
      (conv1): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (norm2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (sem_seg_head): FCCLIPHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0-8): 9 x SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0-8): 9 x CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0-8): 9 x FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(250, 256)
      (query_embed): Embedding(250, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0-2): 3 x Sequential()
      )
      (mask_embed): MLP(
        (layers): ModuleList(
          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (mask_pooling): MaskPooling()
      (_mask_pooling_proj): Sequential(
        (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
      )
      (class_embed): MLP(
        (layers): ModuleList(
          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=1024, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 19
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
  (mask_pooling): MaskPooling()
  (void_embedding): Embedding(1, 1024)
)
[10/08 13:44:59] fcclip.data.dataset_mappers.mask_former_semantic_dataset_mapper INFO: [MaskFormerPanopticDatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(800,), max_size=1333, sample_style='choice'), RandomFlip()]
[10/08 13:45:00] fcclip.data.datasets.register_cityscapes_panoptic INFO: 18 cities found in 'datasets/cityscapes/leftImg8bit/train'.
[10/08 13:45:00] d2.data.build INFO: Using training sampler TrainingSampler
[10/08 13:45:00] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/08 13:45:00] d2.data.common INFO: Serializing 2975 elements to byte tensors and concatenating them all ...
[10/08 13:45:00] d2.data.common INFO: Serialized dataset takes 4.12 MiB
[10/08 13:45:00] d2.data.build INFO: Making batched data loader with batch_size=4
[10/08 13:45:00] d2.solver.build WARNING: SOLVER.STEPS contains values larger than SOLVER.MAX_ITER. These values will be ignored.
[10/08 13:45:00] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /tsi/hi-paris/GB/segmentation/models/fcclip_cocopan_r50.pth ...
[10/08 13:45:00] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /tsi/hi-paris/GB/segmentation/models/fcclip_cocopan_r50.pth ...
[10/08 13:45:00] fvcore.common.checkpoint WARNING: Skip loading parameter 'criterion.empty_weight' to the model due to incompatible shapes: (134,) in the checkpoint but (20,) in the model! You might want to double check if this is expected.
[10/08 13:45:00] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.clip_model.ln_final.{bias, weight}[0m
[34mbackbone.clip_model.token_embedding.weight[0m
[34mbackbone.clip_model.transformer.resblocks.0.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.attnpool.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.attnpool.k_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.attnpool.positional_embedding[0m
[34mbackbone.clip_model.visual.attnpool.q_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.attnpool.v_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.conv1.weight[0m
[34mbackbone.clip_model.visual.conv2.weight[0m
[34mbackbone.clip_model.visual.conv3.weight[0m
[34mbackbone.clip_model.visual.layer1.0.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.0.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.0.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.0.conv1.weight[0m
[34mbackbone.clip_model.visual.layer1.0.conv2.weight[0m
[34mbackbone.clip_model.visual.layer1.0.conv3.weight[0m
[34mbackbone.clip_model.visual.layer1.0.downsample.0.weight[0m
[34mbackbone.clip_model.visual.layer1.0.downsample.1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.1.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.1.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.1.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.1.conv1.weight[0m
[34mbackbone.clip_model.visual.layer1.1.conv2.weight[0m
[34mbackbone.clip_model.visual.layer1.1.conv3.weight[0m
[34mbackbone.clip_model.visual.layer1.2.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.2.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.2.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.2.conv1.weight[0m
[34mbackbone.clip_model.visual.layer1.2.conv2.weight[0m
[34mbackbone.clip_model.visual.layer1.2.conv3.weight[0m
[34mbackbone.clip_model.visual.layer2.0.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.0.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.0.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.0.conv1.weight[0m
[34mbackbone.clip_model.visual.layer2.0.conv2.weight[0m
[34mbackbone.clip_model.visual.layer2.0.conv3.weight[0m
[34mbackbone.clip_model.visual.layer2.0.downsample.0.weight[0m
[34mbackbone.clip_model.visual.layer2.0.downsample.1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.1.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.1.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.1.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.1.conv1.weight[0m
[34mbackbone.clip_model.visual.layer2.1.conv2.weight[0m
[34mbackbone.clip_model.visual.layer2.1.conv3.weight[0m
[34mbackbone.clip_model.visual.layer2.2.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.2.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.2.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.2.conv1.weight[0m
[34mbackbone.clip_model.visual.layer2.2.conv2.weight[0m
[34mbackbone.clip_model.visual.layer2.2.conv3.weight[0m
[34mbackbone.clip_model.visual.layer2.3.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.3.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.3.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.3.conv1.weight[0m
[34mbackbone.clip_model.visual.layer2.3.conv2.weight[0m
[34mbackbone.clip_model.visual.layer2.3.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.0.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.0.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.0.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.0.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.0.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.0.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.0.downsample.0.weight[0m
[34mbackbone.clip_model.visual.layer3.0.downsample.1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.1.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.1.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.1.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.1.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.1.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.1.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.2.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.2.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.2.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.2.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.2.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.2.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.3.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.3.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.3.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.3.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.3.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.3.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.4.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.4.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.4.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.4.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.4.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.4.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.5.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.5.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.5.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.5.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.5.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.5.conv3.weight[0m
[34mbackbone.clip_model.visual.layer4.0.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.0.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.0.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.0.conv1.weight[0m
[34mbackbone.clip_model.visual.layer4.0.conv2.weight[0m
[34mbackbone.clip_model.visual.layer4.0.conv3.weight[0m
[34mbackbone.clip_model.visual.layer4.0.downsample.0.weight[0m
[34mbackbone.clip_model.visual.layer4.0.downsample.1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.1.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.1.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.1.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.1.conv1.weight[0m
[34mbackbone.clip_model.visual.layer4.1.conv2.weight[0m
[34mbackbone.clip_model.visual.layer4.1.conv3.weight[0m
[34mbackbone.clip_model.visual.layer4.2.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.2.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.2.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.2.conv1.weight[0m
[34mbackbone.clip_model.visual.layer4.2.conv2.weight[0m
[34mbackbone.clip_model.visual.layer4.2.conv3.weight[0m
[34mbackbone.clip_model.{logit_scale, positional_embedding, text_projection}[0m
[34mbackbone.decoder_adapter.alpha[0m
[34mbackbone.decoder_adapter.conv1.weight[0m
[34mbackbone.decoder_adapter.conv2.weight[0m
[34mbackbone.decoder_adapter.norm1.{bias, running_mean, running_var, weight}[0m
[34mbackbone.decoder_adapter.norm2.{bias, running_mean, running_var, weight}[0m
[34mcriterion.empty_weight[0m
[10/08 13:45:00] d2.engine.train_loop INFO: Starting training from iteration 0
[10/08 13:45:05] d2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/home/ids/gbrison/segmentation/segmentation/detectron2/detectron2/engine/train_loop.py", line 155, in train
    self.run_step()
  File "/home/ids/gbrison/segmentation/segmentation/detectron2/detectron2/engine/defaults.py", line 498, in run_step
    self._trainer.run_step()
  File "/home/ids/gbrison/segmentation/segmentation/detectron2/detectron2/engine/train_loop.py", line 494, in run_step
    loss_dict = self.model(data)
  File "/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1636, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1454, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ids/gbrison/segmentation/segmentation/fc-clip/fcclip/fcclip.py", line 326, in forward
    features = self.backbone(images.tensor)
  File "/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ids/gbrison/segmentation/segmentation/fc-clip/fcclip/modeling/backbone/clip.py", line 151, in forward
    return self.extract_features(x)
  File "/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1729, in __getattr__
    raise AttributeError(f"'{type(self).__name__}' object has no attribute '{name}'")
AttributeError: 'CLIP' object has no attribute 'extract_features'
[10/08 13:45:05] d2.engine.hooks INFO: Total training time: 0:00:04 (0:00:00 on hooks)
[10/08 13:45:05] d2.utils.events INFO:  iter: 0       lr: N/A  max_mem: 946M
[10/08 13:53:35] detectron2 INFO: Rank of current process: 0. World size: 2
[10/08 13:53:36] detectron2 INFO: Environment info:
-------------------------------  -------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.8.19 | packaged by conda-forge | (default, Mar 20 2024, 12:47:35) [GCC 12.3.0]
numpy                            1.24.4
detectron2                       0.6 @/home/ids/gbrison/segmentation/segmentation/detectron2/detectron2
Compiler                         GCC 11.4
CUDA compiler                    CUDA 12.5
detectron2 arch flags            8.9
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.0+cu121 @/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0,1                          NVIDIA L40S (arch=8.9)
Driver version                   555.42.06
CUDA_HOME                        /usr/local/cuda
Pillow                           9.4.0
torchvision                      0.19.0+cu121 @/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torchvision
torchvision arch flags           5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.10.0
-------------------------------  -------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX512
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.0, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/08 13:53:36] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco/panoptic-segmentation/fcclip/r50_exp_1000_a_decoder.yaml', dist_url='tcp://127.0.0.1:51163', eval_only=False, machine_rank=0, num_gpus=2, num_machines=1, opts=[], resume=False)
[10/08 13:53:36] detectron2 INFO: Contents of args.config_file=configs/coco/panoptic-segmentation/fcclip/r50_exp_1000_a_decoder.yaml:
_BASE_: ./fcclip_convnext_large_eval_ade20k_r50.yaml

INPUT:
  MIN_SIZE_TEST: 1024
  MAX_SIZE_TEST: 2560

MODEL:
  SEM_SEG_HEAD:
    NUM_CLASSES: 19
  WEIGHTS: /tsi/hi-paris/GB/segmentation/models/fcclip_cocopan_r50.pth
OUTPUT_DIR: /tsi/hi-paris/GB/segmentation/results/In_vocab/r50_008_1000_19_a_decoder
DATASETS:
  TRAIN: ("openvocab_cityscapes_fine_panoptic_train",)
  TEST: ("openvocab_cityscapes_fine_panoptic_val",)
SOLVER:
  IMS_PER_BATCH: 8
  MAX_ITER: 1000
TEST:
  EVAL_PERIOD: 1000


[10/08 13:53:36] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - openvocab_cityscapes_fine_panoptic_val
  TRAIN:
  - openvocab_cityscapes_fine_panoptic_train
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: false
  CROP:
    ENABLED: false
    MINIMUM_INST_AREA: 1
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  DATASET_MAPPER_NAME: coco_panoptic_lsj
  FORMAT: RGB
  IMAGE_SIZE: 1024
  MASK_FORMAT: polygon
  MAX_SCALE: 2.0
  MAX_SIZE_TEST: 2560
  MAX_SIZE_TRAIN: 1333
  MIN_SCALE: 0.1
  MIN_SIZE_TEST: 1024
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: -1
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 0
    NAME: CLIP
  DEVICE: cuda
  FC_CLIP:
    CLIP_MODEL_NAME: RN50
    CLIP_PRETRAINED_WEIGHTS: openai
    EMBED_DIM: 1024
    ENSEMBLE_ON_VALID_MASK: true
    GEOMETRIC_ENSEMBLE_ALPHA: 0.4
    GEOMETRIC_ENSEMBLE_BETA: 0.8
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_FORMER:
    CLASS_WEIGHT: 2.0
    DEC_LAYERS: 10
    DEEP_SUPERVISION: true
    DICE_WEIGHT: 5.0
    DIM_FEEDFORWARD: 2048
    DROPOUT: 0.0
    ENC_LAYERS: 0
    ENFORCE_INPUT_PROJ: false
    HIDDEN_DIM: 256
    IMPORTANCE_SAMPLE_RATIO: 0.75
    MASK_WEIGHT: 5.0
    NHEADS: 8
    NO_OBJECT_WEIGHT: 0.1
    NUM_OBJECT_QUERIES: 250
    OVERSAMPLE_RATIO: 3.0
    PRE_NORM: false
    SIZE_DIVISIBILITY: 32
    TEST:
      INSTANCE_ON: true
      OBJECT_MASK_THRESHOLD: 0.0
      OVERLAP_THRESHOLD: 0.8
      PANOPTIC_ON: true
      SEMANTIC_ON: true
      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: false
    TRAIN_NUM_POINTS: 12544
    TRANSFORMER_DECODER_NAME: MultiScaleMaskedTransformerDecoder
    TRANSFORMER_IN_FEATURE: multi_scale_pixel_decoder
  MASK_ON: false
  META_ARCHITECTURE: FCCLIP
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 122.7709383
  - 116.7460125
  - 104.09373615
  PIXEL_STD:
  - 68.5005327
  - 66.6321579
  - 70.32316305
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 1
    - 1
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: basic
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 256
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES:
    - res3
    - res4
    - res5
    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8
    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    MASK_DIM: 256
    NAME: FCCLIPHead
    NORM: GN
    NUM_CLASSES: 19
    PIXEL_DECODER_NAME: MSDeformAttnPixelDecoder
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    TRANSFORMER_ENC_LAYERS: 6
    USE_DEPTHWISE_SEPARABLE_CONV: false
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    USE_CHECKPOINT: false
    WINDOW_SIZE: 7
  WEIGHTS: /tsi/hi-paris/GB/segmentation/models/fcclip_cocopan_r50.pth
OUTPUT_DIR: /tsi/hi-paris/GB/segmentation/results/In_vocab/r50_008_1000_19_a_decoder
SEED: -1
SOLVER:
  AMP:
    ENABLED: true
  BACKBONE_MULTIPLIER: 0.1
  BASE_LR: 0.0001
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 5000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.01
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 8
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 1000
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 327778
  - 355092
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 1000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[10/08 13:53:36] detectron2 INFO: Full config saved to /tsi/hi-paris/GB/segmentation/results/In_vocab/r50_008_1000_19_a_decoder/config.yaml
[10/08 13:53:36] d2.utils.env INFO: Using a generated random seed 37554530
[10/08 13:53:40] d2.engine.defaults INFO: Model:
FCCLIP(
  (backbone): CLIP(
    (clip_model): CLIP(
      (visual): ModifiedResNet(
        (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
        (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        (layer1): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
            (downsample): Sequential(
              (-1): AvgPool2d(kernel_size=1, stride=1, padding=0)
              (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
        )
        (layer2): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
            (downsample): Sequential(
              (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (3): Bottleneck(
            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
        )
        (layer3): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
            (downsample): Sequential(
              (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
              (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (3): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (4): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (5): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
        )
        (layer4): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
            (downsample): Sequential(
              (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
              (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
        )
        (attnpool): AttentionPool2d(
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (c_proj): Linear(in_features=2048, out_features=1024, bias=True)
        )
      )
      (transformer): Transformer(
        (resblocks): ModuleList(
          (0-11): 12 x ResidualAttentionBlock(
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ls_2): Identity()
          )
        )
      )
      (token_embedding): Embedding(49408, 512)
      (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (decoder_adapter): DecoderAdapter(
      (conv1): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (norm2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (sem_seg_head): FCCLIPHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0-8): 9 x SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0-8): 9 x CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0-8): 9 x FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(250, 256)
      (query_embed): Embedding(250, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0-2): 3 x Sequential()
      )
      (mask_embed): MLP(
        (layers): ModuleList(
          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (mask_pooling): MaskPooling()
      (_mask_pooling_proj): Sequential(
        (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
      )
      (class_embed): MLP(
        (layers): ModuleList(
          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=1024, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 19
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
  (mask_pooling): MaskPooling()
  (void_embedding): Embedding(1, 1024)
)
[10/08 13:53:40] fcclip.data.dataset_mappers.mask_former_semantic_dataset_mapper INFO: [MaskFormerPanopticDatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(800,), max_size=1333, sample_style='choice'), RandomFlip()]
[10/08 13:53:40] fcclip.data.datasets.register_cityscapes_panoptic INFO: 18 cities found in 'datasets/cityscapes/leftImg8bit/train'.
[10/08 13:53:40] d2.data.build INFO: Using training sampler TrainingSampler
[10/08 13:53:41] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/08 13:53:41] d2.data.common INFO: Serializing 2975 elements to byte tensors and concatenating them all ...
[10/08 13:53:41] d2.data.common INFO: Serialized dataset takes 4.12 MiB
[10/08 13:53:41] d2.data.build INFO: Making batched data loader with batch_size=4
[10/08 13:53:41] d2.solver.build WARNING: SOLVER.STEPS contains values larger than SOLVER.MAX_ITER. These values will be ignored.
[10/08 13:53:41] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /tsi/hi-paris/GB/segmentation/models/fcclip_cocopan_r50.pth ...
[10/08 13:53:41] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /tsi/hi-paris/GB/segmentation/models/fcclip_cocopan_r50.pth ...
[10/08 13:53:41] fvcore.common.checkpoint WARNING: Skip loading parameter 'criterion.empty_weight' to the model due to incompatible shapes: (134,) in the checkpoint but (20,) in the model! You might want to double check if this is expected.
[10/08 13:53:41] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.clip_model.ln_final.{bias, weight}[0m
[34mbackbone.clip_model.token_embedding.weight[0m
[34mbackbone.clip_model.transformer.resblocks.0.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.attnpool.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.attnpool.k_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.attnpool.positional_embedding[0m
[34mbackbone.clip_model.visual.attnpool.q_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.attnpool.v_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.conv1.weight[0m
[34mbackbone.clip_model.visual.conv2.weight[0m
[34mbackbone.clip_model.visual.conv3.weight[0m
[34mbackbone.clip_model.visual.layer1.0.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.0.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.0.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.0.conv1.weight[0m
[34mbackbone.clip_model.visual.layer1.0.conv2.weight[0m
[34mbackbone.clip_model.visual.layer1.0.conv3.weight[0m
[34mbackbone.clip_model.visual.layer1.0.downsample.0.weight[0m
[34mbackbone.clip_model.visual.layer1.0.downsample.1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.1.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.1.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.1.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.1.conv1.weight[0m
[34mbackbone.clip_model.visual.layer1.1.conv2.weight[0m
[34mbackbone.clip_model.visual.layer1.1.conv3.weight[0m
[34mbackbone.clip_model.visual.layer1.2.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.2.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.2.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.2.conv1.weight[0m
[34mbackbone.clip_model.visual.layer1.2.conv2.weight[0m
[34mbackbone.clip_model.visual.layer1.2.conv3.weight[0m
[34mbackbone.clip_model.visual.layer2.0.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.0.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.0.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.0.conv1.weight[0m
[34mbackbone.clip_model.visual.layer2.0.conv2.weight[0m
[34mbackbone.clip_model.visual.layer2.0.conv3.weight[0m
[34mbackbone.clip_model.visual.layer2.0.downsample.0.weight[0m
[34mbackbone.clip_model.visual.layer2.0.downsample.1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.1.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.1.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.1.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.1.conv1.weight[0m
[34mbackbone.clip_model.visual.layer2.1.conv2.weight[0m
[34mbackbone.clip_model.visual.layer2.1.conv3.weight[0m
[34mbackbone.clip_model.visual.layer2.2.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.2.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.2.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.2.conv1.weight[0m
[34mbackbone.clip_model.visual.layer2.2.conv2.weight[0m
[34mbackbone.clip_model.visual.layer2.2.conv3.weight[0m
[34mbackbone.clip_model.visual.layer2.3.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.3.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.3.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.3.conv1.weight[0m
[34mbackbone.clip_model.visual.layer2.3.conv2.weight[0m
[34mbackbone.clip_model.visual.layer2.3.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.0.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.0.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.0.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.0.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.0.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.0.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.0.downsample.0.weight[0m
[34mbackbone.clip_model.visual.layer3.0.downsample.1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.1.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.1.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.1.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.1.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.1.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.1.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.2.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.2.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.2.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.2.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.2.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.2.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.3.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.3.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.3.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.3.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.3.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.3.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.4.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.4.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.4.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.4.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.4.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.4.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.5.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.5.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.5.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.5.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.5.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.5.conv3.weight[0m
[34mbackbone.clip_model.visual.layer4.0.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.0.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.0.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.0.conv1.weight[0m
[34mbackbone.clip_model.visual.layer4.0.conv2.weight[0m
[34mbackbone.clip_model.visual.layer4.0.conv3.weight[0m
[34mbackbone.clip_model.visual.layer4.0.downsample.0.weight[0m
[34mbackbone.clip_model.visual.layer4.0.downsample.1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.1.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.1.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.1.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.1.conv1.weight[0m
[34mbackbone.clip_model.visual.layer4.1.conv2.weight[0m
[34mbackbone.clip_model.visual.layer4.1.conv3.weight[0m
[34mbackbone.clip_model.visual.layer4.2.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.2.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.2.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.2.conv1.weight[0m
[34mbackbone.clip_model.visual.layer4.2.conv2.weight[0m
[34mbackbone.clip_model.visual.layer4.2.conv3.weight[0m
[34mbackbone.clip_model.{logit_scale, positional_embedding, text_projection}[0m
[34mbackbone.decoder_adapter.alpha[0m
[34mbackbone.decoder_adapter.conv1.weight[0m
[34mbackbone.decoder_adapter.conv2.weight[0m
[34mbackbone.decoder_adapter.norm1.{bias, running_mean, running_var, weight}[0m
[34mbackbone.decoder_adapter.norm2.{bias, running_mean, running_var, weight}[0m
[34mcriterion.empty_weight[0m
[10/08 13:53:41] d2.engine.train_loop INFO: Starting training from iteration 0
[10/08 13:53:46] d2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/home/ids/gbrison/segmentation/segmentation/detectron2/detectron2/engine/train_loop.py", line 155, in train
    self.run_step()
  File "/home/ids/gbrison/segmentation/segmentation/detectron2/detectron2/engine/defaults.py", line 498, in run_step
    self._trainer.run_step()
  File "/home/ids/gbrison/segmentation/segmentation/detectron2/detectron2/engine/train_loop.py", line 494, in run_step
    loss_dict = self.model(data)
  File "/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1636, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1454, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ids/gbrison/segmentation/segmentation/fc-clip/fcclip/fcclip.py", line 327, in forward
    text_classifier, num_templates = self.get_text_classifier()
  File "/home/ids/gbrison/segmentation/segmentation/fc-clip/fcclip/fcclip.py", line 193, in get_text_classifier
    text_classifier.append(self.backbone.get_text_classifier(self.train_class_names[idx:idx+bs], self.device).detach())
  File "/home/ids/gbrison/segmentation/segmentation/fc-clip/fcclip/modeling/backbone/clip.py", line 184, in get_text_classifier
    text_tokens = self.tokenize_text(text_list)
  File "/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1729, in __getattr__
    raise AttributeError(f"'{type(self).__name__}' object has no attribute '{name}'")
AttributeError: 'CLIP' object has no attribute 'tokenize_text'
[10/08 13:53:46] d2.engine.hooks INFO: Total training time: 0:00:05 (0:00:00 on hooks)
[10/08 13:53:46] d2.utils.events INFO:  iter: 0       lr: N/A  max_mem: 1133M
[10/08 14:19:48] detectron2 INFO: Rank of current process: 0. World size: 2
[10/08 14:19:50] detectron2 INFO: Environment info:
-------------------------------  -------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.8.19 | packaged by conda-forge | (default, Mar 20 2024, 12:47:35) [GCC 12.3.0]
numpy                            1.24.4
detectron2                       0.6 @/home/ids/gbrison/segmentation/segmentation/detectron2/detectron2
Compiler                         GCC 11.4
CUDA compiler                    CUDA 12.5
detectron2 arch flags            8.9
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.0+cu121 @/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0,1                          NVIDIA L40S (arch=8.9)
Driver version                   555.42.06
CUDA_HOME                        /usr/local/cuda
Pillow                           9.4.0
torchvision                      0.19.0+cu121 @/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torchvision
torchvision arch flags           5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.10.0
-------------------------------  -------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX512
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.0, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/08 14:19:50] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco/panoptic-segmentation/fcclip/r50_exp_1000_a_decoder.yaml', dist_url='tcp://127.0.0.1:51163', eval_only=False, machine_rank=0, num_gpus=2, num_machines=1, opts=[], resume=False)
[10/08 14:19:50] detectron2 INFO: Contents of args.config_file=configs/coco/panoptic-segmentation/fcclip/r50_exp_1000_a_decoder.yaml:
_BASE_: ./fcclip_convnext_large_eval_ade20k_r50.yaml

INPUT:
  MIN_SIZE_TEST: 1024
  MAX_SIZE_TEST: 2560

MODEL:
  SEM_SEG_HEAD:
    NUM_CLASSES: 19
  WEIGHTS: /tsi/hi-paris/GB/segmentation/models/fcclip_cocopan_r50.pth
OUTPUT_DIR: /tsi/hi-paris/GB/segmentation/results/In_vocab/r50_008_1000_19_a_decoder
DATASETS:
  TRAIN: ("openvocab_cityscapes_fine_panoptic_train",)
  TEST: ("openvocab_cityscapes_fine_panoptic_val",)
SOLVER:
  IMS_PER_BATCH: 8
  MAX_ITER: 1000
TEST:
  EVAL_PERIOD: 1000


[10/08 14:19:50] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - openvocab_cityscapes_fine_panoptic_val
  TRAIN:
  - openvocab_cityscapes_fine_panoptic_train
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: false
  CROP:
    ENABLED: false
    MINIMUM_INST_AREA: 1
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  DATASET_MAPPER_NAME: coco_panoptic_lsj
  FORMAT: RGB
  IMAGE_SIZE: 1024
  MASK_FORMAT: polygon
  MAX_SCALE: 2.0
  MAX_SIZE_TEST: 2560
  MAX_SIZE_TRAIN: 1333
  MIN_SCALE: 0.1
  MIN_SIZE_TEST: 1024
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: -1
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 0
    NAME: CLIP
  DEVICE: cuda
  FC_CLIP:
    CLIP_MODEL_NAME: RN50
    CLIP_PRETRAINED_WEIGHTS: openai
    EMBED_DIM: 1024
    ENSEMBLE_ON_VALID_MASK: true
    GEOMETRIC_ENSEMBLE_ALPHA: 0.4
    GEOMETRIC_ENSEMBLE_BETA: 0.8
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_FORMER:
    CLASS_WEIGHT: 2.0
    DEC_LAYERS: 10
    DEEP_SUPERVISION: true
    DICE_WEIGHT: 5.0
    DIM_FEEDFORWARD: 2048
    DROPOUT: 0.0
    ENC_LAYERS: 0
    ENFORCE_INPUT_PROJ: false
    HIDDEN_DIM: 256
    IMPORTANCE_SAMPLE_RATIO: 0.75
    MASK_WEIGHT: 5.0
    NHEADS: 8
    NO_OBJECT_WEIGHT: 0.1
    NUM_OBJECT_QUERIES: 250
    OVERSAMPLE_RATIO: 3.0
    PRE_NORM: false
    SIZE_DIVISIBILITY: 32
    TEST:
      INSTANCE_ON: true
      OBJECT_MASK_THRESHOLD: 0.0
      OVERLAP_THRESHOLD: 0.8
      PANOPTIC_ON: true
      SEMANTIC_ON: true
      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: false
    TRAIN_NUM_POINTS: 12544
    TRANSFORMER_DECODER_NAME: MultiScaleMaskedTransformerDecoder
    TRANSFORMER_IN_FEATURE: multi_scale_pixel_decoder
  MASK_ON: false
  META_ARCHITECTURE: FCCLIP
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 122.7709383
  - 116.7460125
  - 104.09373615
  PIXEL_STD:
  - 68.5005327
  - 66.6321579
  - 70.32316305
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 1
    - 1
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: basic
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 256
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES:
    - res3
    - res4
    - res5
    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8
    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    MASK_DIM: 256
    NAME: FCCLIPHead
    NORM: GN
    NUM_CLASSES: 19
    PIXEL_DECODER_NAME: MSDeformAttnPixelDecoder
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    TRANSFORMER_ENC_LAYERS: 6
    USE_DEPTHWISE_SEPARABLE_CONV: false
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    USE_CHECKPOINT: false
    WINDOW_SIZE: 7
  WEIGHTS: /tsi/hi-paris/GB/segmentation/models/fcclip_cocopan_r50.pth
OUTPUT_DIR: /tsi/hi-paris/GB/segmentation/results/In_vocab/r50_008_1000_19_a_decoder
SEED: -1
SOLVER:
  AMP:
    ENABLED: true
  BACKBONE_MULTIPLIER: 0.1
  BASE_LR: 0.0001
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 5000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.01
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 8
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 1000
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 327778
  - 355092
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 1000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[10/08 14:19:50] detectron2 INFO: Full config saved to /tsi/hi-paris/GB/segmentation/results/In_vocab/r50_008_1000_19_a_decoder/config.yaml
[10/08 14:19:50] d2.utils.env INFO: Using a generated random seed 51320307
[10/08 14:19:54] d2.engine.defaults INFO: Model:
FCCLIP(
  (backbone): CLIP(
    (clip_model): CLIP(
      (visual): ModifiedResNet(
        (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
        (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        (layer1): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
            (downsample): Sequential(
              (-1): AvgPool2d(kernel_size=1, stride=1, padding=0)
              (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
        )
        (layer2): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
            (downsample): Sequential(
              (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (3): Bottleneck(
            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
        )
        (layer3): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
            (downsample): Sequential(
              (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
              (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (3): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (4): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (5): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
        )
        (layer4): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
            (downsample): Sequential(
              (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
              (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
        )
        (attnpool): AttentionPool2d(
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (c_proj): Linear(in_features=2048, out_features=1024, bias=True)
        )
      )
      (transformer): Transformer(
        (resblocks): ModuleList(
          (0-11): 12 x ResidualAttentionBlock(
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ls_2): Identity()
          )
        )
      )
      (token_embedding): Embedding(49408, 512)
      (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
  )
  (sem_seg_head): FCCLIPHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0-8): 9 x SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0-8): 9 x CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0-8): 9 x FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(250, 256)
      (query_embed): Embedding(250, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0-2): 3 x Sequential()
      )
      (mask_embed): MLP(
        (layers): ModuleList(
          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (mask_pooling): MaskPooling()
      (_mask_pooling_proj): Sequential(
        (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
      )
      (class_embed): MLP(
        (layers): ModuleList(
          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=1024, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 19
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
  (mask_pooling): MaskPooling()
  (decoder_adapter): DecoderAdapter(
    (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
    (conv2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    (norm1): LayerNorm((64, 1, 1), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((256, 1, 1), eps=1e-05, elementwise_affine=True)
    (relu): ReLU()
  )
  (void_embedding): Embedding(1, 1024)
)
[10/08 14:19:54] fcclip.data.dataset_mappers.mask_former_semantic_dataset_mapper INFO: [MaskFormerPanopticDatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(800,), max_size=1333, sample_style='choice'), RandomFlip()]
[10/08 14:19:54] fcclip.data.datasets.register_cityscapes_panoptic INFO: 18 cities found in 'datasets/cityscapes/leftImg8bit/train'.
[10/08 14:19:54] d2.data.build INFO: Using training sampler TrainingSampler
[10/08 14:19:55] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/08 14:19:55] d2.data.common INFO: Serializing 2975 elements to byte tensors and concatenating them all ...
[10/08 14:19:55] d2.data.common INFO: Serialized dataset takes 4.12 MiB
[10/08 14:19:55] d2.data.build INFO: Making batched data loader with batch_size=4
[10/08 14:19:55] d2.solver.build WARNING: SOLVER.STEPS contains values larger than SOLVER.MAX_ITER. These values will be ignored.
[10/08 14:19:55] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /tsi/hi-paris/GB/segmentation/models/fcclip_cocopan_r50.pth ...
[10/08 14:19:55] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /tsi/hi-paris/GB/segmentation/models/fcclip_cocopan_r50.pth ...
[10/08 14:19:55] fvcore.common.checkpoint WARNING: Skip loading parameter 'criterion.empty_weight' to the model due to incompatible shapes: (134,) in the checkpoint but (20,) in the model! You might want to double check if this is expected.
[10/08 14:19:55] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.clip_model.ln_final.{bias, weight}[0m
[34mbackbone.clip_model.token_embedding.weight[0m
[34mbackbone.clip_model.transformer.resblocks.0.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.attnpool.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.attnpool.k_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.attnpool.positional_embedding[0m
[34mbackbone.clip_model.visual.attnpool.q_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.attnpool.v_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.conv1.weight[0m
[34mbackbone.clip_model.visual.conv2.weight[0m
[34mbackbone.clip_model.visual.conv3.weight[0m
[34mbackbone.clip_model.visual.layer1.0.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.0.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.0.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.0.conv1.weight[0m
[34mbackbone.clip_model.visual.layer1.0.conv2.weight[0m
[34mbackbone.clip_model.visual.layer1.0.conv3.weight[0m
[34mbackbone.clip_model.visual.layer1.0.downsample.0.weight[0m
[34mbackbone.clip_model.visual.layer1.0.downsample.1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.1.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.1.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.1.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.1.conv1.weight[0m
[34mbackbone.clip_model.visual.layer1.1.conv2.weight[0m
[34mbackbone.clip_model.visual.layer1.1.conv3.weight[0m
[34mbackbone.clip_model.visual.layer1.2.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.2.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.2.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.2.conv1.weight[0m
[34mbackbone.clip_model.visual.layer1.2.conv2.weight[0m
[34mbackbone.clip_model.visual.layer1.2.conv3.weight[0m
[34mbackbone.clip_model.visual.layer2.0.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.0.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.0.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.0.conv1.weight[0m
[34mbackbone.clip_model.visual.layer2.0.conv2.weight[0m
[34mbackbone.clip_model.visual.layer2.0.conv3.weight[0m
[34mbackbone.clip_model.visual.layer2.0.downsample.0.weight[0m
[34mbackbone.clip_model.visual.layer2.0.downsample.1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.1.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.1.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.1.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.1.conv1.weight[0m
[34mbackbone.clip_model.visual.layer2.1.conv2.weight[0m
[34mbackbone.clip_model.visual.layer2.1.conv3.weight[0m
[34mbackbone.clip_model.visual.layer2.2.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.2.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.2.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.2.conv1.weight[0m
[34mbackbone.clip_model.visual.layer2.2.conv2.weight[0m
[34mbackbone.clip_model.visual.layer2.2.conv3.weight[0m
[34mbackbone.clip_model.visual.layer2.3.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.3.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.3.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.3.conv1.weight[0m
[34mbackbone.clip_model.visual.layer2.3.conv2.weight[0m
[34mbackbone.clip_model.visual.layer2.3.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.0.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.0.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.0.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.0.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.0.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.0.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.0.downsample.0.weight[0m
[34mbackbone.clip_model.visual.layer3.0.downsample.1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.1.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.1.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.1.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.1.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.1.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.1.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.2.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.2.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.2.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.2.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.2.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.2.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.3.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.3.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.3.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.3.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.3.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.3.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.4.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.4.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.4.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.4.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.4.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.4.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.5.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.5.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.5.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.5.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.5.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.5.conv3.weight[0m
[34mbackbone.clip_model.visual.layer4.0.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.0.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.0.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.0.conv1.weight[0m
[34mbackbone.clip_model.visual.layer4.0.conv2.weight[0m
[34mbackbone.clip_model.visual.layer4.0.conv3.weight[0m
[34mbackbone.clip_model.visual.layer4.0.downsample.0.weight[0m
[34mbackbone.clip_model.visual.layer4.0.downsample.1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.1.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.1.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.1.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.1.conv1.weight[0m
[34mbackbone.clip_model.visual.layer4.1.conv2.weight[0m
[34mbackbone.clip_model.visual.layer4.1.conv3.weight[0m
[34mbackbone.clip_model.visual.layer4.2.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.2.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.2.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.2.conv1.weight[0m
[34mbackbone.clip_model.visual.layer4.2.conv2.weight[0m
[34mbackbone.clip_model.visual.layer4.2.conv3.weight[0m
[34mbackbone.clip_model.{logit_scale, positional_embedding, text_projection}[0m
[34mcriterion.empty_weight[0m
[34mdecoder_adapter.conv1.{bias, weight}[0m
[34mdecoder_adapter.conv2.{bias, weight}[0m
[34mdecoder_adapter.norm1.{bias, weight}[0m
[34mdecoder_adapter.norm2.{bias, weight}[0m
[10/08 14:19:55] d2.engine.train_loop INFO: Starting training from iteration 0
[10/08 14:20:00] d2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/home/ids/gbrison/segmentation/segmentation/detectron2/detectron2/engine/train_loop.py", line 155, in train
    self.run_step()
  File "/home/ids/gbrison/segmentation/segmentation/detectron2/detectron2/engine/defaults.py", line 498, in run_step
    self._trainer.run_step()
  File "/home/ids/gbrison/segmentation/segmentation/detectron2/detectron2/engine/train_loop.py", line 494, in run_step
    loss_dict = self.model(data)
  File "/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1636, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1454, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ids/gbrison/segmentation/segmentation/fc-clip/fcclip/fcclip.py", line 315, in forward
    features = self.decoder_adapter(features)  # Apply the panoptic decoder adapter here
  File "/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ids/gbrison/segmentation/segmentation/fc-clip/fcclip/fcclip.py", line 60, in forward
    x = self.conv1(x)
  File "/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch/nn/modules/conv.py", line 458, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch/nn/modules/conv.py", line 454, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
TypeError: conv2d() received an invalid combination of arguments - got (dict, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:
 * (Tensor input, Tensor weight, Tensor bias = None, tuple of ints stride = 1, tuple of ints padding = 0, tuple of ints dilation = 1, int groups = 1)
      didn't match because some of the arguments have invalid types: ([31;1mdict[0m, [31;1mParameter[0m, [31;1mParameter[0m, [31;1mtuple of (int, int)[0m, [31;1mtuple of (int, int)[0m, [31;1mtuple of (int, int)[0m, [31;1mint[0m)
 * (Tensor input, Tensor weight, Tensor bias = None, tuple of ints stride = 1, str padding = "valid", tuple of ints dilation = 1, int groups = 1)
      didn't match because some of the arguments have invalid types: ([31;1mdict[0m, [31;1mParameter[0m, [31;1mParameter[0m, [31;1mtuple of (int, int)[0m, [31;1mtuple of (int, int)[0m, [31;1mtuple of (int, int)[0m, [31;1mint[0m)

[10/08 14:20:00] d2.engine.hooks INFO: Total training time: 0:00:05 (0:00:00 on hooks)
[10/08 14:20:00] d2.utils.events INFO:  iter: 0       lr: N/A  max_mem: 1129M
[10/08 14:25:22] detectron2 INFO: Rank of current process: 0. World size: 2
[10/08 14:25:23] detectron2 INFO: Environment info:
-------------------------------  -------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.8.19 | packaged by conda-forge | (default, Mar 20 2024, 12:47:35) [GCC 12.3.0]
numpy                            1.24.4
detectron2                       0.6 @/home/ids/gbrison/segmentation/segmentation/detectron2/detectron2
Compiler                         GCC 11.4
CUDA compiler                    CUDA 12.5
detectron2 arch flags            8.9
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.0+cu121 @/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0,1                          NVIDIA L40S (arch=8.9)
Driver version                   555.42.06
CUDA_HOME                        /usr/local/cuda
Pillow                           9.4.0
torchvision                      0.19.0+cu121 @/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torchvision
torchvision arch flags           5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.10.0
-------------------------------  -------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX512
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.0, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/08 14:25:23] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco/panoptic-segmentation/fcclip/r50_exp_1000_a_decoder.yaml', dist_url='tcp://127.0.0.1:51163', eval_only=False, machine_rank=0, num_gpus=2, num_machines=1, opts=[], resume=False)
[10/08 14:25:23] detectron2 INFO: Contents of args.config_file=configs/coco/panoptic-segmentation/fcclip/r50_exp_1000_a_decoder.yaml:
_BASE_: ./fcclip_convnext_large_eval_ade20k_r50.yaml

INPUT:
  MIN_SIZE_TEST: 1024
  MAX_SIZE_TEST: 2560

MODEL:
  SEM_SEG_HEAD:
    NUM_CLASSES: 19
  WEIGHTS: /tsi/hi-paris/GB/segmentation/models/fcclip_cocopan_r50.pth
OUTPUT_DIR: /tsi/hi-paris/GB/segmentation/results/In_vocab/r50_008_1000_19_a_decoder
DATASETS:
  TRAIN: ("openvocab_cityscapes_fine_panoptic_train",)
  TEST: ("openvocab_cityscapes_fine_panoptic_val",)
SOLVER:
  IMS_PER_BATCH: 8
  MAX_ITER: 1000
TEST:
  EVAL_PERIOD: 1000


[10/08 14:25:23] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - openvocab_cityscapes_fine_panoptic_val
  TRAIN:
  - openvocab_cityscapes_fine_panoptic_train
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: false
  CROP:
    ENABLED: false
    MINIMUM_INST_AREA: 1
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  DATASET_MAPPER_NAME: coco_panoptic_lsj
  FORMAT: RGB
  IMAGE_SIZE: 1024
  MASK_FORMAT: polygon
  MAX_SCALE: 2.0
  MAX_SIZE_TEST: 2560
  MAX_SIZE_TRAIN: 1333
  MIN_SCALE: 0.1
  MIN_SIZE_TEST: 1024
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: -1
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 0
    NAME: CLIP
  DEVICE: cuda
  FC_CLIP:
    CLIP_MODEL_NAME: RN50
    CLIP_PRETRAINED_WEIGHTS: openai
    EMBED_DIM: 1024
    ENSEMBLE_ON_VALID_MASK: true
    GEOMETRIC_ENSEMBLE_ALPHA: 0.4
    GEOMETRIC_ENSEMBLE_BETA: 0.8
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_FORMER:
    CLASS_WEIGHT: 2.0
    DEC_LAYERS: 10
    DEEP_SUPERVISION: true
    DICE_WEIGHT: 5.0
    DIM_FEEDFORWARD: 2048
    DROPOUT: 0.0
    ENC_LAYERS: 0
    ENFORCE_INPUT_PROJ: false
    HIDDEN_DIM: 256
    IMPORTANCE_SAMPLE_RATIO: 0.75
    MASK_WEIGHT: 5.0
    NHEADS: 8
    NO_OBJECT_WEIGHT: 0.1
    NUM_OBJECT_QUERIES: 250
    OVERSAMPLE_RATIO: 3.0
    PRE_NORM: false
    SIZE_DIVISIBILITY: 32
    TEST:
      INSTANCE_ON: true
      OBJECT_MASK_THRESHOLD: 0.0
      OVERLAP_THRESHOLD: 0.8
      PANOPTIC_ON: true
      SEMANTIC_ON: true
      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: false
    TRAIN_NUM_POINTS: 12544
    TRANSFORMER_DECODER_NAME: MultiScaleMaskedTransformerDecoder
    TRANSFORMER_IN_FEATURE: multi_scale_pixel_decoder
  MASK_ON: false
  META_ARCHITECTURE: FCCLIP
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 122.7709383
  - 116.7460125
  - 104.09373615
  PIXEL_STD:
  - 68.5005327
  - 66.6321579
  - 70.32316305
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 1
    - 1
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: basic
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 256
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES:
    - res3
    - res4
    - res5
    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8
    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    MASK_DIM: 256
    NAME: FCCLIPHead
    NORM: GN
    NUM_CLASSES: 19
    PIXEL_DECODER_NAME: MSDeformAttnPixelDecoder
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    TRANSFORMER_ENC_LAYERS: 6
    USE_DEPTHWISE_SEPARABLE_CONV: false
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    USE_CHECKPOINT: false
    WINDOW_SIZE: 7
  WEIGHTS: /tsi/hi-paris/GB/segmentation/models/fcclip_cocopan_r50.pth
OUTPUT_DIR: /tsi/hi-paris/GB/segmentation/results/In_vocab/r50_008_1000_19_a_decoder
SEED: -1
SOLVER:
  AMP:
    ENABLED: true
  BACKBONE_MULTIPLIER: 0.1
  BASE_LR: 0.0001
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 5000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.01
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 8
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 1000
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 327778
  - 355092
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 1000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[10/08 14:25:23] detectron2 INFO: Full config saved to /tsi/hi-paris/GB/segmentation/results/In_vocab/r50_008_1000_19_a_decoder/config.yaml
[10/08 14:25:24] d2.utils.env INFO: Using a generated random seed 25153502
[10/08 14:25:28] d2.engine.defaults INFO: Model:
FCCLIP(
  (backbone): CLIP(
    (clip_model): CLIP(
      (visual): ModifiedResNet(
        (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
        (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        (layer1): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
            (downsample): Sequential(
              (-1): AvgPool2d(kernel_size=1, stride=1, padding=0)
              (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
        )
        (layer2): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
            (downsample): Sequential(
              (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (3): Bottleneck(
            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
        )
        (layer3): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
            (downsample): Sequential(
              (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
              (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (3): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (4): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (5): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
        )
        (layer4): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
            (downsample): Sequential(
              (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
              (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
        )
        (attnpool): AttentionPool2d(
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (c_proj): Linear(in_features=2048, out_features=1024, bias=True)
        )
      )
      (transformer): Transformer(
        (resblocks): ModuleList(
          (0-11): 12 x ResidualAttentionBlock(
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ls_2): Identity()
          )
        )
      )
      (token_embedding): Embedding(49408, 512)
      (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
  )
  (sem_seg_head): FCCLIPHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0-8): 9 x SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0-8): 9 x CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0-8): 9 x FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(250, 256)
      (query_embed): Embedding(250, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0-2): 3 x Sequential()
      )
      (mask_embed): MLP(
        (layers): ModuleList(
          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (mask_pooling): MaskPooling()
      (_mask_pooling_proj): Sequential(
        (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
      )
      (class_embed): MLP(
        (layers): ModuleList(
          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=1024, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 19
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
  (mask_pooling): MaskPooling()
  (decoder_adapter): DecoderAdapter(
    (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
    (conv2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    (norm1): LayerNorm((64, 1, 1), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((256, 1, 1), eps=1e-05, elementwise_affine=True)
    (relu): ReLU()
  )
  (void_embedding): Embedding(1, 1024)
)
[10/08 14:25:28] fcclip.data.dataset_mappers.mask_former_semantic_dataset_mapper INFO: [MaskFormerPanopticDatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(800,), max_size=1333, sample_style='choice'), RandomFlip()]
[10/08 14:25:28] fcclip.data.datasets.register_cityscapes_panoptic INFO: 18 cities found in 'datasets/cityscapes/leftImg8bit/train'.
[10/08 14:25:28] d2.data.build INFO: Using training sampler TrainingSampler
[10/08 14:25:29] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/08 14:25:29] d2.data.common INFO: Serializing 2975 elements to byte tensors and concatenating them all ...
[10/08 14:25:29] d2.data.common INFO: Serialized dataset takes 4.12 MiB
[10/08 14:25:29] d2.data.build INFO: Making batched data loader with batch_size=4
[10/08 14:25:29] d2.solver.build WARNING: SOLVER.STEPS contains values larger than SOLVER.MAX_ITER. These values will be ignored.
[10/08 14:25:29] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /tsi/hi-paris/GB/segmentation/models/fcclip_cocopan_r50.pth ...
[10/08 14:25:29] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /tsi/hi-paris/GB/segmentation/models/fcclip_cocopan_r50.pth ...
[10/08 14:25:29] fvcore.common.checkpoint WARNING: Skip loading parameter 'criterion.empty_weight' to the model due to incompatible shapes: (134,) in the checkpoint but (20,) in the model! You might want to double check if this is expected.
[10/08 14:25:29] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.clip_model.ln_final.{bias, weight}[0m
[34mbackbone.clip_model.token_embedding.weight[0m
[34mbackbone.clip_model.transformer.resblocks.0.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.attnpool.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.attnpool.k_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.attnpool.positional_embedding[0m
[34mbackbone.clip_model.visual.attnpool.q_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.attnpool.v_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.conv1.weight[0m
[34mbackbone.clip_model.visual.conv2.weight[0m
[34mbackbone.clip_model.visual.conv3.weight[0m
[34mbackbone.clip_model.visual.layer1.0.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.0.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.0.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.0.conv1.weight[0m
[34mbackbone.clip_model.visual.layer1.0.conv2.weight[0m
[34mbackbone.clip_model.visual.layer1.0.conv3.weight[0m
[34mbackbone.clip_model.visual.layer1.0.downsample.0.weight[0m
[34mbackbone.clip_model.visual.layer1.0.downsample.1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.1.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.1.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.1.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.1.conv1.weight[0m
[34mbackbone.clip_model.visual.layer1.1.conv2.weight[0m
[34mbackbone.clip_model.visual.layer1.1.conv3.weight[0m
[34mbackbone.clip_model.visual.layer1.2.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.2.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.2.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.2.conv1.weight[0m
[34mbackbone.clip_model.visual.layer1.2.conv2.weight[0m
[34mbackbone.clip_model.visual.layer1.2.conv3.weight[0m
[34mbackbone.clip_model.visual.layer2.0.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.0.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.0.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.0.conv1.weight[0m
[34mbackbone.clip_model.visual.layer2.0.conv2.weight[0m
[34mbackbone.clip_model.visual.layer2.0.conv3.weight[0m
[34mbackbone.clip_model.visual.layer2.0.downsample.0.weight[0m
[34mbackbone.clip_model.visual.layer2.0.downsample.1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.1.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.1.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.1.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.1.conv1.weight[0m
[34mbackbone.clip_model.visual.layer2.1.conv2.weight[0m
[34mbackbone.clip_model.visual.layer2.1.conv3.weight[0m
[34mbackbone.clip_model.visual.layer2.2.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.2.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.2.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.2.conv1.weight[0m
[34mbackbone.clip_model.visual.layer2.2.conv2.weight[0m
[34mbackbone.clip_model.visual.layer2.2.conv3.weight[0m
[34mbackbone.clip_model.visual.layer2.3.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.3.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.3.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.3.conv1.weight[0m
[34mbackbone.clip_model.visual.layer2.3.conv2.weight[0m
[34mbackbone.clip_model.visual.layer2.3.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.0.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.0.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.0.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.0.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.0.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.0.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.0.downsample.0.weight[0m
[34mbackbone.clip_model.visual.layer3.0.downsample.1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.1.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.1.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.1.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.1.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.1.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.1.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.2.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.2.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.2.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.2.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.2.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.2.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.3.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.3.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.3.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.3.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.3.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.3.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.4.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.4.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.4.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.4.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.4.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.4.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.5.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.5.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.5.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.5.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.5.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.5.conv3.weight[0m
[34mbackbone.clip_model.visual.layer4.0.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.0.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.0.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.0.conv1.weight[0m
[34mbackbone.clip_model.visual.layer4.0.conv2.weight[0m
[34mbackbone.clip_model.visual.layer4.0.conv3.weight[0m
[34mbackbone.clip_model.visual.layer4.0.downsample.0.weight[0m
[34mbackbone.clip_model.visual.layer4.0.downsample.1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.1.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.1.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.1.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.1.conv1.weight[0m
[34mbackbone.clip_model.visual.layer4.1.conv2.weight[0m
[34mbackbone.clip_model.visual.layer4.1.conv3.weight[0m
[34mbackbone.clip_model.visual.layer4.2.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.2.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.2.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.2.conv1.weight[0m
[34mbackbone.clip_model.visual.layer4.2.conv2.weight[0m
[34mbackbone.clip_model.visual.layer4.2.conv3.weight[0m
[34mbackbone.clip_model.{logit_scale, positional_embedding, text_projection}[0m
[34mcriterion.empty_weight[0m
[34mdecoder_adapter.conv1.{bias, weight}[0m
[34mdecoder_adapter.conv2.{bias, weight}[0m
[34mdecoder_adapter.norm1.{bias, weight}[0m
[34mdecoder_adapter.norm2.{bias, weight}[0m
[10/08 14:25:29] d2.engine.train_loop INFO: Starting training from iteration 0
[10/08 14:25:34] d2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/home/ids/gbrison/segmentation/segmentation/detectron2/detectron2/engine/train_loop.py", line 155, in train
    self.run_step()
  File "/home/ids/gbrison/segmentation/segmentation/detectron2/detectron2/engine/defaults.py", line 498, in run_step
    self._trainer.run_step()
  File "/home/ids/gbrison/segmentation/segmentation/detectron2/detectron2/engine/train_loop.py", line 494, in run_step
    loss_dict = self.model(data)
  File "/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1636, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1454, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ids/gbrison/segmentation/segmentation/fc-clip/fcclip/fcclip.py", line 355, in forward
    x = self.conv1(x)  # First convolution
  File "/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1729, in __getattr__
    raise AttributeError(f"'{type(self).__name__}' object has no attribute '{name}'")
AttributeError: 'FCCLIP' object has no attribute 'conv1'
[10/08 14:25:34] d2.engine.hooks INFO: Total training time: 0:00:05 (0:00:00 on hooks)
[10/08 14:25:34] d2.utils.events INFO:  iter: 0       lr: N/A  max_mem: 1129M
[10/08 14:28:23] detectron2 INFO: Rank of current process: 0. World size: 2
[10/08 14:28:24] detectron2 INFO: Environment info:
-------------------------------  -------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.8.19 | packaged by conda-forge | (default, Mar 20 2024, 12:47:35) [GCC 12.3.0]
numpy                            1.24.4
detectron2                       0.6 @/home/ids/gbrison/segmentation/segmentation/detectron2/detectron2
Compiler                         GCC 11.4
CUDA compiler                    CUDA 12.5
detectron2 arch flags            8.9
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.0+cu121 @/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0,1                          NVIDIA L40S (arch=8.9)
Driver version                   555.42.06
CUDA_HOME                        /usr/local/cuda
Pillow                           9.4.0
torchvision                      0.19.0+cu121 @/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torchvision
torchvision arch flags           5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.10.0
-------------------------------  -------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX512
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.0, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/08 14:28:24] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco/panoptic-segmentation/fcclip/r50_exp_1000_a_decoder.yaml', dist_url='tcp://127.0.0.1:51163', eval_only=False, machine_rank=0, num_gpus=2, num_machines=1, opts=[], resume=False)
[10/08 14:28:24] detectron2 INFO: Contents of args.config_file=configs/coco/panoptic-segmentation/fcclip/r50_exp_1000_a_decoder.yaml:
_BASE_: ./fcclip_convnext_large_eval_ade20k_r50.yaml

INPUT:
  MIN_SIZE_TEST: 1024
  MAX_SIZE_TEST: 2560

MODEL:
  SEM_SEG_HEAD:
    NUM_CLASSES: 19
  WEIGHTS: /tsi/hi-paris/GB/segmentation/models/fcclip_cocopan_r50.pth
OUTPUT_DIR: /tsi/hi-paris/GB/segmentation/results/In_vocab/r50_008_1000_19_a_decoder
DATASETS:
  TRAIN: ("openvocab_cityscapes_fine_panoptic_train",)
  TEST: ("openvocab_cityscapes_fine_panoptic_val",)
SOLVER:
  IMS_PER_BATCH: 8
  MAX_ITER: 1000
TEST:
  EVAL_PERIOD: 1000


[10/08 14:28:24] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - openvocab_cityscapes_fine_panoptic_val
  TRAIN:
  - openvocab_cityscapes_fine_panoptic_train
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: false
  CROP:
    ENABLED: false
    MINIMUM_INST_AREA: 1
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  DATASET_MAPPER_NAME: coco_panoptic_lsj
  FORMAT: RGB
  IMAGE_SIZE: 1024
  MASK_FORMAT: polygon
  MAX_SCALE: 2.0
  MAX_SIZE_TEST: 2560
  MAX_SIZE_TRAIN: 1333
  MIN_SCALE: 0.1
  MIN_SIZE_TEST: 1024
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: -1
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 0
    NAME: CLIP
  DEVICE: cuda
  FC_CLIP:
    CLIP_MODEL_NAME: RN50
    CLIP_PRETRAINED_WEIGHTS: openai
    EMBED_DIM: 1024
    ENSEMBLE_ON_VALID_MASK: true
    GEOMETRIC_ENSEMBLE_ALPHA: 0.4
    GEOMETRIC_ENSEMBLE_BETA: 0.8
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_FORMER:
    CLASS_WEIGHT: 2.0
    DEC_LAYERS: 10
    DEEP_SUPERVISION: true
    DICE_WEIGHT: 5.0
    DIM_FEEDFORWARD: 2048
    DROPOUT: 0.0
    ENC_LAYERS: 0
    ENFORCE_INPUT_PROJ: false
    HIDDEN_DIM: 256
    IMPORTANCE_SAMPLE_RATIO: 0.75
    MASK_WEIGHT: 5.0
    NHEADS: 8
    NO_OBJECT_WEIGHT: 0.1
    NUM_OBJECT_QUERIES: 250
    OVERSAMPLE_RATIO: 3.0
    PRE_NORM: false
    SIZE_DIVISIBILITY: 32
    TEST:
      INSTANCE_ON: true
      OBJECT_MASK_THRESHOLD: 0.0
      OVERLAP_THRESHOLD: 0.8
      PANOPTIC_ON: true
      SEMANTIC_ON: true
      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: false
    TRAIN_NUM_POINTS: 12544
    TRANSFORMER_DECODER_NAME: MultiScaleMaskedTransformerDecoder
    TRANSFORMER_IN_FEATURE: multi_scale_pixel_decoder
  MASK_ON: false
  META_ARCHITECTURE: FCCLIP
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 122.7709383
  - 116.7460125
  - 104.09373615
  PIXEL_STD:
  - 68.5005327
  - 66.6321579
  - 70.32316305
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 1
    - 1
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: basic
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 256
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES:
    - res3
    - res4
    - res5
    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8
    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    MASK_DIM: 256
    NAME: FCCLIPHead
    NORM: GN
    NUM_CLASSES: 19
    PIXEL_DECODER_NAME: MSDeformAttnPixelDecoder
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    TRANSFORMER_ENC_LAYERS: 6
    USE_DEPTHWISE_SEPARABLE_CONV: false
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    USE_CHECKPOINT: false
    WINDOW_SIZE: 7
  WEIGHTS: /tsi/hi-paris/GB/segmentation/models/fcclip_cocopan_r50.pth
OUTPUT_DIR: /tsi/hi-paris/GB/segmentation/results/In_vocab/r50_008_1000_19_a_decoder
SEED: -1
SOLVER:
  AMP:
    ENABLED: true
  BACKBONE_MULTIPLIER: 0.1
  BASE_LR: 0.0001
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 5000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.01
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 8
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 1000
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 327778
  - 355092
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 1000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[10/08 14:28:24] detectron2 INFO: Full config saved to /tsi/hi-paris/GB/segmentation/results/In_vocab/r50_008_1000_19_a_decoder/config.yaml
[10/08 14:28:24] d2.utils.env INFO: Using a generated random seed 25521539
[10/08 14:40:41] detectron2 INFO: Rank of current process: 0. World size: 2
[10/08 14:40:42] detectron2 INFO: Environment info:
-------------------------------  -------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.8.19 | packaged by conda-forge | (default, Mar 20 2024, 12:47:35) [GCC 12.3.0]
numpy                            1.24.4
detectron2                       0.6 @/home/ids/gbrison/segmentation/segmentation/detectron2/detectron2
Compiler                         GCC 11.4
CUDA compiler                    CUDA 12.5
detectron2 arch flags            8.9
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.0+cu121 @/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0,1                          NVIDIA L40S (arch=8.9)
Driver version                   555.42.06
CUDA_HOME                        /usr/local/cuda
Pillow                           9.4.0
torchvision                      0.19.0+cu121 @/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torchvision
torchvision arch flags           5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.10.0
-------------------------------  -------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX512
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.0, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/08 14:40:42] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco/panoptic-segmentation/fcclip/r50_exp_1000_a_decoder.yaml', dist_url='tcp://127.0.0.1:51163', eval_only=False, machine_rank=0, num_gpus=2, num_machines=1, opts=[], resume=False)
[10/08 14:40:42] detectron2 INFO: Contents of args.config_file=configs/coco/panoptic-segmentation/fcclip/r50_exp_1000_a_decoder.yaml:
_BASE_: ./fcclip_convnext_large_eval_ade20k_r50.yaml

INPUT:
  MIN_SIZE_TEST: 1024
  MAX_SIZE_TEST: 2560

MODEL:
  SEM_SEG_HEAD:
    NUM_CLASSES: 19
  WEIGHTS: /tsi/hi-paris/GB/segmentation/models/fcclip_cocopan_r50.pth
OUTPUT_DIR: /tsi/hi-paris/GB/segmentation/results/In_vocab/r50_008_1000_19_a_decoder
DATASETS:
  TRAIN: ("openvocab_cityscapes_fine_panoptic_train",)
  TEST: ("openvocab_cityscapes_fine_panoptic_val",)
SOLVER:
  IMS_PER_BATCH: 8
  MAX_ITER: 1000
TEST:
  EVAL_PERIOD: 1000


[10/08 14:40:42] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - openvocab_cityscapes_fine_panoptic_val
  TRAIN:
  - openvocab_cityscapes_fine_panoptic_train
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: false
  CROP:
    ENABLED: false
    MINIMUM_INST_AREA: 1
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  DATASET_MAPPER_NAME: coco_panoptic_lsj
  FORMAT: RGB
  IMAGE_SIZE: 1024
  MASK_FORMAT: polygon
  MAX_SCALE: 2.0
  MAX_SIZE_TEST: 2560
  MAX_SIZE_TRAIN: 1333
  MIN_SCALE: 0.1
  MIN_SIZE_TEST: 1024
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: -1
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 0
    NAME: CLIP
  DEVICE: cuda
  FC_CLIP:
    CLIP_MODEL_NAME: RN50
    CLIP_PRETRAINED_WEIGHTS: openai
    EMBED_DIM: 1024
    ENSEMBLE_ON_VALID_MASK: true
    GEOMETRIC_ENSEMBLE_ALPHA: 0.4
    GEOMETRIC_ENSEMBLE_BETA: 0.8
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_FORMER:
    CLASS_WEIGHT: 2.0
    DEC_LAYERS: 10
    DEEP_SUPERVISION: true
    DICE_WEIGHT: 5.0
    DIM_FEEDFORWARD: 2048
    DROPOUT: 0.0
    ENC_LAYERS: 0
    ENFORCE_INPUT_PROJ: false
    HIDDEN_DIM: 256
    IMPORTANCE_SAMPLE_RATIO: 0.75
    MASK_WEIGHT: 5.0
    NHEADS: 8
    NO_OBJECT_WEIGHT: 0.1
    NUM_OBJECT_QUERIES: 250
    OVERSAMPLE_RATIO: 3.0
    PRE_NORM: false
    SIZE_DIVISIBILITY: 32
    TEST:
      INSTANCE_ON: true
      OBJECT_MASK_THRESHOLD: 0.0
      OVERLAP_THRESHOLD: 0.8
      PANOPTIC_ON: true
      SEMANTIC_ON: true
      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: false
    TRAIN_NUM_POINTS: 12544
    TRANSFORMER_DECODER_NAME: MultiScaleMaskedTransformerDecoder
    TRANSFORMER_IN_FEATURE: multi_scale_pixel_decoder
  MASK_ON: false
  META_ARCHITECTURE: FCCLIP
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 122.7709383
  - 116.7460125
  - 104.09373615
  PIXEL_STD:
  - 68.5005327
  - 66.6321579
  - 70.32316305
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 1
    - 1
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: basic
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 256
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES:
    - res3
    - res4
    - res5
    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8
    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    MASK_DIM: 256
    NAME: FCCLIPHead
    NORM: GN
    NUM_CLASSES: 19
    PIXEL_DECODER_NAME: MSDeformAttnPixelDecoder
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    TRANSFORMER_ENC_LAYERS: 6
    USE_DEPTHWISE_SEPARABLE_CONV: false
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    USE_CHECKPOINT: false
    WINDOW_SIZE: 7
  WEIGHTS: /tsi/hi-paris/GB/segmentation/models/fcclip_cocopan_r50.pth
OUTPUT_DIR: /tsi/hi-paris/GB/segmentation/results/In_vocab/r50_008_1000_19_a_decoder
SEED: -1
SOLVER:
  AMP:
    ENABLED: true
  BACKBONE_MULTIPLIER: 0.1
  BASE_LR: 0.0001
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 5000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.01
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 8
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 1000
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 327778
  - 355092
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 1000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[10/08 14:40:42] detectron2 INFO: Full config saved to /tsi/hi-paris/GB/segmentation/results/In_vocab/r50_008_1000_19_a_decoder/config.yaml
[10/08 14:40:42] d2.utils.env INFO: Using a generated random seed 43276816
[10/08 14:40:45] d2.engine.defaults INFO: Model:
FCCLIP(
  (backbone): CLIP(
    (clip_model): CLIP(
      (visual): ModifiedResNet(
        (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
        (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        (layer1): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
            (downsample): Sequential(
              (-1): AvgPool2d(kernel_size=1, stride=1, padding=0)
              (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
        )
        (layer2): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
            (downsample): Sequential(
              (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (3): Bottleneck(
            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
        )
        (layer3): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
            (downsample): Sequential(
              (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
              (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (3): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (4): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (5): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
        )
        (layer4): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
            (downsample): Sequential(
              (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
              (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
        )
        (attnpool): AttentionPool2d(
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (c_proj): Linear(in_features=2048, out_features=1024, bias=True)
        )
      )
      (transformer): Transformer(
        (resblocks): ModuleList(
          (0-11): 12 x ResidualAttentionBlock(
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ls_2): Identity()
          )
        )
      )
      (token_embedding): Embedding(49408, 512)
      (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
  )
  (sem_seg_head): FCCLIPHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0-8): 9 x SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0-8): 9 x CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0-8): 9 x FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(250, 256)
      (query_embed): Embedding(250, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0-2): 3 x Sequential()
      )
      (mask_embed): MLP(
        (layers): ModuleList(
          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (mask_pooling): MaskPooling()
      (_mask_pooling_proj): Sequential(
        (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
      )
      (class_embed): MLP(
        (layers): ModuleList(
          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=1024, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 19
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
  (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu): ReLU()
  (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (mask_pooling): MaskPooling()
  (decoder_adapter): DecoderAdapter(
    (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
    (conv2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    (norm1): LayerNorm((64, 1, 1), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((256, 1, 1), eps=1e-05, elementwise_affine=True)
    (relu): ReLU()
  )
  (void_embedding): Embedding(1, 1024)
)
[10/08 14:40:45] fcclip.data.dataset_mappers.mask_former_semantic_dataset_mapper INFO: [MaskFormerPanopticDatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(800,), max_size=1333, sample_style='choice'), RandomFlip()]
[10/08 14:40:46] fcclip.data.datasets.register_cityscapes_panoptic INFO: 18 cities found in 'datasets/cityscapes/leftImg8bit/train'.
[10/08 14:40:46] d2.data.build INFO: Using training sampler TrainingSampler
[10/08 14:40:47] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/08 14:40:47] d2.data.common INFO: Serializing 2975 elements to byte tensors and concatenating them all ...
[10/08 14:40:47] d2.data.common INFO: Serialized dataset takes 4.12 MiB
[10/08 14:40:47] d2.data.build INFO: Making batched data loader with batch_size=4
[10/08 14:40:47] d2.solver.build WARNING: SOLVER.STEPS contains values larger than SOLVER.MAX_ITER. These values will be ignored.
[10/08 14:40:47] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /tsi/hi-paris/GB/segmentation/models/fcclip_cocopan_r50.pth ...
[10/08 14:40:47] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /tsi/hi-paris/GB/segmentation/models/fcclip_cocopan_r50.pth ...
[10/08 14:40:47] fvcore.common.checkpoint WARNING: Skip loading parameter 'criterion.empty_weight' to the model due to incompatible shapes: (134,) in the checkpoint but (20,) in the model! You might want to double check if this is expected.
[10/08 14:40:47] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.clip_model.ln_final.{bias, weight}[0m
[34mbackbone.clip_model.token_embedding.weight[0m
[34mbackbone.clip_model.transformer.resblocks.0.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.attnpool.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.attnpool.k_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.attnpool.positional_embedding[0m
[34mbackbone.clip_model.visual.attnpool.q_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.attnpool.v_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.conv1.weight[0m
[34mbackbone.clip_model.visual.conv2.weight[0m
[34mbackbone.clip_model.visual.conv3.weight[0m
[34mbackbone.clip_model.visual.layer1.0.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.0.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.0.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.0.conv1.weight[0m
[34mbackbone.clip_model.visual.layer1.0.conv2.weight[0m
[34mbackbone.clip_model.visual.layer1.0.conv3.weight[0m
[34mbackbone.clip_model.visual.layer1.0.downsample.0.weight[0m
[34mbackbone.clip_model.visual.layer1.0.downsample.1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.1.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.1.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.1.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.1.conv1.weight[0m
[34mbackbone.clip_model.visual.layer1.1.conv2.weight[0m
[34mbackbone.clip_model.visual.layer1.1.conv3.weight[0m
[34mbackbone.clip_model.visual.layer1.2.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.2.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.2.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.2.conv1.weight[0m
[34mbackbone.clip_model.visual.layer1.2.conv2.weight[0m
[34mbackbone.clip_model.visual.layer1.2.conv3.weight[0m
[34mbackbone.clip_model.visual.layer2.0.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.0.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.0.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.0.conv1.weight[0m
[34mbackbone.clip_model.visual.layer2.0.conv2.weight[0m
[34mbackbone.clip_model.visual.layer2.0.conv3.weight[0m
[34mbackbone.clip_model.visual.layer2.0.downsample.0.weight[0m
[34mbackbone.clip_model.visual.layer2.0.downsample.1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.1.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.1.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.1.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.1.conv1.weight[0m
[34mbackbone.clip_model.visual.layer2.1.conv2.weight[0m
[34mbackbone.clip_model.visual.layer2.1.conv3.weight[0m
[34mbackbone.clip_model.visual.layer2.2.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.2.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.2.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.2.conv1.weight[0m
[34mbackbone.clip_model.visual.layer2.2.conv2.weight[0m
[34mbackbone.clip_model.visual.layer2.2.conv3.weight[0m
[34mbackbone.clip_model.visual.layer2.3.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.3.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.3.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.3.conv1.weight[0m
[34mbackbone.clip_model.visual.layer2.3.conv2.weight[0m
[34mbackbone.clip_model.visual.layer2.3.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.0.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.0.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.0.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.0.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.0.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.0.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.0.downsample.0.weight[0m
[34mbackbone.clip_model.visual.layer3.0.downsample.1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.1.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.1.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.1.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.1.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.1.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.1.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.2.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.2.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.2.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.2.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.2.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.2.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.3.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.3.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.3.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.3.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.3.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.3.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.4.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.4.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.4.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.4.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.4.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.4.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.5.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.5.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.5.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.5.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.5.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.5.conv3.weight[0m
[34mbackbone.clip_model.visual.layer4.0.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.0.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.0.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.0.conv1.weight[0m
[34mbackbone.clip_model.visual.layer4.0.conv2.weight[0m
[34mbackbone.clip_model.visual.layer4.0.conv3.weight[0m
[34mbackbone.clip_model.visual.layer4.0.downsample.0.weight[0m
[34mbackbone.clip_model.visual.layer4.0.downsample.1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.1.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.1.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.1.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.1.conv1.weight[0m
[34mbackbone.clip_model.visual.layer4.1.conv2.weight[0m
[34mbackbone.clip_model.visual.layer4.1.conv3.weight[0m
[34mbackbone.clip_model.visual.layer4.2.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.2.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.2.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.2.conv1.weight[0m
[34mbackbone.clip_model.visual.layer4.2.conv2.weight[0m
[34mbackbone.clip_model.visual.layer4.2.conv3.weight[0m
[34mbackbone.clip_model.{logit_scale, positional_embedding, text_projection}[0m
[34mbn1.{bias, running_mean, running_var, weight}[0m
[34mbn2.{bias, running_mean, running_var, weight}[0m
[34mconv1.{bias, weight}[0m
[34mconv2.{bias, weight}[0m
[34mcriterion.empty_weight[0m
[34mdecoder_adapter.conv1.{bias, weight}[0m
[34mdecoder_adapter.conv2.{bias, weight}[0m
[34mdecoder_adapter.norm1.{bias, weight}[0m
[34mdecoder_adapter.norm2.{bias, weight}[0m
[10/08 14:40:47] d2.engine.train_loop INFO: Starting training from iteration 0
[10/08 14:40:52] d2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/home/ids/gbrison/segmentation/segmentation/detectron2/detectron2/engine/train_loop.py", line 155, in train
    self.run_step()
  File "/home/ids/gbrison/segmentation/segmentation/detectron2/detectron2/engine/defaults.py", line 498, in run_step
    self._trainer.run_step()
  File "/home/ids/gbrison/segmentation/segmentation/detectron2/detectron2/engine/train_loop.py", line 494, in run_step
    loss_dict = self.model(data)
  File "/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1636, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1454, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ids/gbrison/segmentation/segmentation/fc-clip/fcclip/fcclip.py", line 315, in forward
    x = self.conv1(x)
  File "/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch/nn/modules/conv.py", line 458, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch/nn/modules/conv.py", line 454, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: Given groups=1, weight of size [512, 256, 3, 3], expected input[4, 1024, 42, 84] to have 256 channels, but got 1024 channels instead
[10/08 14:40:52] d2.engine.hooks INFO: Total training time: 0:00:04 (0:00:00 on hooks)
[10/08 14:40:52] d2.utils.events INFO:  iter: 0       lr: N/A  max_mem: 1155M
[10/08 14:43:25] detectron2 INFO: Rank of current process: 0. World size: 2
[10/08 14:43:26] detectron2 INFO: Environment info:
-------------------------------  -------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.8.19 | packaged by conda-forge | (default, Mar 20 2024, 12:47:35) [GCC 12.3.0]
numpy                            1.24.4
detectron2                       0.6 @/home/ids/gbrison/segmentation/segmentation/detectron2/detectron2
Compiler                         GCC 11.4
CUDA compiler                    CUDA 12.5
detectron2 arch flags            8.9
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.0+cu121 @/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0,1                          NVIDIA L40S (arch=8.9)
Driver version                   555.42.06
CUDA_HOME                        /usr/local/cuda
Pillow                           9.4.0
torchvision                      0.19.0+cu121 @/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torchvision
torchvision arch flags           5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.10.0
-------------------------------  -------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX512
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.0, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/08 14:43:26] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco/panoptic-segmentation/fcclip/r50_exp_1000_a_decoder.yaml', dist_url='tcp://127.0.0.1:51163', eval_only=False, machine_rank=0, num_gpus=2, num_machines=1, opts=[], resume=False)
[10/08 14:43:26] detectron2 INFO: Contents of args.config_file=configs/coco/panoptic-segmentation/fcclip/r50_exp_1000_a_decoder.yaml:
_BASE_: ./fcclip_convnext_large_eval_ade20k_r50.yaml

INPUT:
  MIN_SIZE_TEST: 1024
  MAX_SIZE_TEST: 2560

MODEL:
  SEM_SEG_HEAD:
    NUM_CLASSES: 19
  WEIGHTS: /tsi/hi-paris/GB/segmentation/models/fcclip_cocopan_r50.pth
OUTPUT_DIR: /tsi/hi-paris/GB/segmentation/results/In_vocab/r50_008_1000_19_a_decoder
DATASETS:
  TRAIN: ("openvocab_cityscapes_fine_panoptic_train",)
  TEST: ("openvocab_cityscapes_fine_panoptic_val",)
SOLVER:
  IMS_PER_BATCH: 8
  MAX_ITER: 1000
TEST:
  EVAL_PERIOD: 1000


[10/08 14:43:26] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - openvocab_cityscapes_fine_panoptic_val
  TRAIN:
  - openvocab_cityscapes_fine_panoptic_train
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: false
  CROP:
    ENABLED: false
    MINIMUM_INST_AREA: 1
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  DATASET_MAPPER_NAME: coco_panoptic_lsj
  FORMAT: RGB
  IMAGE_SIZE: 1024
  MASK_FORMAT: polygon
  MAX_SCALE: 2.0
  MAX_SIZE_TEST: 2560
  MAX_SIZE_TRAIN: 1333
  MIN_SCALE: 0.1
  MIN_SIZE_TEST: 1024
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: -1
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 0
    NAME: CLIP
  DEVICE: cuda
  FC_CLIP:
    CLIP_MODEL_NAME: RN50
    CLIP_PRETRAINED_WEIGHTS: openai
    EMBED_DIM: 1024
    ENSEMBLE_ON_VALID_MASK: true
    GEOMETRIC_ENSEMBLE_ALPHA: 0.4
    GEOMETRIC_ENSEMBLE_BETA: 0.8
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_FORMER:
    CLASS_WEIGHT: 2.0
    DEC_LAYERS: 10
    DEEP_SUPERVISION: true
    DICE_WEIGHT: 5.0
    DIM_FEEDFORWARD: 2048
    DROPOUT: 0.0
    ENC_LAYERS: 0
    ENFORCE_INPUT_PROJ: false
    HIDDEN_DIM: 256
    IMPORTANCE_SAMPLE_RATIO: 0.75
    MASK_WEIGHT: 5.0
    NHEADS: 8
    NO_OBJECT_WEIGHT: 0.1
    NUM_OBJECT_QUERIES: 250
    OVERSAMPLE_RATIO: 3.0
    PRE_NORM: false
    SIZE_DIVISIBILITY: 32
    TEST:
      INSTANCE_ON: true
      OBJECT_MASK_THRESHOLD: 0.0
      OVERLAP_THRESHOLD: 0.8
      PANOPTIC_ON: true
      SEMANTIC_ON: true
      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: false
    TRAIN_NUM_POINTS: 12544
    TRANSFORMER_DECODER_NAME: MultiScaleMaskedTransformerDecoder
    TRANSFORMER_IN_FEATURE: multi_scale_pixel_decoder
  MASK_ON: false
  META_ARCHITECTURE: FCCLIP
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 122.7709383
  - 116.7460125
  - 104.09373615
  PIXEL_STD:
  - 68.5005327
  - 66.6321579
  - 70.32316305
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 1
    - 1
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: basic
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 256
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES:
    - res3
    - res4
    - res5
    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8
    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    MASK_DIM: 256
    NAME: FCCLIPHead
    NORM: GN
    NUM_CLASSES: 19
    PIXEL_DECODER_NAME: MSDeformAttnPixelDecoder
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    TRANSFORMER_ENC_LAYERS: 6
    USE_DEPTHWISE_SEPARABLE_CONV: false
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    USE_CHECKPOINT: false
    WINDOW_SIZE: 7
  WEIGHTS: /tsi/hi-paris/GB/segmentation/models/fcclip_cocopan_r50.pth
OUTPUT_DIR: /tsi/hi-paris/GB/segmentation/results/In_vocab/r50_008_1000_19_a_decoder
SEED: -1
SOLVER:
  AMP:
    ENABLED: true
  BACKBONE_MULTIPLIER: 0.1
  BASE_LR: 0.0001
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 5000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.01
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 8
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 1000
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 327778
  - 355092
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 1000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[10/08 14:43:26] detectron2 INFO: Full config saved to /tsi/hi-paris/GB/segmentation/results/In_vocab/r50_008_1000_19_a_decoder/config.yaml
[10/08 14:43:26] d2.utils.env INFO: Using a generated random seed 27871222
[10/08 14:43:31] d2.engine.defaults INFO: Model:
FCCLIP(
  (backbone): CLIP(
    (clip_model): CLIP(
      (visual): ModifiedResNet(
        (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
        (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        (layer1): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
            (downsample): Sequential(
              (-1): AvgPool2d(kernel_size=1, stride=1, padding=0)
              (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
        )
        (layer2): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
            (downsample): Sequential(
              (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (3): Bottleneck(
            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
        )
        (layer3): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
            (downsample): Sequential(
              (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
              (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (3): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (4): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (5): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
        )
        (layer4): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
            (downsample): Sequential(
              (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
              (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
        )
        (attnpool): AttentionPool2d(
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (c_proj): Linear(in_features=2048, out_features=1024, bias=True)
        )
      )
      (transformer): Transformer(
        (resblocks): ModuleList(
          (0-11): 12 x ResidualAttentionBlock(
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ls_2): Identity()
          )
        )
      )
      (token_embedding): Embedding(49408, 512)
      (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
  )
  (sem_seg_head): FCCLIPHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0-8): 9 x SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0-8): 9 x CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0-8): 9 x FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(250, 256)
      (query_embed): Embedding(250, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0-2): 3 x Sequential()
      )
      (mask_embed): MLP(
        (layers): ModuleList(
          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (mask_pooling): MaskPooling()
      (_mask_pooling_proj): Sequential(
        (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
      )
      (class_embed): MLP(
        (layers): ModuleList(
          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=1024, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 19
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
  (conv1): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv2): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU()
  (mask_pooling): MaskPooling()
  (decoder_adapter): DecoderAdapter(
    (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
    (conv2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    (norm1): LayerNorm((64, 1, 1), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((256, 1, 1), eps=1e-05, elementwise_affine=True)
    (relu): ReLU()
  )
  (void_embedding): Embedding(1, 1024)
)
[10/08 14:43:31] fcclip.data.dataset_mappers.mask_former_semantic_dataset_mapper INFO: [MaskFormerPanopticDatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(800,), max_size=1333, sample_style='choice'), RandomFlip()]
[10/08 14:43:31] fcclip.data.datasets.register_cityscapes_panoptic INFO: 18 cities found in 'datasets/cityscapes/leftImg8bit/train'.
[10/08 14:43:31] d2.data.build INFO: Using training sampler TrainingSampler
[10/08 14:43:32] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/08 14:43:32] d2.data.common INFO: Serializing 2975 elements to byte tensors and concatenating them all ...
[10/08 14:43:32] d2.data.common INFO: Serialized dataset takes 4.12 MiB
[10/08 14:43:32] d2.data.build INFO: Making batched data loader with batch_size=4
[10/08 14:43:32] d2.solver.build WARNING: SOLVER.STEPS contains values larger than SOLVER.MAX_ITER. These values will be ignored.
[10/08 14:43:32] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /tsi/hi-paris/GB/segmentation/models/fcclip_cocopan_r50.pth ...
[10/08 14:43:32] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /tsi/hi-paris/GB/segmentation/models/fcclip_cocopan_r50.pth ...
[10/08 14:43:32] fvcore.common.checkpoint WARNING: Skip loading parameter 'criterion.empty_weight' to the model due to incompatible shapes: (134,) in the checkpoint but (20,) in the model! You might want to double check if this is expected.
[10/08 14:43:32] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.clip_model.ln_final.{bias, weight}[0m
[34mbackbone.clip_model.token_embedding.weight[0m
[34mbackbone.clip_model.transformer.resblocks.0.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.attnpool.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.attnpool.k_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.attnpool.positional_embedding[0m
[34mbackbone.clip_model.visual.attnpool.q_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.attnpool.v_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.conv1.weight[0m
[34mbackbone.clip_model.visual.conv2.weight[0m
[34mbackbone.clip_model.visual.conv3.weight[0m
[34mbackbone.clip_model.visual.layer1.0.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.0.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.0.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.0.conv1.weight[0m
[34mbackbone.clip_model.visual.layer1.0.conv2.weight[0m
[34mbackbone.clip_model.visual.layer1.0.conv3.weight[0m
[34mbackbone.clip_model.visual.layer1.0.downsample.0.weight[0m
[34mbackbone.clip_model.visual.layer1.0.downsample.1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.1.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.1.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.1.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.1.conv1.weight[0m
[34mbackbone.clip_model.visual.layer1.1.conv2.weight[0m
[34mbackbone.clip_model.visual.layer1.1.conv3.weight[0m
[34mbackbone.clip_model.visual.layer1.2.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.2.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.2.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.2.conv1.weight[0m
[34mbackbone.clip_model.visual.layer1.2.conv2.weight[0m
[34mbackbone.clip_model.visual.layer1.2.conv3.weight[0m
[34mbackbone.clip_model.visual.layer2.0.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.0.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.0.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.0.conv1.weight[0m
[34mbackbone.clip_model.visual.layer2.0.conv2.weight[0m
[34mbackbone.clip_model.visual.layer2.0.conv3.weight[0m
[34mbackbone.clip_model.visual.layer2.0.downsample.0.weight[0m
[34mbackbone.clip_model.visual.layer2.0.downsample.1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.1.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.1.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.1.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.1.conv1.weight[0m
[34mbackbone.clip_model.visual.layer2.1.conv2.weight[0m
[34mbackbone.clip_model.visual.layer2.1.conv3.weight[0m
[34mbackbone.clip_model.visual.layer2.2.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.2.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.2.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.2.conv1.weight[0m
[34mbackbone.clip_model.visual.layer2.2.conv2.weight[0m
[34mbackbone.clip_model.visual.layer2.2.conv3.weight[0m
[34mbackbone.clip_model.visual.layer2.3.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.3.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.3.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.3.conv1.weight[0m
[34mbackbone.clip_model.visual.layer2.3.conv2.weight[0m
[34mbackbone.clip_model.visual.layer2.3.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.0.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.0.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.0.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.0.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.0.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.0.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.0.downsample.0.weight[0m
[34mbackbone.clip_model.visual.layer3.0.downsample.1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.1.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.1.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.1.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.1.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.1.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.1.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.2.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.2.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.2.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.2.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.2.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.2.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.3.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.3.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.3.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.3.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.3.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.3.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.4.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.4.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.4.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.4.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.4.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.4.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.5.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.5.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.5.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.5.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.5.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.5.conv3.weight[0m
[34mbackbone.clip_model.visual.layer4.0.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.0.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.0.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.0.conv1.weight[0m
[34mbackbone.clip_model.visual.layer4.0.conv2.weight[0m
[34mbackbone.clip_model.visual.layer4.0.conv3.weight[0m
[34mbackbone.clip_model.visual.layer4.0.downsample.0.weight[0m
[34mbackbone.clip_model.visual.layer4.0.downsample.1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.1.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.1.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.1.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.1.conv1.weight[0m
[34mbackbone.clip_model.visual.layer4.1.conv2.weight[0m
[34mbackbone.clip_model.visual.layer4.1.conv3.weight[0m
[34mbackbone.clip_model.visual.layer4.2.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.2.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.2.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.2.conv1.weight[0m
[34mbackbone.clip_model.visual.layer4.2.conv2.weight[0m
[34mbackbone.clip_model.visual.layer4.2.conv3.weight[0m
[34mbackbone.clip_model.{logit_scale, positional_embedding, text_projection}[0m
[34mbn1.{bias, running_mean, running_var, weight}[0m
[34mbn2.{bias, running_mean, running_var, weight}[0m
[34mconv1.{bias, weight}[0m
[34mconv2.{bias, weight}[0m
[34mcriterion.empty_weight[0m
[34mdecoder_adapter.conv1.{bias, weight}[0m
[34mdecoder_adapter.conv2.{bias, weight}[0m
[34mdecoder_adapter.norm1.{bias, weight}[0m
[34mdecoder_adapter.norm2.{bias, weight}[0m
[10/08 14:43:32] d2.engine.train_loop INFO: Starting training from iteration 0
[10/08 14:43:40] d2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/home/ids/gbrison/segmentation/segmentation/detectron2/detectron2/engine/train_loop.py", line 155, in train
    self.run_step()
  File "/home/ids/gbrison/segmentation/segmentation/detectron2/detectron2/engine/defaults.py", line 498, in run_step
    self._trainer.run_step()
  File "/home/ids/gbrison/segmentation/segmentation/detectron2/detectron2/engine/train_loop.py", line 494, in run_step
    loss_dict = self.model(data)
  File "/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1632, in forward
    inputs, kwargs = self._pre_forward(*inputs, **kwargs)
  File "/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1523, in _pre_forward
    if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameter indices which did not receive grad for rank 0: 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316
 In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[10/08 14:43:40] d2.engine.hooks INFO: Total training time: 0:00:00 (0:00:00 on hooks)
[10/08 14:43:40] d2.utils.events INFO:  iter: 1  total_loss: 38.28  loss_ce: 2.121  loss_mask: 0.2801  loss_dice: 1.424  loss_ce_0: 1.987  loss_mask_0: 0.2986  loss_dice_0: 1.755  loss_ce_1: 2.051  loss_mask_1: 0.2985  loss_dice_1: 1.538  loss_ce_2: 2.04  loss_mask_2: 0.2847  loss_dice_2: 1.529  loss_ce_3: 2.029  loss_mask_3: 0.2871  loss_dice_3: 1.505  loss_ce_4: 2.024  loss_mask_4: 0.2867  loss_dice_4: 1.424  loss_ce_5: 2.058  loss_mask_5: 0.2834  loss_dice_5: 1.393  loss_ce_6: 2.171  loss_mask_6: 0.2734  loss_dice_6: 1.416  loss_ce_7: 2.037  loss_mask_7: 0.2902  loss_dice_7: 1.417  loss_ce_8: 2.094  loss_mask_8: 0.2745  loss_dice_8: 1.406    data_time: 5.1036  last_data_time: 5.1036   lr: 0.0001  max_mem: 15527M
[10/08 14:45:38] detectron2 INFO: Rank of current process: 0. World size: 2
[10/08 14:45:39] detectron2 INFO: Environment info:
-------------------------------  -------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.8.19 | packaged by conda-forge | (default, Mar 20 2024, 12:47:35) [GCC 12.3.0]
numpy                            1.24.4
detectron2                       0.6 @/home/ids/gbrison/segmentation/segmentation/detectron2/detectron2
Compiler                         GCC 11.4
CUDA compiler                    CUDA 12.5
detectron2 arch flags            8.9
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.0+cu121 @/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0,1                          NVIDIA L40S (arch=8.9)
Driver version                   555.42.06
CUDA_HOME                        /usr/local/cuda
Pillow                           9.4.0
torchvision                      0.19.0+cu121 @/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torchvision
torchvision arch flags           5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.10.0
-------------------------------  -------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX512
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.0, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/08 14:45:39] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco/panoptic-segmentation/fcclip/r50_exp_1000_a_decoder.yaml', dist_url='tcp://127.0.0.1:51163', eval_only=False, machine_rank=0, num_gpus=2, num_machines=1, opts=[], resume=False)
[10/08 14:45:39] detectron2 INFO: Contents of args.config_file=configs/coco/panoptic-segmentation/fcclip/r50_exp_1000_a_decoder.yaml:
_BASE_: ./fcclip_convnext_large_eval_ade20k_r50.yaml

INPUT:
  MIN_SIZE_TEST: 1024
  MAX_SIZE_TEST: 2560

MODEL:
  SEM_SEG_HEAD:
    NUM_CLASSES: 19
  WEIGHTS: /tsi/hi-paris/GB/segmentation/models/fcclip_cocopan_r50.pth
OUTPUT_DIR: /tsi/hi-paris/GB/segmentation/results/In_vocab/r50_008_1000_19_a_decoder
DATASETS:
  TRAIN: ("openvocab_cityscapes_fine_panoptic_train",)
  TEST: ("openvocab_cityscapes_fine_panoptic_val",)
SOLVER:
  IMS_PER_BATCH: 8
  MAX_ITER: 1000
TEST:
  EVAL_PERIOD: 1000


[10/08 14:45:39] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - openvocab_cityscapes_fine_panoptic_val
  TRAIN:
  - openvocab_cityscapes_fine_panoptic_train
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: false
  CROP:
    ENABLED: false
    MINIMUM_INST_AREA: 1
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  DATASET_MAPPER_NAME: coco_panoptic_lsj
  FORMAT: RGB
  IMAGE_SIZE: 1024
  MASK_FORMAT: polygon
  MAX_SCALE: 2.0
  MAX_SIZE_TEST: 2560
  MAX_SIZE_TRAIN: 1333
  MIN_SCALE: 0.1
  MIN_SIZE_TEST: 1024
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: -1
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 0
    NAME: CLIP
  DEVICE: cuda
  FC_CLIP:
    CLIP_MODEL_NAME: RN50
    CLIP_PRETRAINED_WEIGHTS: openai
    EMBED_DIM: 1024
    ENSEMBLE_ON_VALID_MASK: true
    GEOMETRIC_ENSEMBLE_ALPHA: 0.4
    GEOMETRIC_ENSEMBLE_BETA: 0.8
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_FORMER:
    CLASS_WEIGHT: 2.0
    DEC_LAYERS: 10
    DEEP_SUPERVISION: true
    DICE_WEIGHT: 5.0
    DIM_FEEDFORWARD: 2048
    DROPOUT: 0.0
    ENC_LAYERS: 0
    ENFORCE_INPUT_PROJ: false
    HIDDEN_DIM: 256
    IMPORTANCE_SAMPLE_RATIO: 0.75
    MASK_WEIGHT: 5.0
    NHEADS: 8
    NO_OBJECT_WEIGHT: 0.1
    NUM_OBJECT_QUERIES: 250
    OVERSAMPLE_RATIO: 3.0
    PRE_NORM: false
    SIZE_DIVISIBILITY: 32
    TEST:
      INSTANCE_ON: true
      OBJECT_MASK_THRESHOLD: 0.0
      OVERLAP_THRESHOLD: 0.8
      PANOPTIC_ON: true
      SEMANTIC_ON: true
      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: false
    TRAIN_NUM_POINTS: 12544
    TRANSFORMER_DECODER_NAME: MultiScaleMaskedTransformerDecoder
    TRANSFORMER_IN_FEATURE: multi_scale_pixel_decoder
  MASK_ON: false
  META_ARCHITECTURE: FCCLIP
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 122.7709383
  - 116.7460125
  - 104.09373615
  PIXEL_STD:
  - 68.5005327
  - 66.6321579
  - 70.32316305
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 1
    - 1
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: basic
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 256
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES:
    - res3
    - res4
    - res5
    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8
    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    MASK_DIM: 256
    NAME: FCCLIPHead
    NORM: GN
    NUM_CLASSES: 19
    PIXEL_DECODER_NAME: MSDeformAttnPixelDecoder
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    TRANSFORMER_ENC_LAYERS: 6
    USE_DEPTHWISE_SEPARABLE_CONV: false
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    USE_CHECKPOINT: false
    WINDOW_SIZE: 7
  WEIGHTS: /tsi/hi-paris/GB/segmentation/models/fcclip_cocopan_r50.pth
OUTPUT_DIR: /tsi/hi-paris/GB/segmentation/results/In_vocab/r50_008_1000_19_a_decoder
SEED: -1
SOLVER:
  AMP:
    ENABLED: true
  BACKBONE_MULTIPLIER: 0.1
  BASE_LR: 0.0001
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 5000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.01
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 8
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 1000
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 327778
  - 355092
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 1000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[10/08 14:45:39] detectron2 INFO: Full config saved to /tsi/hi-paris/GB/segmentation/results/In_vocab/r50_008_1000_19_a_decoder/config.yaml
[10/08 14:45:39] d2.utils.env INFO: Using a generated random seed 40699208
[10/08 14:45:43] d2.engine.defaults INFO: Model:
FCCLIP(
  (backbone): CLIP(
    (clip_model): CLIP(
      (visual): ModifiedResNet(
        (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
        (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        (layer1): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
            (downsample): Sequential(
              (-1): AvgPool2d(kernel_size=1, stride=1, padding=0)
              (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
        )
        (layer2): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
            (downsample): Sequential(
              (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (3): Bottleneck(
            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
        )
        (layer3): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
            (downsample): Sequential(
              (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
              (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (3): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (4): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (5): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
        )
        (layer4): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
            (downsample): Sequential(
              (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
              (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
        )
        (attnpool): AttentionPool2d(
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (c_proj): Linear(in_features=2048, out_features=1024, bias=True)
        )
      )
      (transformer): Transformer(
        (resblocks): ModuleList(
          (0-11): 12 x ResidualAttentionBlock(
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ls_2): Identity()
          )
        )
      )
      (token_embedding): Embedding(49408, 512)
      (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
  )
  (sem_seg_head): FCCLIPHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0-8): 9 x SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0-8): 9 x CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0-8): 9 x FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(250, 256)
      (query_embed): Embedding(250, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0-2): 3 x Sequential()
      )
      (mask_embed): MLP(
        (layers): ModuleList(
          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (mask_pooling): MaskPooling()
      (_mask_pooling_proj): Sequential(
        (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
      )
      (class_embed): MLP(
        (layers): ModuleList(
          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=1024, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 19
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
  (conv1): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv2): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU()
  (mask_pooling): MaskPooling()
  (decoder_adapter): DecoderAdapter(
    (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
    (conv2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    (norm1): LayerNorm((64, 1, 1), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((256, 1, 1), eps=1e-05, elementwise_affine=True)
    (relu): ReLU()
  )
  (void_embedding): Embedding(1, 1024)
)
[10/08 14:45:43] fcclip.data.dataset_mappers.mask_former_semantic_dataset_mapper INFO: [MaskFormerPanopticDatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(800,), max_size=1333, sample_style='choice'), RandomFlip()]
[10/08 14:45:43] fcclip.data.datasets.register_cityscapes_panoptic INFO: 18 cities found in 'datasets/cityscapes/leftImg8bit/train'.
[10/08 14:45:43] d2.data.build INFO: Using training sampler TrainingSampler
[10/08 14:45:44] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/08 14:45:44] d2.data.common INFO: Serializing 2975 elements to byte tensors and concatenating them all ...
[10/08 14:45:44] d2.data.common INFO: Serialized dataset takes 4.12 MiB
[10/08 14:45:44] d2.data.build INFO: Making batched data loader with batch_size=4
[10/08 14:45:44] d2.solver.build WARNING: SOLVER.STEPS contains values larger than SOLVER.MAX_ITER. These values will be ignored.
[10/08 14:45:44] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /tsi/hi-paris/GB/segmentation/models/fcclip_cocopan_r50.pth ...
[10/08 14:45:44] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /tsi/hi-paris/GB/segmentation/models/fcclip_cocopan_r50.pth ...
[10/08 14:45:44] fvcore.common.checkpoint WARNING: Skip loading parameter 'criterion.empty_weight' to the model due to incompatible shapes: (134,) in the checkpoint but (20,) in the model! You might want to double check if this is expected.
[10/08 14:45:44] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.clip_model.ln_final.{bias, weight}[0m
[34mbackbone.clip_model.token_embedding.weight[0m
[34mbackbone.clip_model.transformer.resblocks.0.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.attnpool.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.attnpool.k_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.attnpool.positional_embedding[0m
[34mbackbone.clip_model.visual.attnpool.q_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.attnpool.v_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.conv1.weight[0m
[34mbackbone.clip_model.visual.conv2.weight[0m
[34mbackbone.clip_model.visual.conv3.weight[0m
[34mbackbone.clip_model.visual.layer1.0.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.0.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.0.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.0.conv1.weight[0m
[34mbackbone.clip_model.visual.layer1.0.conv2.weight[0m
[34mbackbone.clip_model.visual.layer1.0.conv3.weight[0m
[34mbackbone.clip_model.visual.layer1.0.downsample.0.weight[0m
[34mbackbone.clip_model.visual.layer1.0.downsample.1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.1.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.1.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.1.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.1.conv1.weight[0m
[34mbackbone.clip_model.visual.layer1.1.conv2.weight[0m
[34mbackbone.clip_model.visual.layer1.1.conv3.weight[0m
[34mbackbone.clip_model.visual.layer1.2.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.2.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.2.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.2.conv1.weight[0m
[34mbackbone.clip_model.visual.layer1.2.conv2.weight[0m
[34mbackbone.clip_model.visual.layer1.2.conv3.weight[0m
[34mbackbone.clip_model.visual.layer2.0.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.0.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.0.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.0.conv1.weight[0m
[34mbackbone.clip_model.visual.layer2.0.conv2.weight[0m
[34mbackbone.clip_model.visual.layer2.0.conv3.weight[0m
[34mbackbone.clip_model.visual.layer2.0.downsample.0.weight[0m
[34mbackbone.clip_model.visual.layer2.0.downsample.1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.1.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.1.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.1.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.1.conv1.weight[0m
[34mbackbone.clip_model.visual.layer2.1.conv2.weight[0m
[34mbackbone.clip_model.visual.layer2.1.conv3.weight[0m
[34mbackbone.clip_model.visual.layer2.2.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.2.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.2.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.2.conv1.weight[0m
[34mbackbone.clip_model.visual.layer2.2.conv2.weight[0m
[34mbackbone.clip_model.visual.layer2.2.conv3.weight[0m
[34mbackbone.clip_model.visual.layer2.3.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.3.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.3.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.3.conv1.weight[0m
[34mbackbone.clip_model.visual.layer2.3.conv2.weight[0m
[34mbackbone.clip_model.visual.layer2.3.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.0.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.0.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.0.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.0.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.0.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.0.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.0.downsample.0.weight[0m
[34mbackbone.clip_model.visual.layer3.0.downsample.1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.1.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.1.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.1.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.1.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.1.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.1.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.2.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.2.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.2.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.2.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.2.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.2.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.3.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.3.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.3.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.3.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.3.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.3.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.4.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.4.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.4.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.4.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.4.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.4.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.5.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.5.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.5.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.5.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.5.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.5.conv3.weight[0m
[34mbackbone.clip_model.visual.layer4.0.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.0.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.0.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.0.conv1.weight[0m
[34mbackbone.clip_model.visual.layer4.0.conv2.weight[0m
[34mbackbone.clip_model.visual.layer4.0.conv3.weight[0m
[34mbackbone.clip_model.visual.layer4.0.downsample.0.weight[0m
[34mbackbone.clip_model.visual.layer4.0.downsample.1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.1.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.1.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.1.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.1.conv1.weight[0m
[34mbackbone.clip_model.visual.layer4.1.conv2.weight[0m
[34mbackbone.clip_model.visual.layer4.1.conv3.weight[0m
[34mbackbone.clip_model.visual.layer4.2.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.2.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.2.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.2.conv1.weight[0m
[34mbackbone.clip_model.visual.layer4.2.conv2.weight[0m
[34mbackbone.clip_model.visual.layer4.2.conv3.weight[0m
[34mbackbone.clip_model.{logit_scale, positional_embedding, text_projection}[0m
[34mbn1.{bias, running_mean, running_var, weight}[0m
[34mbn2.{bias, running_mean, running_var, weight}[0m
[34mconv1.{bias, weight}[0m
[34mconv2.{bias, weight}[0m
[34mcriterion.empty_weight[0m
[34mdecoder_adapter.conv1.{bias, weight}[0m
[34mdecoder_adapter.conv2.{bias, weight}[0m
[34mdecoder_adapter.norm1.{bias, weight}[0m
[34mdecoder_adapter.norm2.{bias, weight}[0m
[10/08 14:45:44] d2.engine.train_loop INFO: Starting training from iteration 0
[10/08 14:45:52] d2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/home/ids/gbrison/segmentation/segmentation/detectron2/detectron2/engine/train_loop.py", line 155, in train
    self.run_step()
  File "/home/ids/gbrison/segmentation/segmentation/detectron2/detectron2/engine/defaults.py", line 498, in run_step
    self._trainer.run_step()
  File "/home/ids/gbrison/segmentation/segmentation/detectron2/detectron2/engine/train_loop.py", line 494, in run_step
    loss_dict = self.model(data)
  File "/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1632, in forward
    inputs, kwargs = self._pre_forward(*inputs, **kwargs)
  File "/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1523, in _pre_forward
    if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameter indices which did not receive grad for rank 0: 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316
 In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[10/08 14:45:52] d2.engine.hooks INFO: Total training time: 0:00:00 (0:00:00 on hooks)
[10/08 14:45:52] d2.utils.events INFO:  iter: 1  total_loss: 35.76  loss_ce: 1.695  loss_mask: 0.4281  loss_dice: 1.361  loss_ce_0: 1.629  loss_mask_0: 0.4163  loss_dice_0: 1.701  loss_ce_1: 1.835  loss_mask_1: 0.4093  loss_dice_1: 1.533  loss_ce_2: 1.73  loss_mask_2: 0.4441  loss_dice_2: 1.472  loss_ce_3: 1.69  loss_mask_3: 0.4388  loss_dice_3: 1.414  loss_ce_4: 1.68  loss_mask_4: 0.4351  loss_dice_4: 1.421  loss_ce_5: 1.647  loss_mask_5: 0.4366  loss_dice_5: 1.401  loss_ce_6: 1.653  loss_mask_6: 0.4278  loss_dice_6: 1.351  loss_ce_7: 1.811  loss_mask_7: 0.4334  loss_dice_7: 1.376  loss_ce_8: 1.672  loss_mask_8: 0.424  loss_dice_8: 1.393    data_time: 4.8746  last_data_time: 4.8746   lr: 0.0001  max_mem: 15306M
[10/08 14:46:02] detectron2 INFO: Rank of current process: 0. World size: 1
[10/08 14:46:03] detectron2 INFO: Environment info:
-------------------------------  -------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.8.19 | packaged by conda-forge | (default, Mar 20 2024, 12:47:35) [GCC 12.3.0]
numpy                            1.24.4
detectron2                       0.6 @/home/ids/gbrison/segmentation/segmentation/detectron2/detectron2
Compiler                         GCC 11.4
CUDA compiler                    CUDA 12.5
detectron2 arch flags            8.9
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.0+cu121 @/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0,1                          NVIDIA L40S (arch=8.9)
Driver version                   555.42.06
CUDA_HOME                        /usr/local/cuda
Pillow                           9.4.0
torchvision                      0.19.0+cu121 @/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torchvision
torchvision arch flags           5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.10.0
-------------------------------  -------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX512
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.0, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/08 14:46:03] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco/panoptic-segmentation/fcclip/r50_exp_1000_a_decoder.yaml', dist_url='tcp://127.0.0.1:51163', eval_only=False, machine_rank=0, num_gpus=1, num_machines=1, opts=[], resume=False)
[10/08 14:46:03] detectron2 INFO: Contents of args.config_file=configs/coco/panoptic-segmentation/fcclip/r50_exp_1000_a_decoder.yaml:
_BASE_: ./fcclip_convnext_large_eval_ade20k_r50.yaml

INPUT:
  MIN_SIZE_TEST: 1024
  MAX_SIZE_TEST: 2560

MODEL:
  SEM_SEG_HEAD:
    NUM_CLASSES: 19
  WEIGHTS: /tsi/hi-paris/GB/segmentation/models/fcclip_cocopan_r50.pth
OUTPUT_DIR: /tsi/hi-paris/GB/segmentation/results/In_vocab/r50_008_1000_19_a_decoder
DATASETS:
  TRAIN: ("openvocab_cityscapes_fine_panoptic_train",)
  TEST: ("openvocab_cityscapes_fine_panoptic_val",)
SOLVER:
  IMS_PER_BATCH: 8
  MAX_ITER: 1000
TEST:
  EVAL_PERIOD: 1000


[10/08 14:46:03] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - openvocab_cityscapes_fine_panoptic_val
  TRAIN:
  - openvocab_cityscapes_fine_panoptic_train
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: false
  CROP:
    ENABLED: false
    MINIMUM_INST_AREA: 1
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  DATASET_MAPPER_NAME: coco_panoptic_lsj
  FORMAT: RGB
  IMAGE_SIZE: 1024
  MASK_FORMAT: polygon
  MAX_SCALE: 2.0
  MAX_SIZE_TEST: 2560
  MAX_SIZE_TRAIN: 1333
  MIN_SCALE: 0.1
  MIN_SIZE_TEST: 1024
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: -1
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 0
    NAME: CLIP
  DEVICE: cuda
  FC_CLIP:
    CLIP_MODEL_NAME: RN50
    CLIP_PRETRAINED_WEIGHTS: openai
    EMBED_DIM: 1024
    ENSEMBLE_ON_VALID_MASK: true
    GEOMETRIC_ENSEMBLE_ALPHA: 0.4
    GEOMETRIC_ENSEMBLE_BETA: 0.8
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_FORMER:
    CLASS_WEIGHT: 2.0
    DEC_LAYERS: 10
    DEEP_SUPERVISION: true
    DICE_WEIGHT: 5.0
    DIM_FEEDFORWARD: 2048
    DROPOUT: 0.0
    ENC_LAYERS: 0
    ENFORCE_INPUT_PROJ: false
    HIDDEN_DIM: 256
    IMPORTANCE_SAMPLE_RATIO: 0.75
    MASK_WEIGHT: 5.0
    NHEADS: 8
    NO_OBJECT_WEIGHT: 0.1
    NUM_OBJECT_QUERIES: 250
    OVERSAMPLE_RATIO: 3.0
    PRE_NORM: false
    SIZE_DIVISIBILITY: 32
    TEST:
      INSTANCE_ON: true
      OBJECT_MASK_THRESHOLD: 0.0
      OVERLAP_THRESHOLD: 0.8
      PANOPTIC_ON: true
      SEMANTIC_ON: true
      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: false
    TRAIN_NUM_POINTS: 12544
    TRANSFORMER_DECODER_NAME: MultiScaleMaskedTransformerDecoder
    TRANSFORMER_IN_FEATURE: multi_scale_pixel_decoder
  MASK_ON: false
  META_ARCHITECTURE: FCCLIP
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 122.7709383
  - 116.7460125
  - 104.09373615
  PIXEL_STD:
  - 68.5005327
  - 66.6321579
  - 70.32316305
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 1
    - 1
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: basic
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 256
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES:
    - res3
    - res4
    - res5
    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8
    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    MASK_DIM: 256
    NAME: FCCLIPHead
    NORM: GN
    NUM_CLASSES: 19
    PIXEL_DECODER_NAME: MSDeformAttnPixelDecoder
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    TRANSFORMER_ENC_LAYERS: 6
    USE_DEPTHWISE_SEPARABLE_CONV: false
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    USE_CHECKPOINT: false
    WINDOW_SIZE: 7
  WEIGHTS: /tsi/hi-paris/GB/segmentation/models/fcclip_cocopan_r50.pth
OUTPUT_DIR: /tsi/hi-paris/GB/segmentation/results/In_vocab/r50_008_1000_19_a_decoder
SEED: -1
SOLVER:
  AMP:
    ENABLED: true
  BACKBONE_MULTIPLIER: 0.1
  BASE_LR: 0.0001
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 5000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.01
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 8
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 1000
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 327778
  - 355092
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 1000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[10/08 14:46:03] detectron2 INFO: Full config saved to /tsi/hi-paris/GB/segmentation/results/In_vocab/r50_008_1000_19_a_decoder/config.yaml
[10/08 14:46:03] d2.utils.env INFO: Using a generated random seed 4499435
[10/08 14:46:07] d2.engine.defaults INFO: Model:
FCCLIP(
  (backbone): CLIP(
    (clip_model): CLIP(
      (visual): ModifiedResNet(
        (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
        (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        (layer1): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
            (downsample): Sequential(
              (-1): AvgPool2d(kernel_size=1, stride=1, padding=0)
              (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
        )
        (layer2): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
            (downsample): Sequential(
              (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (3): Bottleneck(
            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
        )
        (layer3): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
            (downsample): Sequential(
              (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
              (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (3): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (4): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (5): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
        )
        (layer4): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
            (downsample): Sequential(
              (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
              (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
        )
        (attnpool): AttentionPool2d(
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (c_proj): Linear(in_features=2048, out_features=1024, bias=True)
        )
      )
      (transformer): Transformer(
        (resblocks): ModuleList(
          (0-11): 12 x ResidualAttentionBlock(
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ls_2): Identity()
          )
        )
      )
      (token_embedding): Embedding(49408, 512)
      (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
  )
  (sem_seg_head): FCCLIPHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0-8): 9 x SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0-8): 9 x CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0-8): 9 x FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(250, 256)
      (query_embed): Embedding(250, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0-2): 3 x Sequential()
      )
      (mask_embed): MLP(
        (layers): ModuleList(
          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (mask_pooling): MaskPooling()
      (_mask_pooling_proj): Sequential(
        (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
      )
      (class_embed): MLP(
        (layers): ModuleList(
          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=1024, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 19
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
  (conv1): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv2): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU()
  (mask_pooling): MaskPooling()
  (decoder_adapter): DecoderAdapter(
    (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
    (conv2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    (norm1): LayerNorm((64, 1, 1), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((256, 1, 1), eps=1e-05, elementwise_affine=True)
    (relu): ReLU()
  )
  (void_embedding): Embedding(1, 1024)
)
[10/08 14:46:07] fcclip.data.dataset_mappers.mask_former_semantic_dataset_mapper INFO: [MaskFormerPanopticDatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(800,), max_size=1333, sample_style='choice'), RandomFlip()]
[10/08 14:46:07] fcclip.data.datasets.register_cityscapes_panoptic INFO: 18 cities found in 'datasets/cityscapes/leftImg8bit/train'.
[10/08 14:46:07] d2.data.build INFO: Using training sampler TrainingSampler
[10/08 14:46:07] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/08 14:46:07] d2.data.common INFO: Serializing 2975 elements to byte tensors and concatenating them all ...
[10/08 14:46:07] d2.data.common INFO: Serialized dataset takes 4.12 MiB
[10/08 14:46:07] d2.data.build INFO: Making batched data loader with batch_size=8
[10/08 14:46:07] d2.solver.build WARNING: SOLVER.STEPS contains values larger than SOLVER.MAX_ITER. These values will be ignored.
[10/08 14:46:07] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /tsi/hi-paris/GB/segmentation/models/fcclip_cocopan_r50.pth ...
[10/08 14:46:07] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /tsi/hi-paris/GB/segmentation/models/fcclip_cocopan_r50.pth ...
[10/08 14:46:07] fvcore.common.checkpoint WARNING: Skip loading parameter 'criterion.empty_weight' to the model due to incompatible shapes: (134,) in the checkpoint but (20,) in the model! You might want to double check if this is expected.
[10/08 14:46:07] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.clip_model.ln_final.{bias, weight}[0m
[34mbackbone.clip_model.token_embedding.weight[0m
[34mbackbone.clip_model.transformer.resblocks.0.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.attnpool.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.attnpool.k_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.attnpool.positional_embedding[0m
[34mbackbone.clip_model.visual.attnpool.q_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.attnpool.v_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.conv1.weight[0m
[34mbackbone.clip_model.visual.conv2.weight[0m
[34mbackbone.clip_model.visual.conv3.weight[0m
[34mbackbone.clip_model.visual.layer1.0.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.0.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.0.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.0.conv1.weight[0m
[34mbackbone.clip_model.visual.layer1.0.conv2.weight[0m
[34mbackbone.clip_model.visual.layer1.0.conv3.weight[0m
[34mbackbone.clip_model.visual.layer1.0.downsample.0.weight[0m
[34mbackbone.clip_model.visual.layer1.0.downsample.1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.1.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.1.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.1.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.1.conv1.weight[0m
[34mbackbone.clip_model.visual.layer1.1.conv2.weight[0m
[34mbackbone.clip_model.visual.layer1.1.conv3.weight[0m
[34mbackbone.clip_model.visual.layer1.2.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.2.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.2.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.2.conv1.weight[0m
[34mbackbone.clip_model.visual.layer1.2.conv2.weight[0m
[34mbackbone.clip_model.visual.layer1.2.conv3.weight[0m
[34mbackbone.clip_model.visual.layer2.0.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.0.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.0.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.0.conv1.weight[0m
[34mbackbone.clip_model.visual.layer2.0.conv2.weight[0m
[34mbackbone.clip_model.visual.layer2.0.conv3.weight[0m
[34mbackbone.clip_model.visual.layer2.0.downsample.0.weight[0m
[34mbackbone.clip_model.visual.layer2.0.downsample.1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.1.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.1.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.1.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.1.conv1.weight[0m
[34mbackbone.clip_model.visual.layer2.1.conv2.weight[0m
[34mbackbone.clip_model.visual.layer2.1.conv3.weight[0m
[34mbackbone.clip_model.visual.layer2.2.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.2.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.2.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.2.conv1.weight[0m
[34mbackbone.clip_model.visual.layer2.2.conv2.weight[0m
[34mbackbone.clip_model.visual.layer2.2.conv3.weight[0m
[34mbackbone.clip_model.visual.layer2.3.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.3.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.3.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.3.conv1.weight[0m
[34mbackbone.clip_model.visual.layer2.3.conv2.weight[0m
[34mbackbone.clip_model.visual.layer2.3.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.0.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.0.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.0.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.0.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.0.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.0.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.0.downsample.0.weight[0m
[34mbackbone.clip_model.visual.layer3.0.downsample.1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.1.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.1.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.1.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.1.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.1.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.1.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.2.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.2.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.2.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.2.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.2.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.2.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.3.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.3.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.3.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.3.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.3.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.3.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.4.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.4.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.4.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.4.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.4.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.4.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.5.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.5.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.5.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.5.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.5.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.5.conv3.weight[0m
[34mbackbone.clip_model.visual.layer4.0.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.0.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.0.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.0.conv1.weight[0m
[34mbackbone.clip_model.visual.layer4.0.conv2.weight[0m
[34mbackbone.clip_model.visual.layer4.0.conv3.weight[0m
[34mbackbone.clip_model.visual.layer4.0.downsample.0.weight[0m
[34mbackbone.clip_model.visual.layer4.0.downsample.1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.1.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.1.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.1.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.1.conv1.weight[0m
[34mbackbone.clip_model.visual.layer4.1.conv2.weight[0m
[34mbackbone.clip_model.visual.layer4.1.conv3.weight[0m
[34mbackbone.clip_model.visual.layer4.2.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.2.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.2.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.2.conv1.weight[0m
[34mbackbone.clip_model.visual.layer4.2.conv2.weight[0m
[34mbackbone.clip_model.visual.layer4.2.conv3.weight[0m
[34mbackbone.clip_model.{logit_scale, positional_embedding, text_projection}[0m
[34mbn1.{bias, running_mean, running_var, weight}[0m
[34mbn2.{bias, running_mean, running_var, weight}[0m
[34mconv1.{bias, weight}[0m
[34mconv2.{bias, weight}[0m
[34mcriterion.empty_weight[0m
[34mdecoder_adapter.conv1.{bias, weight}[0m
[34mdecoder_adapter.conv2.{bias, weight}[0m
[34mdecoder_adapter.norm1.{bias, weight}[0m
[34mdecoder_adapter.norm2.{bias, weight}[0m
[10/08 14:46:07] d2.engine.train_loop INFO: Starting training from iteration 0
[10/08 14:46:31] d2.utils.events INFO:  eta: 0:17:10  iter: 19  total_loss: 30.73  loss_ce: 1.107  loss_mask: 0.2878  loss_dice: 1.517  loss_ce_0: 1.358  loss_mask_0: 0.3115  loss_dice_0: 1.809  loss_ce_1: 1.397  loss_mask_1: 0.2887  loss_dice_1: 1.667  loss_ce_2: 1.246  loss_mask_2: 0.2807  loss_dice_2: 1.627  loss_ce_3: 1.243  loss_mask_3: 0.2963  loss_dice_3: 1.55  loss_ce_4: 1.148  loss_mask_4: 0.2978  loss_dice_4: 1.552  loss_ce_5: 1.165  loss_mask_5: 0.2918  loss_dice_5: 1.511  loss_ce_6: 1.092  loss_mask_6: 0.2843  loss_dice_6: 1.514  loss_ce_7: 1.068  loss_mask_7: 0.2848  loss_dice_7: 1.538  loss_ce_8: 1.136  loss_mask_8: 0.2842  loss_dice_8: 1.506    time: 1.0578  last_time: 1.0781  data_time: 0.1102  last_data_time: 0.0854   lr: 0.0001  max_mem: 30820M
[10/08 14:46:53] d2.utils.events INFO:  eta: 0:16:44  iter: 39  total_loss: 24.98  loss_ce: 0.7506  loss_mask: 0.2464  loss_dice: 1.364  loss_ce_0: 0.8647  loss_mask_0: 0.2745  loss_dice_0: 1.696  loss_ce_1: 0.9062  loss_mask_1: 0.2694  loss_dice_1: 1.553  loss_ce_2: 0.8468  loss_mask_2: 0.2492  loss_dice_2: 1.497  loss_ce_3: 0.804  loss_mask_3: 0.2451  loss_dice_3: 1.42  loss_ce_4: 0.761  loss_mask_4: 0.2456  loss_dice_4: 1.414  loss_ce_5: 0.74  loss_mask_5: 0.246  loss_dice_5: 1.394  loss_ce_6: 0.7428  loss_mask_6: 0.246  loss_dice_6: 1.41  loss_ce_7: 0.7244  loss_mask_7: 0.2445  loss_dice_7: 1.38  loss_ce_8: 0.7242  loss_mask_8: 0.2463  loss_dice_8: 1.379    time: 1.0562  last_time: 1.0190  data_time: 0.0710  last_data_time: 0.0706   lr: 0.0001  max_mem: 31735M
[10/08 14:47:14] d2.utils.events INFO:  eta: 0:16:28  iter: 59  total_loss: 23.61  loss_ce: 0.6678  loss_mask: 0.22  loss_dice: 1.307  loss_ce_0: 0.709  loss_mask_0: 0.2461  loss_dice_0: 1.605  loss_ce_1: 0.8087  loss_mask_1: 0.2408  loss_dice_1: 1.479  loss_ce_2: 0.7562  loss_mask_2: 0.2321  loss_dice_2: 1.438  loss_ce_3: 0.7097  loss_mask_3: 0.2229  loss_dice_3: 1.36  loss_ce_4: 0.6973  loss_mask_4: 0.2257  loss_dice_4: 1.355  loss_ce_5: 0.6727  loss_mask_5: 0.2239  loss_dice_5: 1.356  loss_ce_6: 0.6558  loss_mask_6: 0.2219  loss_dice_6: 1.327  loss_ce_7: 0.6455  loss_mask_7: 0.2217  loss_dice_7: 1.35  loss_ce_8: 0.6512  loss_mask_8: 0.2201  loss_dice_8: 1.354    time: 1.0640  last_time: 1.1580  data_time: 0.0722  last_data_time: 0.0849   lr: 0.0001  max_mem: 31735M
[10/08 14:47:36] d2.utils.events INFO:  eta: 0:16:14  iter: 79  total_loss: 23.42  loss_ce: 0.6685  loss_mask: 0.2002  loss_dice: 1.341  loss_ce_0: 0.6912  loss_mask_0: 0.2307  loss_dice_0: 1.67  loss_ce_1: 0.8406  loss_mask_1: 0.2057  loss_dice_1: 1.505  loss_ce_2: 0.7588  loss_mask_2: 0.2009  loss_dice_2: 1.427  loss_ce_3: 0.7247  loss_mask_3: 0.2008  loss_dice_3: 1.399  loss_ce_4: 0.6823  loss_mask_4: 0.2034  loss_dice_4: 1.408  loss_ce_5: 0.6465  loss_mask_5: 0.2014  loss_dice_5: 1.361  loss_ce_6: 0.6484  loss_mask_6: 0.2034  loss_dice_6: 1.34  loss_ce_7: 0.6579  loss_mask_7: 0.2048  loss_dice_7: 1.33  loss_ce_8: 0.6669  loss_mask_8: 0.2002  loss_dice_8: 1.353    time: 1.0735  last_time: 1.0326  data_time: 0.0785  last_data_time: 0.0582   lr: 0.0001  max_mem: 31735M
[10/08 14:47:59] d2.utils.events INFO:  eta: 0:16:10  iter: 99  total_loss: 23.11  loss_ce: 0.6249  loss_mask: 0.2028  loss_dice: 1.383  loss_ce_0: 0.6738  loss_mask_0: 0.2318  loss_dice_0: 1.574  loss_ce_1: 0.8123  loss_mask_1: 0.2157  loss_dice_1: 1.526  loss_ce_2: 0.7319  loss_mask_2: 0.2056  loss_dice_2: 1.48  loss_ce_3: 0.6933  loss_mask_3: 0.1991  loss_dice_3: 1.368  loss_ce_4: 0.6686  loss_mask_4: 0.2034  loss_dice_4: 1.419  loss_ce_5: 0.6276  loss_mask_5: 0.199  loss_dice_5: 1.391  loss_ce_6: 0.6556  loss_mask_6: 0.2032  loss_dice_6: 1.379  loss_ce_7: 0.6201  loss_mask_7: 0.1994  loss_dice_7: 1.37  loss_ce_8: 0.637  loss_mask_8: 0.2018  loss_dice_8: 1.335    time: 1.0879  last_time: 1.1585  data_time: 0.0852  last_data_time: 0.0841   lr: 0.0001  max_mem: 31892M
[10/08 14:48:22] d2.utils.events INFO:  eta: 0:16:05  iter: 119  total_loss: 22.53  loss_ce: 0.6157  loss_mask: 0.1987  loss_dice: 1.321  loss_ce_0: 0.7111  loss_mask_0: 0.2278  loss_dice_0: 1.597  loss_ce_1: 0.8359  loss_mask_1: 0.2126  loss_dice_1: 1.481  loss_ce_2: 0.7008  loss_mask_2: 0.2045  loss_dice_2: 1.429  loss_ce_3: 0.6587  loss_mask_3: 0.2019  loss_dice_3: 1.362  loss_ce_4: 0.645  loss_mask_4: 0.1994  loss_dice_4: 1.344  loss_ce_5: 0.6246  loss_mask_5: 0.1994  loss_dice_5: 1.364  loss_ce_6: 0.6211  loss_mask_6: 0.2005  loss_dice_6: 1.324  loss_ce_7: 0.6317  loss_mask_7: 0.2025  loss_dice_7: 1.324  loss_ce_8: 0.6181  loss_mask_8: 0.1979  loss_dice_8: 1.32    time: 1.0927  last_time: 1.1176  data_time: 0.0746  last_data_time: 0.0634   lr: 0.0001  max_mem: 31892M
[10/08 14:48:44] d2.utils.events INFO:  eta: 0:15:43  iter: 139  total_loss: 23.81  loss_ce: 0.656  loss_mask: 0.2061  loss_dice: 1.384  loss_ce_0: 0.7399  loss_mask_0: 0.2351  loss_dice_0: 1.677  loss_ce_1: 0.7829  loss_mask_1: 0.2178  loss_dice_1: 1.581  loss_ce_2: 0.7441  loss_mask_2: 0.2092  loss_dice_2: 1.522  loss_ce_3: 0.7064  loss_mask_3: 0.2096  loss_dice_3: 1.461  loss_ce_4: 0.696  loss_mask_4: 0.2109  loss_dice_4: 1.421  loss_ce_5: 0.6636  loss_mask_5: 0.2092  loss_dice_5: 1.435  loss_ce_6: 0.6359  loss_mask_6: 0.2058  loss_dice_6: 1.407  loss_ce_7: 0.6338  loss_mask_7: 0.2073  loss_dice_7: 1.396  loss_ce_8: 0.6477  loss_mask_8: 0.2089  loss_dice_8: 1.411    time: 1.0942  last_time: 1.0952  data_time: 0.0767  last_data_time: 0.0950   lr: 0.0001  max_mem: 31892M
[10/08 14:49:06] d2.utils.events INFO:  eta: 0:15:22  iter: 159  total_loss: 22.1  loss_ce: 0.5927  loss_mask: 0.1854  loss_dice: 1.347  loss_ce_0: 0.6811  loss_mask_0: 0.224  loss_dice_0: 1.597  loss_ce_1: 0.7895  loss_mask_1: 0.2034  loss_dice_1: 1.484  loss_ce_2: 0.7084  loss_mask_2: 0.1956  loss_dice_2: 1.429  loss_ce_3: 0.6523  loss_mask_3: 0.1992  loss_dice_3: 1.37  loss_ce_4: 0.607  loss_mask_4: 0.1867  loss_dice_4: 1.381  loss_ce_5: 0.5996  loss_mask_5: 0.1871  loss_dice_5: 1.362  loss_ce_6: 0.5765  loss_mask_6: 0.183  loss_dice_6: 1.362  loss_ce_7: 0.6002  loss_mask_7: 0.1855  loss_dice_7: 1.341  loss_ce_8: 0.5856  loss_mask_8: 0.1838  loss_dice_8: 1.334    time: 1.0961  last_time: 1.0623  data_time: 0.0798  last_data_time: 0.0806   lr: 0.0001  max_mem: 31892M
[10/08 14:49:28] d2.utils.events INFO:  eta: 0:14:59  iter: 179  total_loss: 21.44  loss_ce: 0.5896  loss_mask: 0.188  loss_dice: 1.279  loss_ce_0: 0.6723  loss_mask_0: 0.2133  loss_dice_0: 1.539  loss_ce_1: 0.7662  loss_mask_1: 0.1986  loss_dice_1: 1.469  loss_ce_2: 0.6893  loss_mask_2: 0.1919  loss_dice_2: 1.364  loss_ce_3: 0.6147  loss_mask_3: 0.1875  loss_dice_3: 1.298  loss_ce_4: 0.6017  loss_mask_4: 0.1892  loss_dice_4: 1.302  loss_ce_5: 0.5731  loss_mask_5: 0.1864  loss_dice_5: 1.298  loss_ce_6: 0.6055  loss_mask_6: 0.1859  loss_dice_6: 1.254  loss_ce_7: 0.5582  loss_mask_7: 0.1866  loss_dice_7: 1.26  loss_ce_8: 0.5679  loss_mask_8: 0.1884  loss_dice_8: 1.274    time: 1.0955  last_time: 1.0802  data_time: 0.0771  last_data_time: 0.0760   lr: 0.0001  max_mem: 31892M
[10/08 14:49:50] d2.utils.events INFO:  eta: 0:14:38  iter: 199  total_loss: 21.76  loss_ce: 0.6052  loss_mask: 0.1769  loss_dice: 1.265  loss_ce_0: 0.6625  loss_mask_0: 0.202  loss_dice_0: 1.574  loss_ce_1: 0.7604  loss_mask_1: 0.1919  loss_dice_1: 1.477  loss_ce_2: 0.6725  loss_mask_2: 0.1797  loss_dice_2: 1.402  loss_ce_3: 0.6541  loss_mask_3: 0.1796  loss_dice_3: 1.328  loss_ce_4: 0.61  loss_mask_4: 0.1811  loss_dice_4: 1.334  loss_ce_5: 0.612  loss_mask_5: 0.1769  loss_dice_5: 1.321  loss_ce_6: 0.5994  loss_mask_6: 0.1779  loss_dice_6: 1.311  loss_ce_7: 0.5895  loss_mask_7: 0.1765  loss_dice_7: 1.283  loss_ce_8: 0.596  loss_mask_8: 0.1754  loss_dice_8: 1.315    time: 1.0972  last_time: 1.1649  data_time: 0.0770  last_data_time: 0.0764   lr: 0.0001  max_mem: 31892M
[10/08 14:50:13] d2.utils.events INFO:  eta: 0:14:18  iter: 219  total_loss: 22.33  loss_ce: 0.5792  loss_mask: 0.1725  loss_dice: 1.305  loss_ce_0: 0.6466  loss_mask_0: 0.2137  loss_dice_0: 1.62  loss_ce_1: 0.7381  loss_mask_1: 0.1847  loss_dice_1: 1.496  loss_ce_2: 0.6744  loss_mask_2: 0.1784  loss_dice_2: 1.439  loss_ce_3: 0.6438  loss_mask_3: 0.1725  loss_dice_3: 1.377  loss_ce_4: 0.6377  loss_mask_4: 0.1705  loss_dice_4: 1.353  loss_ce_5: 0.5981  loss_mask_5: 0.1723  loss_dice_5: 1.371  loss_ce_6: 0.6088  loss_mask_6: 0.1749  loss_dice_6: 1.339  loss_ce_7: 0.5938  loss_mask_7: 0.171  loss_dice_7: 1.33  loss_ce_8: 0.6089  loss_mask_8: 0.1742  loss_dice_8: 1.343    time: 1.0995  last_time: 1.1385  data_time: 0.0737  last_data_time: 0.0623   lr: 0.0001  max_mem: 31892M
[10/08 14:50:35] d2.utils.events INFO:  eta: 0:13:57  iter: 239  total_loss: 21.31  loss_ce: 0.5678  loss_mask: 0.2073  loss_dice: 1.234  loss_ce_0: 0.621  loss_mask_0: 0.2355  loss_dice_0: 1.486  loss_ce_1: 0.7108  loss_mask_1: 0.2201  loss_dice_1: 1.399  loss_ce_2: 0.6563  loss_mask_2: 0.2199  loss_dice_2: 1.325  loss_ce_3: 0.5904  loss_mask_3: 0.2112  loss_dice_3: 1.312  loss_ce_4: 0.571  loss_mask_4: 0.2123  loss_dice_4: 1.293  loss_ce_5: 0.5544  loss_mask_5: 0.2114  loss_dice_5: 1.301  loss_ce_6: 0.5864  loss_mask_6: 0.2088  loss_dice_6: 1.261  loss_ce_7: 0.5944  loss_mask_7: 0.2115  loss_dice_7: 1.257  loss_ce_8: 0.5816  loss_mask_8: 0.2091  loss_dice_8: 1.292    time: 1.1006  last_time: 1.1108  data_time: 0.0719  last_data_time: 0.0730   lr: 0.0001  max_mem: 32663M
[10/08 14:50:57] d2.utils.events INFO:  eta: 0:13:36  iter: 259  total_loss: 21.64  loss_ce: 0.5285  loss_mask: 0.1839  loss_dice: 1.267  loss_ce_0: 0.6571  loss_mask_0: 0.2232  loss_dice_0: 1.543  loss_ce_1: 0.7481  loss_mask_1: 0.2036  loss_dice_1: 1.437  loss_ce_2: 0.6607  loss_mask_2: 0.1906  loss_dice_2: 1.354  loss_ce_3: 0.6069  loss_mask_3: 0.1903  loss_dice_3: 1.317  loss_ce_4: 0.5942  loss_mask_4: 0.2008  loss_dice_4: 1.302  loss_ce_5: 0.5881  loss_mask_5: 0.1879  loss_dice_5: 1.305  loss_ce_6: 0.5657  loss_mask_6: 0.1883  loss_dice_6: 1.301  loss_ce_7: 0.5425  loss_mask_7: 0.186  loss_dice_7: 1.285  loss_ce_8: 0.5411  loss_mask_8: 0.1877  loss_dice_8: 1.281    time: 1.1008  last_time: 1.1150  data_time: 0.0762  last_data_time: 0.0866   lr: 0.0001  max_mem: 32663M
[10/08 14:51:19] d2.utils.events INFO:  eta: 0:13:12  iter: 279  total_loss: 21.61  loss_ce: 0.5565  loss_mask: 0.1913  loss_dice: 1.29  loss_ce_0: 0.6262  loss_mask_0: 0.2151  loss_dice_0: 1.568  loss_ce_1: 0.7345  loss_mask_1: 0.2068  loss_dice_1: 1.418  loss_ce_2: 0.6347  loss_mask_2: 0.1953  loss_dice_2: 1.376  loss_ce_3: 0.6036  loss_mask_3: 0.1949  loss_dice_3: 1.333  loss_ce_4: 0.5772  loss_mask_4: 0.1939  loss_dice_4: 1.305  loss_ce_5: 0.5562  loss_mask_5: 0.1913  loss_dice_5: 1.313  loss_ce_6: 0.5541  loss_mask_6: 0.1905  loss_dice_6: 1.281  loss_ce_7: 0.5664  loss_mask_7: 0.1909  loss_dice_7: 1.317  loss_ce_8: 0.5608  loss_mask_8: 0.1901  loss_dice_8: 1.239    time: 1.1007  last_time: 1.0461  data_time: 0.0729  last_data_time: 0.0621   lr: 0.0001  max_mem: 32663M
[10/08 14:51:41] d2.utils.events INFO:  eta: 0:12:51  iter: 299  total_loss: 22.24  loss_ce: 0.5605  loss_mask: 0.1882  loss_dice: 1.339  loss_ce_0: 0.6453  loss_mask_0: 0.2249  loss_dice_0: 1.639  loss_ce_1: 0.7071  loss_mask_1: 0.2015  loss_dice_1: 1.551  loss_ce_2: 0.6553  loss_mask_2: 0.1948  loss_dice_2: 1.435  loss_ce_3: 0.594  loss_mask_3: 0.1934  loss_dice_3: 1.36  loss_ce_4: 0.5774  loss_mask_4: 0.195  loss_dice_4: 1.386  loss_ce_5: 0.5869  loss_mask_5: 0.1929  loss_dice_5: 1.344  loss_ce_6: 0.5842  loss_mask_6: 0.1914  loss_dice_6: 1.33  loss_ce_7: 0.5716  loss_mask_7: 0.1899  loss_dice_7: 1.316  loss_ce_8: 0.5526  loss_mask_8: 0.1908  loss_dice_8: 1.314    time: 1.1009  last_time: 1.1186  data_time: 0.0759  last_data_time: 0.0898   lr: 0.0001  max_mem: 32663M
[10/08 14:52:04] d2.utils.events INFO:  eta: 0:12:30  iter: 319  total_loss: 21.99  loss_ce: 0.5832  loss_mask: 0.1812  loss_dice: 1.319  loss_ce_0: 0.6237  loss_mask_0: 0.2093  loss_dice_0: 1.566  loss_ce_1: 0.7152  loss_mask_1: 0.1941  loss_dice_1: 1.507  loss_ce_2: 0.6407  loss_mask_2: 0.1913  loss_dice_2: 1.408  loss_ce_3: 0.5966  loss_mask_3: 0.1821  loss_dice_3: 1.37  loss_ce_4: 0.5588  loss_mask_4: 0.1829  loss_dice_4: 1.327  loss_ce_5: 0.5807  loss_mask_5: 0.1823  loss_dice_5: 1.347  loss_ce_6: 0.5679  loss_mask_6: 0.1809  loss_dice_6: 1.318  loss_ce_7: 0.5455  loss_mask_7: 0.1833  loss_dice_7: 1.306  loss_ce_8: 0.5457  loss_mask_8: 0.1829  loss_dice_8: 1.321    time: 1.1027  last_time: 1.1221  data_time: 0.0785  last_data_time: 0.0666   lr: 0.0001  max_mem: 32663M
[10/08 14:52:27] d2.utils.events INFO:  eta: 0:12:09  iter: 339  total_loss: 21.85  loss_ce: 0.6033  loss_mask: 0.1798  loss_dice: 1.319  loss_ce_0: 0.6747  loss_mask_0: 0.2076  loss_dice_0: 1.52  loss_ce_1: 0.7249  loss_mask_1: 0.1974  loss_dice_1: 1.475  loss_ce_2: 0.6697  loss_mask_2: 0.1812  loss_dice_2: 1.395  loss_ce_3: 0.6326  loss_mask_3: 0.1726  loss_dice_3: 1.384  loss_ce_4: 0.6226  loss_mask_4: 0.1717  loss_dice_4: 1.343  loss_ce_5: 0.6115  loss_mask_5: 0.1758  loss_dice_5: 1.314  loss_ce_6: 0.5819  loss_mask_6: 0.1735  loss_dice_6: 1.318  loss_ce_7: 0.5893  loss_mask_7: 0.175  loss_dice_7: 1.308  loss_ce_8: 0.6128  loss_mask_8: 0.1788  loss_dice_8: 1.317    time: 1.1046  last_time: 1.1804  data_time: 0.0824  last_data_time: 0.0918   lr: 0.0001  max_mem: 32663M
[10/08 14:52:49] d2.utils.events INFO:  eta: 0:11:47  iter: 359  total_loss: 20.37  loss_ce: 0.5006  loss_mask: 0.1877  loss_dice: 1.242  loss_ce_0: 0.5778  loss_mask_0: 0.2159  loss_dice_0: 1.517  loss_ce_1: 0.669  loss_mask_1: 0.1993  loss_dice_1: 1.385  loss_ce_2: 0.5847  loss_mask_2: 0.193  loss_dice_2: 1.346  loss_ce_3: 0.5957  loss_mask_3: 0.1883  loss_dice_3: 1.265  loss_ce_4: 0.5669  loss_mask_4: 0.1883  loss_dice_4: 1.276  loss_ce_5: 0.5495  loss_mask_5: 0.1897  loss_dice_5: 1.276  loss_ce_6: 0.5249  loss_mask_6: 0.1908  loss_dice_6: 1.235  loss_ce_7: 0.5586  loss_mask_7: 0.186  loss_dice_7: 1.27  loss_ce_8: 0.5319  loss_mask_8: 0.1862  loss_dice_8: 1.265    time: 1.1045  last_time: 1.2185  data_time: 0.0789  last_data_time: 0.1720   lr: 0.0001  max_mem: 32663M
[10/08 14:53:11] d2.utils.events INFO:  eta: 0:11:25  iter: 379  total_loss: 20.66  loss_ce: 0.5385  loss_mask: 0.1888  loss_dice: 1.209  loss_ce_0: 0.6035  loss_mask_0: 0.2176  loss_dice_0: 1.442  loss_ce_1: 0.671  loss_mask_1: 0.2003  loss_dice_1: 1.357  loss_ce_2: 0.6367  loss_mask_2: 0.1952  loss_dice_2: 1.322  loss_ce_3: 0.5602  loss_mask_3: 0.1927  loss_dice_3: 1.245  loss_ce_4: 0.5313  loss_mask_4: 0.1875  loss_dice_4: 1.252  loss_ce_5: 0.5424  loss_mask_5: 0.1886  loss_dice_5: 1.227  loss_ce_6: 0.4935  loss_mask_6: 0.1864  loss_dice_6: 1.209  loss_ce_7: 0.5259  loss_mask_7: 0.1898  loss_dice_7: 1.241  loss_ce_8: 0.5215  loss_mask_8: 0.1887  loss_dice_8: 1.246    time: 1.1055  last_time: 1.1937  data_time: 0.0832  last_data_time: 0.0840   lr: 0.0001  max_mem: 32663M
[10/08 14:53:33] d2.utils.events INFO:  eta: 0:11:03  iter: 399  total_loss: 20.64  loss_ce: 0.4916  loss_mask: 0.1798  loss_dice: 1.288  loss_ce_0: 0.5297  loss_mask_0: 0.2032  loss_dice_0: 1.496  loss_ce_1: 0.6236  loss_mask_1: 0.1916  loss_dice_1: 1.402  loss_ce_2: 0.5464  loss_mask_2: 0.1844  loss_dice_2: 1.383  loss_ce_3: 0.5268  loss_mask_3: 0.1804  loss_dice_3: 1.327  loss_ce_4: 0.4847  loss_mask_4: 0.1837  loss_dice_4: 1.315  loss_ce_5: 0.5173  loss_mask_5: 0.1808  loss_dice_5: 1.29  loss_ce_6: 0.5025  loss_mask_6: 0.1819  loss_dice_6: 1.265  loss_ce_7: 0.5085  loss_mask_7: 0.1787  loss_dice_7: 1.303  loss_ce_8: 0.4953  loss_mask_8: 0.18  loss_dice_8: 1.28    time: 1.1048  last_time: 1.0710  data_time: 0.0769  last_data_time: 0.0846   lr: 0.0001  max_mem: 32663M
[10/08 14:53:55] d2.utils.events INFO:  eta: 0:10:41  iter: 419  total_loss: 21.09  loss_ce: 0.5479  loss_mask: 0.1792  loss_dice: 1.245  loss_ce_0: 0.6092  loss_mask_0: 0.201  loss_dice_0: 1.54  loss_ce_1: 0.6844  loss_mask_1: 0.1895  loss_dice_1: 1.41  loss_ce_2: 0.6245  loss_mask_2: 0.1816  loss_dice_2: 1.352  loss_ce_3: 0.5833  loss_mask_3: 0.1775  loss_dice_3: 1.308  loss_ce_4: 0.5537  loss_mask_4: 0.1815  loss_dice_4: 1.276  loss_ce_5: 0.5509  loss_mask_5: 0.182  loss_dice_5: 1.28  loss_ce_6: 0.526  loss_mask_6: 0.18  loss_dice_6: 1.266  loss_ce_7: 0.5317  loss_mask_7: 0.1792  loss_dice_7: 1.252  loss_ce_8: 0.4948  loss_mask_8: 0.1781  loss_dice_8: 1.279    time: 1.1049  last_time: 1.1331  data_time: 0.0820  last_data_time: 0.1113   lr: 0.0001  max_mem: 32663M
[10/08 14:54:17] d2.utils.events INFO:  eta: 0:10:18  iter: 439  total_loss: 19.8  loss_ce: 0.5294  loss_mask: 0.1902  loss_dice: 1.233  loss_ce_0: 0.5531  loss_mask_0: 0.2102  loss_dice_0: 1.494  loss_ce_1: 0.6562  loss_mask_1: 0.1998  loss_dice_1: 1.362  loss_ce_2: 0.6138  loss_mask_2: 0.1956  loss_dice_2: 1.319  loss_ce_3: 0.5485  loss_mask_3: 0.198  loss_dice_3: 1.273  loss_ce_4: 0.5314  loss_mask_4: 0.1943  loss_dice_4: 1.227  loss_ce_5: 0.5486  loss_mask_5: 0.1923  loss_dice_5: 1.227  loss_ce_6: 0.5456  loss_mask_6: 0.189  loss_dice_6: 1.202  loss_ce_7: 0.5305  loss_mask_7: 0.1911  loss_dice_7: 1.265  loss_ce_8: 0.5403  loss_mask_8: 0.1883  loss_dice_8: 1.265    time: 1.1043  last_time: 1.0940  data_time: 0.0834  last_data_time: 0.0815   lr: 0.0001  max_mem: 32663M
[10/08 14:54:39] d2.utils.events INFO:  eta: 0:09:56  iter: 459  total_loss: 21.01  loss_ce: 0.5205  loss_mask: 0.1863  loss_dice: 1.271  loss_ce_0: 0.5824  loss_mask_0: 0.2131  loss_dice_0: 1.518  loss_ce_1: 0.6747  loss_mask_1: 0.1926  loss_dice_1: 1.433  loss_ce_2: 0.633  loss_mask_2: 0.1952  loss_dice_2: 1.365  loss_ce_3: 0.5863  loss_mask_3: 0.192  loss_dice_3: 1.292  loss_ce_4: 0.5235  loss_mask_4: 0.1884  loss_dice_4: 1.298  loss_ce_5: 0.5598  loss_mask_5: 0.184  loss_dice_5: 1.285  loss_ce_6: 0.5283  loss_mask_6: 0.184  loss_dice_6: 1.302  loss_ce_7: 0.5246  loss_mask_7: 0.186  loss_dice_7: 1.309  loss_ce_8: 0.5362  loss_mask_8: 0.1847  loss_dice_8: 1.253    time: 1.1035  last_time: 1.1041  data_time: 0.0767  last_data_time: 0.0687   lr: 0.0001  max_mem: 32663M
[10/08 14:55:01] d2.utils.events INFO:  eta: 0:09:33  iter: 479  total_loss: 20.63  loss_ce: 0.5322  loss_mask: 0.1821  loss_dice: 1.263  loss_ce_0: 0.619  loss_mask_0: 0.2072  loss_dice_0: 1.503  loss_ce_1: 0.6789  loss_mask_1: 0.1921  loss_dice_1: 1.409  loss_ce_2: 0.6185  loss_mask_2: 0.1855  loss_dice_2: 1.359  loss_ce_3: 0.5782  loss_mask_3: 0.188  loss_dice_3: 1.252  loss_ce_4: 0.5407  loss_mask_4: 0.1863  loss_dice_4: 1.272  loss_ce_5: 0.5404  loss_mask_5: 0.1828  loss_dice_5: 1.281  loss_ce_6: 0.5313  loss_mask_6: 0.1818  loss_dice_6: 1.249  loss_ce_7: 0.5287  loss_mask_7: 0.182  loss_dice_7: 1.29  loss_ce_8: 0.5441  loss_mask_8: 0.1839  loss_dice_8: 1.245    time: 1.1030  last_time: 1.0912  data_time: 0.0770  last_data_time: 0.0755   lr: 0.0001  max_mem: 32663M
[10/08 14:55:23] d2.utils.events INFO:  eta: 0:09:11  iter: 499  total_loss: 20.4  loss_ce: 0.5283  loss_mask: 0.1757  loss_dice: 1.277  loss_ce_0: 0.5826  loss_mask_0: 0.1942  loss_dice_0: 1.459  loss_ce_1: 0.7097  loss_mask_1: 0.1854  loss_dice_1: 1.395  loss_ce_2: 0.6465  loss_mask_2: 0.1786  loss_dice_2: 1.378  loss_ce_3: 0.5567  loss_mask_3: 0.178  loss_dice_3: 1.312  loss_ce_4: 0.5272  loss_mask_4: 0.1794  loss_dice_4: 1.316  loss_ce_5: 0.536  loss_mask_5: 0.1764  loss_dice_5: 1.318  loss_ce_6: 0.5247  loss_mask_6: 0.1741  loss_dice_6: 1.288  loss_ce_7: 0.4977  loss_mask_7: 0.1766  loss_dice_7: 1.259  loss_ce_8: 0.5031  loss_mask_8: 0.1754  loss_dice_8: 1.28    time: 1.1036  last_time: 1.1423  data_time: 0.0811  last_data_time: 0.1048   lr: 0.0001  max_mem: 32663M
[10/08 14:55:46] d2.utils.events INFO:  eta: 0:08:50  iter: 519  total_loss: 21.71  loss_ce: 0.5144  loss_mask: 0.1893  loss_dice: 1.324  loss_ce_0: 0.5745  loss_mask_0: 0.2157  loss_dice_0: 1.532  loss_ce_1: 0.6558  loss_mask_1: 0.2019  loss_dice_1: 1.444  loss_ce_2: 0.6347  loss_mask_2: 0.1925  loss_dice_2: 1.382  loss_ce_3: 0.5859  loss_mask_3: 0.1984  loss_dice_3: 1.337  loss_ce_4: 0.5323  loss_mask_4: 0.1949  loss_dice_4: 1.343  loss_ce_5: 0.5534  loss_mask_5: 0.1902  loss_dice_5: 1.348  loss_ce_6: 0.4907  loss_mask_6: 0.1885  loss_dice_6: 1.301  loss_ce_7: 0.5167  loss_mask_7: 0.1927  loss_dice_7: 1.285  loss_ce_8: 0.5039  loss_mask_8: 0.1896  loss_dice_8: 1.305    time: 1.1054  last_time: 1.0973  data_time: 0.0879  last_data_time: 0.0614   lr: 0.0001  max_mem: 32663M
[10/08 14:56:09] d2.utils.events INFO:  eta: 0:08:28  iter: 539  total_loss: 20.58  loss_ce: 0.4725  loss_mask: 0.1774  loss_dice: 1.29  loss_ce_0: 0.5571  loss_mask_0: 0.2069  loss_dice_0: 1.516  loss_ce_1: 0.6308  loss_mask_1: 0.1859  loss_dice_1: 1.379  loss_ce_2: 0.5916  loss_mask_2: 0.1768  loss_dice_2: 1.35  loss_ce_3: 0.5311  loss_mask_3: 0.1782  loss_dice_3: 1.304  loss_ce_4: 0.4937  loss_mask_4: 0.1807  loss_dice_4: 1.285  loss_ce_5: 0.4634  loss_mask_5: 0.1774  loss_dice_5: 1.289  loss_ce_6: 0.4691  loss_mask_6: 0.1794  loss_dice_6: 1.309  loss_ce_7: 0.4871  loss_mask_7: 0.1776  loss_dice_7: 1.265  loss_ce_8: 0.4762  loss_mask_8: 0.1751  loss_dice_8: 1.309    time: 1.1068  last_time: 1.2226  data_time: 0.0842  last_data_time: 0.1306   lr: 0.0001  max_mem: 32663M
[10/08 14:56:32] d2.utils.events INFO:  eta: 0:08:07  iter: 559  total_loss: 20.33  loss_ce: 0.5226  loss_mask: 0.1926  loss_dice: 1.208  loss_ce_0: 0.6469  loss_mask_0: 0.2157  loss_dice_0: 1.439  loss_ce_1: 0.7298  loss_mask_1: 0.2032  loss_dice_1: 1.358  loss_ce_2: 0.6228  loss_mask_2: 0.1962  loss_dice_2: 1.278  loss_ce_3: 0.5719  loss_mask_3: 0.1964  loss_dice_3: 1.237  loss_ce_4: 0.5377  loss_mask_4: 0.1958  loss_dice_4: 1.255  loss_ce_5: 0.507  loss_mask_5: 0.1924  loss_dice_5: 1.253  loss_ce_6: 0.5126  loss_mask_6: 0.1903  loss_dice_6: 1.229  loss_ce_7: 0.5177  loss_mask_7: 0.1928  loss_dice_7: 1.261  loss_ce_8: 0.5265  loss_mask_8: 0.1895  loss_dice_8: 1.215    time: 1.1078  last_time: 1.1351  data_time: 0.0846  last_data_time: 0.0814   lr: 0.0001  max_mem: 32663M
[10/08 14:56:55] d2.utils.events INFO:  eta: 0:07:46  iter: 579  total_loss: 19.67  loss_ce: 0.5073  loss_mask: 0.1669  loss_dice: 1.21  loss_ce_0: 0.6014  loss_mask_0: 0.1976  loss_dice_0: 1.416  loss_ce_1: 0.6753  loss_mask_1: 0.1853  loss_dice_1: 1.362  loss_ce_2: 0.5915  loss_mask_2: 0.1729  loss_dice_2: 1.28  loss_ce_3: 0.5442  loss_mask_3: 0.168  loss_dice_3: 1.25  loss_ce_4: 0.522  loss_mask_4: 0.1673  loss_dice_4: 1.246  loss_ce_5: 0.54  loss_mask_5: 0.1659  loss_dice_5: 1.221  loss_ce_6: 0.5169  loss_mask_6: 0.1666  loss_dice_6: 1.244  loss_ce_7: 0.5202  loss_mask_7: 0.1694  loss_dice_7: 1.196  loss_ce_8: 0.5123  loss_mask_8: 0.1682  loss_dice_8: 1.219    time: 1.1086  last_time: 1.1151  data_time: 0.0839  last_data_time: 0.0783   lr: 0.0001  max_mem: 32893M
[10/08 14:57:18] d2.utils.events INFO:  eta: 0:07:24  iter: 599  total_loss: 21.35  loss_ce: 0.5452  loss_mask: 0.1722  loss_dice: 1.316  loss_ce_0: 0.595  loss_mask_0: 0.1989  loss_dice_0: 1.53  loss_ce_1: 0.6736  loss_mask_1: 0.1827  loss_dice_1: 1.441  loss_ce_2: 0.6351  loss_mask_2: 0.1795  loss_dice_2: 1.398  loss_ce_3: 0.598  loss_mask_3: 0.1762  loss_dice_3: 1.317  loss_ce_4: 0.5913  loss_mask_4: 0.179  loss_dice_4: 1.329  loss_ce_5: 0.574  loss_mask_5: 0.1759  loss_dice_5: 1.355  loss_ce_6: 0.5563  loss_mask_6: 0.1765  loss_dice_6: 1.301  loss_ce_7: 0.5337  loss_mask_7: 0.1749  loss_dice_7: 1.308  loss_ce_8: 0.5453  loss_mask_8: 0.1743  loss_dice_8: 1.309    time: 1.1100  last_time: 1.1331  data_time: 0.0843  last_data_time: 0.0948   lr: 0.0001  max_mem: 32893M
[10/08 14:57:41] d2.utils.events INFO:  eta: 0:07:02  iter: 619  total_loss: 20.57  loss_ce: 0.501  loss_mask: 0.1816  loss_dice: 1.262  loss_ce_0: 0.5898  loss_mask_0: 0.2113  loss_dice_0: 1.492  loss_ce_1: 0.6472  loss_mask_1: 0.1916  loss_dice_1: 1.353  loss_ce_2: 0.5872  loss_mask_2: 0.1887  loss_dice_2: 1.342  loss_ce_3: 0.5386  loss_mask_3: 0.1851  loss_dice_3: 1.28  loss_ce_4: 0.5231  loss_mask_4: 0.1836  loss_dice_4: 1.276  loss_ce_5: 0.5433  loss_mask_5: 0.1817  loss_dice_5: 1.307  loss_ce_6: 0.4981  loss_mask_6: 0.1838  loss_dice_6: 1.257  loss_ce_7: 0.5118  loss_mask_7: 0.1818  loss_dice_7: 1.268  loss_ce_8: 0.5225  loss_mask_8: 0.1795  loss_dice_8: 1.255    time: 1.1107  last_time: 1.0061  data_time: 0.0876  last_data_time: 0.0558   lr: 0.0001  max_mem: 32893M
[10/08 14:58:03] d2.utils.events INFO:  eta: 0:06:40  iter: 639  total_loss: 21.11  loss_ce: 0.5217  loss_mask: 0.1797  loss_dice: 1.289  loss_ce_0: 0.623  loss_mask_0: 0.2086  loss_dice_0: 1.467  loss_ce_1: 0.712  loss_mask_1: 0.186  loss_dice_1: 1.418  loss_ce_2: 0.6592  loss_mask_2: 0.181  loss_dice_2: 1.334  loss_ce_3: 0.6134  loss_mask_3: 0.1816  loss_dice_3: 1.274  loss_ce_4: 0.5476  loss_mask_4: 0.1805  loss_dice_4: 1.337  loss_ce_5: 0.5655  loss_mask_5: 0.1779  loss_dice_5: 1.286  loss_ce_6: 0.5214  loss_mask_6: 0.178  loss_dice_6: 1.284  loss_ce_7: 0.5375  loss_mask_7: 0.1791  loss_dice_7: 1.275  loss_ce_8: 0.5184  loss_mask_8: 0.1793  loss_dice_8: 1.287    time: 1.1110  last_time: 1.1013  data_time: 0.0819  last_data_time: 0.0774   lr: 0.0001  max_mem: 32893M
[10/08 14:58:26] d2.utils.events INFO:  eta: 0:06:18  iter: 659  total_loss: 20.15  loss_ce: 0.5248  loss_mask: 0.1703  loss_dice: 1.235  loss_ce_0: 0.5935  loss_mask_0: 0.2025  loss_dice_0: 1.438  loss_ce_1: 0.6962  loss_mask_1: 0.19  loss_dice_1: 1.395  loss_ce_2: 0.6016  loss_mask_2: 0.1814  loss_dice_2: 1.335  loss_ce_3: 0.5707  loss_mask_3: 0.1786  loss_dice_3: 1.26  loss_ce_4: 0.5716  loss_mask_4: 0.1782  loss_dice_4: 1.247  loss_ce_5: 0.5348  loss_mask_5: 0.1737  loss_dice_5: 1.268  loss_ce_6: 0.546  loss_mask_6: 0.1726  loss_dice_6: 1.238  loss_ce_7: 0.5132  loss_mask_7: 0.1718  loss_dice_7: 1.256  loss_ce_8: 0.5218  loss_mask_8: 0.1694  loss_dice_8: 1.261    time: 1.1117  last_time: 1.1083  data_time: 0.0872  last_data_time: 0.0884   lr: 0.0001  max_mem: 32893M
[10/08 14:58:48] d2.utils.events INFO:  eta: 0:05:56  iter: 679  total_loss: 20.63  loss_ce: 0.4923  loss_mask: 0.1705  loss_dice: 1.226  loss_ce_0: 0.6982  loss_mask_0: 0.203  loss_dice_0: 1.477  loss_ce_1: 0.6889  loss_mask_1: 0.1919  loss_dice_1: 1.376  loss_ce_2: 0.6259  loss_mask_2: 0.1815  loss_dice_2: 1.327  loss_ce_3: 0.5527  loss_mask_3: 0.1793  loss_dice_3: 1.228  loss_ce_4: 0.5228  loss_mask_4: 0.1769  loss_dice_4: 1.271  loss_ce_5: 0.5048  loss_mask_5: 0.1777  loss_dice_5: 1.276  loss_ce_6: 0.5192  loss_mask_6: 0.1697  loss_dice_6: 1.231  loss_ce_7: 0.4906  loss_mask_7: 0.1728  loss_dice_7: 1.246  loss_ce_8: 0.4811  loss_mask_8: 0.1671  loss_dice_8: 1.222    time: 1.1123  last_time: 1.1446  data_time: 0.0823  last_data_time: 0.0891   lr: 0.0001  max_mem: 32893M
[10/08 14:59:11] d2.utils.events INFO:  eta: 0:05:34  iter: 699  total_loss: 21.51  loss_ce: 0.5111  loss_mask: 0.1791  loss_dice: 1.28  loss_ce_0: 0.6428  loss_mask_0: 0.2093  loss_dice_0: 1.5  loss_ce_1: 0.6585  loss_mask_1: 0.1874  loss_dice_1: 1.413  loss_ce_2: 0.6546  loss_mask_2: 0.1794  loss_dice_2: 1.369  loss_ce_3: 0.5822  loss_mask_3: 0.1793  loss_dice_3: 1.284  loss_ce_4: 0.5404  loss_mask_4: 0.1767  loss_dice_4: 1.337  loss_ce_5: 0.5498  loss_mask_5: 0.1772  loss_dice_5: 1.308  loss_ce_6: 0.5585  loss_mask_6: 0.1749  loss_dice_6: 1.296  loss_ce_7: 0.5383  loss_mask_7: 0.176  loss_dice_7: 1.265  loss_ce_8: 0.5186  loss_mask_8: 0.1758  loss_dice_8: 1.285    time: 1.1127  last_time: 1.1772  data_time: 0.0791  last_data_time: 0.0695   lr: 0.0001  max_mem: 32893M
[10/08 14:59:34] d2.utils.events INFO:  eta: 0:05:12  iter: 719  total_loss: 21.93  loss_ce: 0.5371  loss_mask: 0.1693  loss_dice: 1.275  loss_ce_0: 0.5977  loss_mask_0: 0.1967  loss_dice_0: 1.539  loss_ce_1: 0.706  loss_mask_1: 0.1815  loss_dice_1: 1.445  loss_ce_2: 0.6164  loss_mask_2: 0.179  loss_dice_2: 1.383  loss_ce_3: 0.5941  loss_mask_3: 0.1755  loss_dice_3: 1.315  loss_ce_4: 0.5625  loss_mask_4: 0.1755  loss_dice_4: 1.329  loss_ce_5: 0.5392  loss_mask_5: 0.1711  loss_dice_5: 1.311  loss_ce_6: 0.5305  loss_mask_6: 0.1738  loss_dice_6: 1.284  loss_ce_7: 0.5296  loss_mask_7: 0.17  loss_dice_7: 1.304  loss_ce_8: 0.5224  loss_mask_8: 0.1699  loss_dice_8: 1.302    time: 1.1135  last_time: 1.2258  data_time: 0.0890  last_data_time: 0.1229   lr: 0.0001  max_mem: 32893M
[10/08 14:59:57] d2.utils.events INFO:  eta: 0:04:50  iter: 739  total_loss: 21.17  loss_ce: 0.5225  loss_mask: 0.1701  loss_dice: 1.304  loss_ce_0: 0.6686  loss_mask_0: 0.21  loss_dice_0: 1.501  loss_ce_1: 0.7061  loss_mask_1: 0.1795  loss_dice_1: 1.493  loss_ce_2: 0.6197  loss_mask_2: 0.1734  loss_dice_2: 1.447  loss_ce_3: 0.5756  loss_mask_3: 0.1731  loss_dice_3: 1.348  loss_ce_4: 0.542  loss_mask_4: 0.1724  loss_dice_4: 1.349  loss_ce_5: 0.5324  loss_mask_5: 0.1703  loss_dice_5: 1.299  loss_ce_6: 0.5419  loss_mask_6: 0.1687  loss_dice_6: 1.326  loss_ce_7: 0.5348  loss_mask_7: 0.168  loss_dice_7: 1.289  loss_ce_8: 0.5519  loss_mask_8: 0.1721  loss_dice_8: 1.297    time: 1.1145  last_time: 1.1838  data_time: 0.0860  last_data_time: 0.1053   lr: 0.0001  max_mem: 32893M
[10/08 15:00:20] d2.utils.events INFO:  eta: 0:04:28  iter: 759  total_loss: 20.6  loss_ce: 0.5222  loss_mask: 0.1663  loss_dice: 1.245  loss_ce_0: 0.6177  loss_mask_0: 0.1991  loss_dice_0: 1.447  loss_ce_1: 0.6795  loss_mask_1: 0.1748  loss_dice_1: 1.417  loss_ce_2: 0.6502  loss_mask_2: 0.1755  loss_dice_2: 1.331  loss_ce_3: 0.5607  loss_mask_3: 0.1697  loss_dice_3: 1.303  loss_ce_4: 0.5153  loss_mask_4: 0.1681  loss_dice_4: 1.286  loss_ce_5: 0.5125  loss_mask_5: 0.1671  loss_dice_5: 1.305  loss_ce_6: 0.5223  loss_mask_6: 0.1686  loss_dice_6: 1.257  loss_ce_7: 0.531  loss_mask_7: 0.1672  loss_dice_7: 1.257  loss_ce_8: 0.5342  loss_mask_8: 0.1657  loss_dice_8: 1.224    time: 1.1156  last_time: 1.1281  data_time: 0.0959  last_data_time: 0.0931   lr: 0.0001  max_mem: 32893M
[10/08 15:00:42] d2.utils.events INFO:  eta: 0:04:05  iter: 779  total_loss: 19.89  loss_ce: 0.5022  loss_mask: 0.1689  loss_dice: 1.226  loss_ce_0: 0.5893  loss_mask_0: 0.1844  loss_dice_0: 1.431  loss_ce_1: 0.6175  loss_mask_1: 0.1759  loss_dice_1: 1.401  loss_ce_2: 0.5905  loss_mask_2: 0.1732  loss_dice_2: 1.314  loss_ce_3: 0.5502  loss_mask_3: 0.1718  loss_dice_3: 1.281  loss_ce_4: 0.52  loss_mask_4: 0.1698  loss_dice_4: 1.279  loss_ce_5: 0.5098  loss_mask_5: 0.1729  loss_dice_5: 1.274  loss_ce_6: 0.4983  loss_mask_6: 0.1709  loss_dice_6: 1.223  loss_ce_7: 0.5327  loss_mask_7: 0.171  loss_dice_7: 1.257  loss_ce_8: 0.5183  loss_mask_8: 0.1693  loss_dice_8: 1.238    time: 1.1154  last_time: 1.1431  data_time: 0.0800  last_data_time: 0.0707   lr: 0.0001  max_mem: 32893M
[10/08 15:01:05] d2.utils.events INFO:  eta: 0:03:43  iter: 799  total_loss: 19.98  loss_ce: 0.4783  loss_mask: 0.1891  loss_dice: 1.232  loss_ce_0: 0.5549  loss_mask_0: 0.2111  loss_dice_0: 1.429  loss_ce_1: 0.6447  loss_mask_1: 0.2064  loss_dice_1: 1.389  loss_ce_2: 0.5552  loss_mask_2: 0.1952  loss_dice_2: 1.276  loss_ce_3: 0.5142  loss_mask_3: 0.1898  loss_dice_3: 1.257  loss_ce_4: 0.4949  loss_mask_4: 0.1875  loss_dice_4: 1.261  loss_ce_5: 0.4869  loss_mask_5: 0.1889  loss_dice_5: 1.24  loss_ce_6: 0.4433  loss_mask_6: 0.1902  loss_dice_6: 1.23  loss_ce_7: 0.467  loss_mask_7: 0.1916  loss_dice_7: 1.233  loss_ce_8: 0.4764  loss_mask_8: 0.1878  loss_dice_8: 1.237    time: 1.1152  last_time: 1.1021  data_time: 0.0781  last_data_time: 0.0750   lr: 0.0001  max_mem: 32893M
[10/08 15:01:27] d2.utils.events INFO:  eta: 0:03:21  iter: 819  total_loss: 20.66  loss_ce: 0.4968  loss_mask: 0.1681  loss_dice: 1.286  loss_ce_0: 0.5459  loss_mask_0: 0.1861  loss_dice_0: 1.5  loss_ce_1: 0.6232  loss_mask_1: 0.1768  loss_dice_1: 1.426  loss_ce_2: 0.5506  loss_mask_2: 0.1725  loss_dice_2: 1.358  loss_ce_3: 0.5057  loss_mask_3: 0.1686  loss_dice_3: 1.305  loss_ce_4: 0.4827  loss_mask_4: 0.1693  loss_dice_4: 1.323  loss_ce_5: 0.5116  loss_mask_5: 0.1667  loss_dice_5: 1.292  loss_ce_6: 0.4479  loss_mask_6: 0.1682  loss_dice_6: 1.295  loss_ce_7: 0.493  loss_mask_7: 0.1688  loss_dice_7: 1.277  loss_ce_8: 0.4633  loss_mask_8: 0.1689  loss_dice_8: 1.265    time: 1.1158  last_time: 1.2024  data_time: 0.0887  last_data_time: 0.1226   lr: 0.0001  max_mem: 32893M
[10/08 15:01:50] d2.utils.events INFO:  eta: 0:02:58  iter: 839  total_loss: 20.54  loss_ce: 0.5055  loss_mask: 0.1639  loss_dice: 1.265  loss_ce_0: 0.6017  loss_mask_0: 0.1816  loss_dice_0: 1.486  loss_ce_1: 0.6827  loss_mask_1: 0.1709  loss_dice_1: 1.45  loss_ce_2: 0.6279  loss_mask_2: 0.1679  loss_dice_2: 1.369  loss_ce_3: 0.5676  loss_mask_3: 0.1698  loss_dice_3: 1.305  loss_ce_4: 0.5531  loss_mask_4: 0.1701  loss_dice_4: 1.271  loss_ce_5: 0.5347  loss_mask_5: 0.1696  loss_dice_5: 1.289  loss_ce_6: 0.5492  loss_mask_6: 0.1682  loss_dice_6: 1.236  loss_ce_7: 0.5577  loss_mask_7: 0.1664  loss_dice_7: 1.224  loss_ce_8: 0.503  loss_mask_8: 0.1661  loss_dice_8: 1.248    time: 1.1160  last_time: 1.1847  data_time: 0.0843  last_data_time: 0.0892   lr: 0.0001  max_mem: 32893M
[10/08 15:02:13] d2.utils.events INFO:  eta: 0:02:36  iter: 859  total_loss: 20.33  loss_ce: 0.4854  loss_mask: 0.1771  loss_dice: 1.25  loss_ce_0: 0.5603  loss_mask_0: 0.1954  loss_dice_0: 1.515  loss_ce_1: 0.6336  loss_mask_1: 0.185  loss_dice_1: 1.349  loss_ce_2: 0.5669  loss_mask_2: 0.1796  loss_dice_2: 1.326  loss_ce_3: 0.5579  loss_mask_3: 0.1774  loss_dice_3: 1.259  loss_ce_4: 0.5008  loss_mask_4: 0.1797  loss_dice_4: 1.309  loss_ce_5: 0.5335  loss_mask_5: 0.1793  loss_dice_5: 1.265  loss_ce_6: 0.504  loss_mask_6: 0.1736  loss_dice_6: 1.212  loss_ce_7: 0.467  loss_mask_7: 0.1769  loss_dice_7: 1.217  loss_ce_8: 0.4874  loss_mask_8: 0.1722  loss_dice_8: 1.219    time: 1.1164  last_time: 1.1065  data_time: 0.0861  last_data_time: 0.0793   lr: 0.0001  max_mem: 32893M
[10/08 15:02:35] d2.utils.events INFO:  eta: 0:02:14  iter: 879  total_loss: 20.26  loss_ce: 0.4749  loss_mask: 0.1659  loss_dice: 1.243  loss_ce_0: 0.6051  loss_mask_0: 0.1971  loss_dice_0: 1.443  loss_ce_1: 0.6448  loss_mask_1: 0.1837  loss_dice_1: 1.382  loss_ce_2: 0.6199  loss_mask_2: 0.1764  loss_dice_2: 1.277  loss_ce_3: 0.5536  loss_mask_3: 0.1718  loss_dice_3: 1.255  loss_ce_4: 0.5277  loss_mask_4: 0.1701  loss_dice_4: 1.257  loss_ce_5: 0.5198  loss_mask_5: 0.171  loss_dice_5: 1.263  loss_ce_6: 0.4752  loss_mask_6: 0.1701  loss_dice_6: 1.25  loss_ce_7: 0.4857  loss_mask_7: 0.1689  loss_dice_7: 1.25  loss_ce_8: 0.5024  loss_mask_8: 0.1667  loss_dice_8: 1.229    time: 1.1164  last_time: 1.1649  data_time: 0.0839  last_data_time: 0.0820   lr: 0.0001  max_mem: 32893M
[10/08 15:02:58] d2.utils.events INFO:  eta: 0:01:51  iter: 899  total_loss: 20.4  loss_ce: 0.4828  loss_mask: 0.179  loss_dice: 1.236  loss_ce_0: 0.5593  loss_mask_0: 0.2092  loss_dice_0: 1.51  loss_ce_1: 0.6309  loss_mask_1: 0.1892  loss_dice_1: 1.409  loss_ce_2: 0.6118  loss_mask_2: 0.1759  loss_dice_2: 1.337  loss_ce_3: 0.5265  loss_mask_3: 0.18  loss_dice_3: 1.289  loss_ce_4: 0.5372  loss_mask_4: 0.1804  loss_dice_4: 1.29  loss_ce_5: 0.541  loss_mask_5: 0.1806  loss_dice_5: 1.28  loss_ce_6: 0.4722  loss_mask_6: 0.1745  loss_dice_6: 1.267  loss_ce_7: 0.5327  loss_mask_7: 0.1759  loss_dice_7: 1.222  loss_ce_8: 0.4881  loss_mask_8: 0.1805  loss_dice_8: 1.287    time: 1.1169  last_time: 1.1323  data_time: 0.0871  last_data_time: 0.0713   lr: 0.0001  max_mem: 32893M
[10/08 15:03:21] d2.utils.events INFO:  eta: 0:01:29  iter: 919  total_loss: 20.15  loss_ce: 0.4758  loss_mask: 0.1747  loss_dice: 1.249  loss_ce_0: 0.5441  loss_mask_0: 0.2016  loss_dice_0: 1.539  loss_ce_1: 0.581  loss_mask_1: 0.1878  loss_dice_1: 1.423  loss_ce_2: 0.5913  loss_mask_2: 0.1795  loss_dice_2: 1.31  loss_ce_3: 0.5551  loss_mask_3: 0.178  loss_dice_3: 1.276  loss_ce_4: 0.4966  loss_mask_4: 0.1781  loss_dice_4: 1.315  loss_ce_5: 0.4947  loss_mask_5: 0.1792  loss_dice_5: 1.253  loss_ce_6: 0.4837  loss_mask_6: 0.1759  loss_dice_6: 1.248  loss_ce_7: 0.4943  loss_mask_7: 0.1741  loss_dice_7: 1.264  loss_ce_8: 0.5024  loss_mask_8: 0.1749  loss_dice_8: 1.238    time: 1.1172  last_time: 1.1050  data_time: 0.0866  last_data_time: 0.0682   lr: 0.0001  max_mem: 32893M
[10/08 15:03:44] d2.utils.events INFO:  eta: 0:01:07  iter: 939  total_loss: 20.46  loss_ce: 0.5249  loss_mask: 0.1652  loss_dice: 1.228  loss_ce_0: 0.6156  loss_mask_0: 0.1931  loss_dice_0: 1.535  loss_ce_1: 0.681  loss_mask_1: 0.1758  loss_dice_1: 1.396  loss_ce_2: 0.6327  loss_mask_2: 0.1684  loss_dice_2: 1.33  loss_ce_3: 0.5904  loss_mask_3: 0.1689  loss_dice_3: 1.291  loss_ce_4: 0.5291  loss_mask_4: 0.17  loss_dice_4: 1.272  loss_ce_5: 0.5228  loss_mask_5: 0.1692  loss_dice_5: 1.236  loss_ce_6: 0.5223  loss_mask_6: 0.1663  loss_dice_6: 1.252  loss_ce_7: 0.5013  loss_mask_7: 0.1654  loss_dice_7: 1.258  loss_ce_8: 0.5214  loss_mask_8: 0.1653  loss_dice_8: 1.227    time: 1.1178  last_time: 1.0498  data_time: 0.0879  last_data_time: 0.0895   lr: 0.0001  max_mem: 32893M
[10/08 15:04:06] d2.utils.events INFO:  eta: 0:00:44  iter: 959  total_loss: 21.24  loss_ce: 0.5275  loss_mask: 0.1764  loss_dice: 1.277  loss_ce_0: 0.5934  loss_mask_0: 0.2035  loss_dice_0: 1.536  loss_ce_1: 0.6699  loss_mask_1: 0.1826  loss_dice_1: 1.454  loss_ce_2: 0.6075  loss_mask_2: 0.1768  loss_dice_2: 1.377  loss_ce_3: 0.5381  loss_mask_3: 0.1759  loss_dice_3: 1.355  loss_ce_4: 0.5294  loss_mask_4: 0.1771  loss_dice_4: 1.327  loss_ce_5: 0.499  loss_mask_5: 0.1757  loss_dice_5: 1.315  loss_ce_6: 0.4925  loss_mask_6: 0.1761  loss_dice_6: 1.302  loss_ce_7: 0.5129  loss_mask_7: 0.1767  loss_dice_7: 1.293  loss_ce_8: 0.5218  loss_mask_8: 0.1744  loss_dice_8: 1.316    time: 1.1183  last_time: 1.1030  data_time: 0.0851  last_data_time: 0.0795   lr: 0.0001  max_mem: 32893M
[10/08 15:04:29] d2.utils.events INFO:  eta: 0:00:22  iter: 979  total_loss: 20.06  loss_ce: 0.4651  loss_mask: 0.1761  loss_dice: 1.221  loss_ce_0: 0.5182  loss_mask_0: 0.2053  loss_dice_0: 1.424  loss_ce_1: 0.6156  loss_mask_1: 0.1828  loss_dice_1: 1.367  loss_ce_2: 0.587  loss_mask_2: 0.1762  loss_dice_2: 1.293  loss_ce_3: 0.5445  loss_mask_3: 0.1749  loss_dice_3: 1.247  loss_ce_4: 0.5059  loss_mask_4: 0.1731  loss_dice_4: 1.25  loss_ce_5: 0.503  loss_mask_5: 0.1688  loss_dice_5: 1.219  loss_ce_6: 0.506  loss_mask_6: 0.1785  loss_dice_6: 1.23  loss_ce_7: 0.4989  loss_mask_7: 0.1741  loss_dice_7: 1.245  loss_ce_8: 0.4783  loss_mask_8: 0.1774  loss_dice_8: 1.218    time: 1.1186  last_time: 1.2132  data_time: 0.0883  last_data_time: 0.0858   lr: 0.0001  max_mem: 32893M
[10/08 15:04:52] fvcore.common.checkpoint INFO: Saving checkpoint to /tsi/hi-paris/GB/segmentation/results/In_vocab/r50_008_1000_19_a_decoder/model_final.pth
[10/08 15:04:55] d2.utils.events INFO:  eta: 0:00:00  iter: 999  total_loss: 20.02  loss_ce: 0.4306  loss_mask: 0.1697  loss_dice: 1.274  loss_ce_0: 0.5352  loss_mask_0: 0.1944  loss_dice_0: 1.498  loss_ce_1: 0.6086  loss_mask_1: 0.1813  loss_dice_1: 1.447  loss_ce_2: 0.5527  loss_mask_2: 0.1705  loss_dice_2: 1.417  loss_ce_3: 0.473  loss_mask_3: 0.1732  loss_dice_3: 1.33  loss_ce_4: 0.473  loss_mask_4: 0.1696  loss_dice_4: 1.352  loss_ce_5: 0.482  loss_mask_5: 0.1704  loss_dice_5: 1.31  loss_ce_6: 0.4577  loss_mask_6: 0.1713  loss_dice_6: 1.32  loss_ce_7: 0.4375  loss_mask_7: 0.1716  loss_dice_7: 1.314  loss_ce_8: 0.4252  loss_mask_8: 0.1712  loss_dice_8: 1.317    time: 1.1190  last_time: 1.1635  data_time: 0.0846  last_data_time: 0.0971   lr: 0.0001  max_mem: 32893M
[10/08 15:04:56] d2.engine.hooks INFO: Overall training speed: 998 iterations in 0:18:36 (1.1190 s / it)
[10/08 15:04:56] d2.engine.hooks INFO: Total training time: 0:18:43 (0:00:06 on hooks)
[10/08 15:04:56] fcclip.data.datasets.register_cityscapes_panoptic INFO: 3 cities found in 'datasets/cityscapes/leftImg8bit/val'.
[10/08 15:04:56] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(1024, 1024), max_size=2560, sample_style='choice')]
[10/08 15:04:56] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/08 15:04:56] d2.data.common INFO: Serializing 500 elements to byte tensors and concatenating them all ...
[10/08 15:04:56] d2.data.common INFO: Serialized dataset takes 0.74 MiB
[10/08 15:04:56] d2.evaluation.evaluator INFO: Start inference on 500 batches
[10/08 15:11:02] detectron2 INFO: Rank of current process: 0. World size: 1
[10/08 15:11:04] detectron2 INFO: Environment info:
-------------------------------  -------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.8.19 | packaged by conda-forge | (default, Mar 20 2024, 12:47:35) [GCC 12.3.0]
numpy                            1.24.4
detectron2                       0.6 @/home/ids/gbrison/segmentation/segmentation/detectron2/detectron2
Compiler                         GCC 11.4
CUDA compiler                    CUDA 12.5
detectron2 arch flags            8.9
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.0+cu121 @/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0,1                          NVIDIA L40S (arch=8.9)
Driver version                   555.42.06
CUDA_HOME                        /usr/local/cuda
Pillow                           9.4.0
torchvision                      0.19.0+cu121 @/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torchvision
torchvision arch flags           5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.10.0
-------------------------------  -------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX512
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.0, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/08 15:11:04] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco/panoptic-segmentation/fcclip/r50_exp_1000_a_decoder.yaml', dist_url='tcp://127.0.0.1:51163', eval_only=False, machine_rank=0, num_gpus=1, num_machines=1, opts=[], resume=False)
[10/08 15:11:04] detectron2 INFO: Contents of args.config_file=configs/coco/panoptic-segmentation/fcclip/r50_exp_1000_a_decoder.yaml:
_BASE_: ./fcclip_convnext_large_eval_ade20k_r50.yaml

INPUT:
  MIN_SIZE_TEST: 1024
  MAX_SIZE_TEST: 2560

MODEL:
  SEM_SEG_HEAD:
    NUM_CLASSES: 19
  WEIGHTS: /tsi/hi-paris/GB/segmentation/models/fcclip_cocopan_r50.pth
OUTPUT_DIR: /tsi/hi-paris/GB/segmentation/results/In_vocab/r50_008_1000_19_a_decoder
DATASETS:
  TRAIN: ("openvocab_cityscapes_fine_panoptic_train",)
  TEST: ("openvocab_cityscapes_fine_panoptic_val",)
SOLVER:
  IMS_PER_BATCH: 8
  MAX_ITER: 1000
TEST:
  EVAL_PERIOD: 1000


[10/08 15:11:04] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - openvocab_cityscapes_fine_panoptic_val
  TRAIN:
  - openvocab_cityscapes_fine_panoptic_train
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: false
  CROP:
    ENABLED: false
    MINIMUM_INST_AREA: 1
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  DATASET_MAPPER_NAME: coco_panoptic_lsj
  FORMAT: RGB
  IMAGE_SIZE: 1024
  MASK_FORMAT: polygon
  MAX_SCALE: 2.0
  MAX_SIZE_TEST: 2560
  MAX_SIZE_TRAIN: 1333
  MIN_SCALE: 0.1
  MIN_SIZE_TEST: 1024
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: -1
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 0
    NAME: CLIP
  DEVICE: cuda
  FC_CLIP:
    CLIP_MODEL_NAME: RN50
    CLIP_PRETRAINED_WEIGHTS: openai
    EMBED_DIM: 1024
    ENSEMBLE_ON_VALID_MASK: true
    GEOMETRIC_ENSEMBLE_ALPHA: 0.4
    GEOMETRIC_ENSEMBLE_BETA: 0.8
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_FORMER:
    CLASS_WEIGHT: 2.0
    DEC_LAYERS: 10
    DEEP_SUPERVISION: true
    DICE_WEIGHT: 5.0
    DIM_FEEDFORWARD: 2048
    DROPOUT: 0.0
    ENC_LAYERS: 0
    ENFORCE_INPUT_PROJ: false
    HIDDEN_DIM: 256
    IMPORTANCE_SAMPLE_RATIO: 0.75
    MASK_WEIGHT: 5.0
    NHEADS: 8
    NO_OBJECT_WEIGHT: 0.1
    NUM_OBJECT_QUERIES: 250
    OVERSAMPLE_RATIO: 3.0
    PRE_NORM: false
    SIZE_DIVISIBILITY: 32
    TEST:
      INSTANCE_ON: true
      OBJECT_MASK_THRESHOLD: 0.0
      OVERLAP_THRESHOLD: 0.8
      PANOPTIC_ON: true
      SEMANTIC_ON: true
      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: false
    TRAIN_NUM_POINTS: 12544
    TRANSFORMER_DECODER_NAME: MultiScaleMaskedTransformerDecoder
    TRANSFORMER_IN_FEATURE: multi_scale_pixel_decoder
  MASK_ON: false
  META_ARCHITECTURE: FCCLIP
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 122.7709383
  - 116.7460125
  - 104.09373615
  PIXEL_STD:
  - 68.5005327
  - 66.6321579
  - 70.32316305
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 1
    - 1
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: basic
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 256
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES:
    - res3
    - res4
    - res5
    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8
    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    MASK_DIM: 256
    NAME: FCCLIPHead
    NORM: GN
    NUM_CLASSES: 19
    PIXEL_DECODER_NAME: MSDeformAttnPixelDecoder
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    TRANSFORMER_ENC_LAYERS: 6
    USE_DEPTHWISE_SEPARABLE_CONV: false
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    USE_CHECKPOINT: false
    WINDOW_SIZE: 7
  WEIGHTS: /tsi/hi-paris/GB/segmentation/models/fcclip_cocopan_r50.pth
OUTPUT_DIR: /tsi/hi-paris/GB/segmentation/results/In_vocab/r50_008_1000_19_a_decoder
SEED: -1
SOLVER:
  AMP:
    ENABLED: true
  BACKBONE_MULTIPLIER: 0.1
  BASE_LR: 0.0001
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 5000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.01
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 8
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 1000
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 327778
  - 355092
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 1000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[10/08 15:11:04] detectron2 INFO: Full config saved to /tsi/hi-paris/GB/segmentation/results/In_vocab/r50_008_1000_19_a_decoder/config.yaml
[10/08 15:11:04] d2.utils.env INFO: Using a generated random seed 5278341
[10/08 15:11:08] d2.engine.defaults INFO: Model:
FCCLIP(
  (backbone): CLIP(
    (clip_model): CLIP(
      (visual): ModifiedResNet(
        (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
        (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        (layer1): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
            (downsample): Sequential(
              (-1): AvgPool2d(kernel_size=1, stride=1, padding=0)
              (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
        )
        (layer2): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
            (downsample): Sequential(
              (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (3): Bottleneck(
            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
        )
        (layer3): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
            (downsample): Sequential(
              (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
              (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (3): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (4): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (5): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
        )
        (layer4): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
            (downsample): Sequential(
              (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
              (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
        )
        (attnpool): AttentionPool2d(
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (c_proj): Linear(in_features=2048, out_features=1024, bias=True)
        )
      )
      (transformer): Transformer(
        (resblocks): ModuleList(
          (0-11): 12 x ResidualAttentionBlock(
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ls_2): Identity()
          )
        )
      )
      (token_embedding): Embedding(49408, 512)
      (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
  )
  (sem_seg_head): FCCLIPHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0-8): 9 x SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0-8): 9 x CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0-8): 9 x FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(250, 256)
      (query_embed): Embedding(250, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0-2): 3 x Sequential()
      )
      (mask_embed): MLP(
        (layers): ModuleList(
          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (mask_pooling): MaskPooling()
      (_mask_pooling_proj): Sequential(
        (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
      )
      (class_embed): MLP(
        (layers): ModuleList(
          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=1024, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 19
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
  (conv1): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv2): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU()
  (mask_pooling): MaskPooling()
  (decoder_adapter): DecoderAdapter(
    (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
    (conv2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    (norm1): LayerNorm((64, 1, 1), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((256, 1, 1), eps=1e-05, elementwise_affine=True)
    (relu): ReLU()
  )
  (void_embedding): Embedding(1, 1024)
)
[10/08 15:11:08] fcclip.data.dataset_mappers.mask_former_semantic_dataset_mapper INFO: [MaskFormerPanopticDatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(800,), max_size=1333, sample_style='choice'), RandomFlip()]
[10/08 15:11:08] fcclip.data.datasets.register_cityscapes_panoptic INFO: 18 cities found in 'datasets/cityscapes/leftImg8bit/train'.
[10/08 15:11:08] d2.data.build INFO: Using training sampler TrainingSampler
[10/08 15:11:08] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/08 15:11:08] d2.data.common INFO: Serializing 2975 elements to byte tensors and concatenating them all ...
[10/08 15:11:08] d2.data.common INFO: Serialized dataset takes 4.12 MiB
[10/08 15:11:08] d2.data.build INFO: Making batched data loader with batch_size=8
[10/08 15:11:08] d2.solver.build WARNING: SOLVER.STEPS contains values larger than SOLVER.MAX_ITER. These values will be ignored.
[10/08 15:11:08] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /tsi/hi-paris/GB/segmentation/models/fcclip_cocopan_r50.pth ...
[10/08 15:11:08] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /tsi/hi-paris/GB/segmentation/models/fcclip_cocopan_r50.pth ...
[10/08 15:11:08] fvcore.common.checkpoint WARNING: Skip loading parameter 'criterion.empty_weight' to the model due to incompatible shapes: (134,) in the checkpoint but (20,) in the model! You might want to double check if this is expected.
[10/08 15:11:08] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.clip_model.ln_final.{bias, weight}[0m
[34mbackbone.clip_model.token_embedding.weight[0m
[34mbackbone.clip_model.transformer.resblocks.0.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.attnpool.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.attnpool.k_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.attnpool.positional_embedding[0m
[34mbackbone.clip_model.visual.attnpool.q_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.attnpool.v_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.conv1.weight[0m
[34mbackbone.clip_model.visual.conv2.weight[0m
[34mbackbone.clip_model.visual.conv3.weight[0m
[34mbackbone.clip_model.visual.layer1.0.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.0.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.0.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.0.conv1.weight[0m
[34mbackbone.clip_model.visual.layer1.0.conv2.weight[0m
[34mbackbone.clip_model.visual.layer1.0.conv3.weight[0m
[34mbackbone.clip_model.visual.layer1.0.downsample.0.weight[0m
[34mbackbone.clip_model.visual.layer1.0.downsample.1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.1.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.1.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.1.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.1.conv1.weight[0m
[34mbackbone.clip_model.visual.layer1.1.conv2.weight[0m
[34mbackbone.clip_model.visual.layer1.1.conv3.weight[0m
[34mbackbone.clip_model.visual.layer1.2.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.2.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.2.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.2.conv1.weight[0m
[34mbackbone.clip_model.visual.layer1.2.conv2.weight[0m
[34mbackbone.clip_model.visual.layer1.2.conv3.weight[0m
[34mbackbone.clip_model.visual.layer2.0.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.0.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.0.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.0.conv1.weight[0m
[34mbackbone.clip_model.visual.layer2.0.conv2.weight[0m
[34mbackbone.clip_model.visual.layer2.0.conv3.weight[0m
[34mbackbone.clip_model.visual.layer2.0.downsample.0.weight[0m
[34mbackbone.clip_model.visual.layer2.0.downsample.1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.1.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.1.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.1.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.1.conv1.weight[0m
[34mbackbone.clip_model.visual.layer2.1.conv2.weight[0m
[34mbackbone.clip_model.visual.layer2.1.conv3.weight[0m
[34mbackbone.clip_model.visual.layer2.2.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.2.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.2.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.2.conv1.weight[0m
[34mbackbone.clip_model.visual.layer2.2.conv2.weight[0m
[34mbackbone.clip_model.visual.layer2.2.conv3.weight[0m
[34mbackbone.clip_model.visual.layer2.3.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.3.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.3.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.3.conv1.weight[0m
[34mbackbone.clip_model.visual.layer2.3.conv2.weight[0m
[34mbackbone.clip_model.visual.layer2.3.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.0.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.0.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.0.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.0.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.0.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.0.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.0.downsample.0.weight[0m
[34mbackbone.clip_model.visual.layer3.0.downsample.1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.1.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.1.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.1.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.1.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.1.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.1.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.2.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.2.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.2.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.2.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.2.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.2.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.3.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.3.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.3.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.3.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.3.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.3.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.4.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.4.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.4.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.4.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.4.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.4.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.5.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.5.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.5.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.5.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.5.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.5.conv3.weight[0m
[34mbackbone.clip_model.visual.layer4.0.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.0.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.0.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.0.conv1.weight[0m
[34mbackbone.clip_model.visual.layer4.0.conv2.weight[0m
[34mbackbone.clip_model.visual.layer4.0.conv3.weight[0m
[34mbackbone.clip_model.visual.layer4.0.downsample.0.weight[0m
[34mbackbone.clip_model.visual.layer4.0.downsample.1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.1.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.1.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.1.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.1.conv1.weight[0m
[34mbackbone.clip_model.visual.layer4.1.conv2.weight[0m
[34mbackbone.clip_model.visual.layer4.1.conv3.weight[0m
[34mbackbone.clip_model.visual.layer4.2.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.2.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.2.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.2.conv1.weight[0m
[34mbackbone.clip_model.visual.layer4.2.conv2.weight[0m
[34mbackbone.clip_model.visual.layer4.2.conv3.weight[0m
[34mbackbone.clip_model.{logit_scale, positional_embedding, text_projection}[0m
[34mbn1.{bias, running_mean, running_var, weight}[0m
[34mbn2.{bias, running_mean, running_var, weight}[0m
[34mconv1.{bias, weight}[0m
[34mconv2.{bias, weight}[0m
[34mcriterion.empty_weight[0m
[34mdecoder_adapter.conv1.{bias, weight}[0m
[34mdecoder_adapter.conv2.{bias, weight}[0m
[34mdecoder_adapter.norm1.{bias, weight}[0m
[34mdecoder_adapter.norm2.{bias, weight}[0m
[10/08 15:11:08] d2.engine.train_loop INFO: Starting training from iteration 0
[10/08 15:11:33] d2.utils.events INFO:  eta: 0:17:15  iter: 19  total_loss: 33.31  loss_ce: 1.29  loss_mask: 0.2964  loss_dice: 1.481  loss_ce_0: 1.521  loss_mask_0: 0.3252  loss_dice_0: 1.815  loss_ce_1: 1.533  loss_mask_1: 0.3115  loss_dice_1: 1.662  loss_ce_2: 1.47  loss_mask_2: 0.2996  loss_dice_2: 1.591  loss_ce_3: 1.437  loss_mask_3: 0.3063  loss_dice_3: 1.545  loss_ce_4: 1.314  loss_mask_4: 0.2989  loss_dice_4: 1.538  loss_ce_5: 1.334  loss_mask_5: 0.3052  loss_dice_5: 1.514  loss_ce_6: 1.314  loss_mask_6: 0.3028  loss_dice_6: 1.509  loss_ce_7: 1.305  loss_mask_7: 0.302  loss_dice_7: 1.519  loss_ce_8: 1.265  loss_mask_8: 0.2983  loss_dice_8: 1.503    time: 1.0628  last_time: 1.0613  data_time: 0.1054  last_data_time: 0.0641   lr: 0.0001  max_mem: 31121M
[10/08 15:11:54] d2.utils.events INFO:  eta: 0:16:38  iter: 39  total_loss: 25.76  loss_ce: 0.7829  loss_mask: 0.2222  loss_dice: 1.407  loss_ce_0: 0.8333  loss_mask_0: 0.2531  loss_dice_0: 1.771  loss_ce_1: 0.9522  loss_mask_1: 0.2505  loss_dice_1: 1.597  loss_ce_2: 0.8626  loss_mask_2: 0.235  loss_dice_2: 1.542  loss_ce_3: 0.8334  loss_mask_3: 0.2301  loss_dice_3: 1.415  loss_ce_4: 0.8016  loss_mask_4: 0.2284  loss_dice_4: 1.487  loss_ce_5: 0.8126  loss_mask_5: 0.233  loss_dice_5: 1.462  loss_ce_6: 0.8339  loss_mask_6: 0.2284  loss_dice_6: 1.408  loss_ce_7: 0.7794  loss_mask_7: 0.2307  loss_dice_7: 1.429  loss_ce_8: 0.751  loss_mask_8: 0.2255  loss_dice_8: 1.449    time: 1.0519  last_time: 1.0155  data_time: 0.0684  last_data_time: 0.0858   lr: 0.0001  max_mem: 31680M
[10/08 15:12:15] d2.utils.events INFO:  eta: 0:16:20  iter: 59  total_loss: 23.85  loss_ce: 0.6615  loss_mask: 0.2203  loss_dice: 1.445  loss_ce_0: 0.6765  loss_mask_0: 0.2561  loss_dice_0: 1.703  loss_ce_1: 0.7924  loss_mask_1: 0.242  loss_dice_1: 1.571  loss_ce_2: 0.7414  loss_mask_2: 0.2304  loss_dice_2: 1.46  loss_ce_3: 0.6821  loss_mask_3: 0.2228  loss_dice_3: 1.431  loss_ce_4: 0.6173  loss_mask_4: 0.2228  loss_dice_4: 1.45  loss_ce_5: 0.6597  loss_mask_5: 0.2216  loss_dice_5: 1.447  loss_ce_6: 0.6392  loss_mask_6: 0.2158  loss_dice_6: 1.397  loss_ce_7: 0.6219  loss_mask_7: 0.2194  loss_dice_7: 1.402  loss_ce_8: 0.6635  loss_mask_8: 0.215  loss_dice_8: 1.406    time: 1.0505  last_time: 1.1002  data_time: 0.0677  last_data_time: 0.0796   lr: 0.0001  max_mem: 31680M
[10/08 15:12:35] d2.utils.events INFO:  eta: 0:15:58  iter: 79  total_loss: 22.14  loss_ce: 0.5997  loss_mask: 0.2032  loss_dice: 1.327  loss_ce_0: 0.7158  loss_mask_0: 0.2261  loss_dice_0: 1.611  loss_ce_1: 0.7641  loss_mask_1: 0.2138  loss_dice_1: 1.513  loss_ce_2: 0.7066  loss_mask_2: 0.2103  loss_dice_2: 1.433  loss_ce_3: 0.6691  loss_mask_3: 0.2091  loss_dice_3: 1.381  loss_ce_4: 0.6351  loss_mask_4: 0.2071  loss_dice_4: 1.357  loss_ce_5: 0.6279  loss_mask_5: 0.2085  loss_dice_5: 1.327  loss_ce_6: 0.608  loss_mask_6: 0.2065  loss_dice_6: 1.294  loss_ce_7: 0.6032  loss_mask_7: 0.205  loss_dice_7: 1.333  loss_ce_8: 0.6028  loss_mask_8: 0.2041  loss_dice_8: 1.333    time: 1.0482  last_time: 1.0635  data_time: 0.0673  last_data_time: 0.0683   lr: 0.0001  max_mem: 31716M
[10/08 15:12:57] d2.utils.events INFO:  eta: 0:15:41  iter: 99  total_loss: 23.53  loss_ce: 0.6589  loss_mask: 0.2025  loss_dice: 1.388  loss_ce_0: 0.7339  loss_mask_0: 0.2278  loss_dice_0: 1.656  loss_ce_1: 0.8633  loss_mask_1: 0.2126  loss_dice_1: 1.555  loss_ce_2: 0.8146  loss_mask_2: 0.207  loss_dice_2: 1.516  loss_ce_3: 0.716  loss_mask_3: 0.2028  loss_dice_3: 1.394  loss_ce_4: 0.6875  loss_mask_4: 0.2031  loss_dice_4: 1.435  loss_ce_5: 0.6559  loss_mask_5: 0.2023  loss_dice_5: 1.438  loss_ce_6: 0.6313  loss_mask_6: 0.2055  loss_dice_6: 1.377  loss_ce_7: 0.667  loss_mask_7: 0.2065  loss_dice_7: 1.363  loss_ce_8: 0.6276  loss_mask_8: 0.203  loss_dice_8: 1.388    time: 1.0532  last_time: 1.0443  data_time: 0.0727  last_data_time: 0.0702   lr: 0.0001  max_mem: 31779M
[10/08 15:13:18] d2.utils.events INFO:  eta: 0:15:22  iter: 119  total_loss: 22.82  loss_ce: 0.6475  loss_mask: 0.1912  loss_dice: 1.3  loss_ce_0: 0.7223  loss_mask_0: 0.224  loss_dice_0: 1.62  loss_ce_1: 0.8509  loss_mask_1: 0.2059  loss_dice_1: 1.495  loss_ce_2: 0.7461  loss_mask_2: 0.1921  loss_dice_2: 1.364  loss_ce_3: 0.6883  loss_mask_3: 0.1943  loss_dice_3: 1.365  loss_ce_4: 0.6494  loss_mask_4: 0.1986  loss_dice_4: 1.368  loss_ce_5: 0.6384  loss_mask_5: 0.1936  loss_dice_5: 1.319  loss_ce_6: 0.6544  loss_mask_6: 0.1894  loss_dice_6: 1.296  loss_ce_7: 0.6495  loss_mask_7: 0.195  loss_dice_7: 1.303  loss_ce_8: 0.6411  loss_mask_8: 0.1924  loss_dice_8: 1.329    time: 1.0564  last_time: 1.0771  data_time: 0.0720  last_data_time: 0.0703   lr: 0.0001  max_mem: 31779M
[10/08 15:13:40] d2.utils.events INFO:  eta: 0:15:04  iter: 139  total_loss: 22.82  loss_ce: 0.616  loss_mask: 0.195  loss_dice: 1.368  loss_ce_0: 0.6859  loss_mask_0: 0.2287  loss_dice_0: 1.586  loss_ce_1: 0.7928  loss_mask_1: 0.211  loss_dice_1: 1.474  loss_ce_2: 0.7241  loss_mask_2: 0.1967  loss_dice_2: 1.455  loss_ce_3: 0.6726  loss_mask_3: 0.1935  loss_dice_3: 1.382  loss_ce_4: 0.628  loss_mask_4: 0.1947  loss_dice_4: 1.413  loss_ce_5: 0.6228  loss_mask_5: 0.1922  loss_dice_5: 1.364  loss_ce_6: 0.632  loss_mask_6: 0.1924  loss_dice_6: 1.395  loss_ce_7: 0.6155  loss_mask_7: 0.1896  loss_dice_7: 1.348  loss_ce_8: 0.6178  loss_mask_8: 0.1897  loss_dice_8: 1.351    time: 1.0591  last_time: 1.0203  data_time: 0.0748  last_data_time: 0.0725   lr: 0.0001  max_mem: 31779M
[10/08 15:14:01] d2.utils.events INFO:  eta: 0:14:43  iter: 159  total_loss: 21.27  loss_ce: 0.5686  loss_mask: 0.1974  loss_dice: 1.235  loss_ce_0: 0.6445  loss_mask_0: 0.2262  loss_dice_0: 1.489  loss_ce_1: 0.7545  loss_mask_1: 0.2032  loss_dice_1: 1.444  loss_ce_2: 0.6993  loss_mask_2: 0.1934  loss_dice_2: 1.357  loss_ce_3: 0.6217  loss_mask_3: 0.2008  loss_dice_3: 1.315  loss_ce_4: 0.6152  loss_mask_4: 0.1968  loss_dice_4: 1.296  loss_ce_5: 0.585  loss_mask_5: 0.1974  loss_dice_5: 1.282  loss_ce_6: 0.5511  loss_mask_6: 0.1965  loss_dice_6: 1.266  loss_ce_7: 0.5902  loss_mask_7: 0.1967  loss_dice_7: 1.277  loss_ce_8: 0.5773  loss_mask_8: 0.1968  loss_dice_8: 1.237    time: 1.0601  last_time: 1.1389  data_time: 0.0697  last_data_time: 0.0638   lr: 0.0001  max_mem: 32651M
[10/08 15:14:23] d2.utils.events INFO:  eta: 0:14:23  iter: 179  total_loss: 22.13  loss_ce: 0.593  loss_mask: 0.1936  loss_dice: 1.286  loss_ce_0: 0.6599  loss_mask_0: 0.2266  loss_dice_0: 1.529  loss_ce_1: 0.7654  loss_mask_1: 0.214  loss_dice_1: 1.454  loss_ce_2: 0.7017  loss_mask_2: 0.202  loss_dice_2: 1.366  loss_ce_3: 0.6746  loss_mask_3: 0.2041  loss_dice_3: 1.314  loss_ce_4: 0.6389  loss_mask_4: 0.2016  loss_dice_4: 1.319  loss_ce_5: 0.6038  loss_mask_5: 0.1966  loss_dice_5: 1.302  loss_ce_6: 0.6123  loss_mask_6: 0.1938  loss_dice_6: 1.284  loss_ce_7: 0.601  loss_mask_7: 0.1944  loss_dice_7: 1.294  loss_ce_8: 0.6328  loss_mask_8: 0.1942  loss_dice_8: 1.264    time: 1.0604  last_time: 1.1121  data_time: 0.0704  last_data_time: 0.0768   lr: 0.0001  max_mem: 32651M
[10/08 15:14:44] d2.utils.events INFO:  eta: 0:14:02  iter: 199  total_loss: 21.8  loss_ce: 0.5268  loss_mask: 0.1924  loss_dice: 1.313  loss_ce_0: 0.6046  loss_mask_0: 0.2234  loss_dice_0: 1.583  loss_ce_1: 0.7221  loss_mask_1: 0.2017  loss_dice_1: 1.475  loss_ce_2: 0.6426  loss_mask_2: 0.1973  loss_dice_2: 1.404  loss_ce_3: 0.6312  loss_mask_3: 0.1961  loss_dice_3: 1.345  loss_ce_4: 0.5404  loss_mask_4: 0.1955  loss_dice_4: 1.374  loss_ce_5: 0.53  loss_mask_5: 0.1936  loss_dice_5: 1.338  loss_ce_6: 0.5614  loss_mask_6: 0.1932  loss_dice_6: 1.342  loss_ce_7: 0.5612  loss_mask_7: 0.1926  loss_dice_7: 1.335  loss_ce_8: 0.5098  loss_mask_8: 0.1934  loss_dice_8: 1.331    time: 1.0610  last_time: 1.0084  data_time: 0.0728  last_data_time: 0.0620   lr: 0.0001  max_mem: 32651M
[10/08 15:15:05] d2.utils.events INFO:  eta: 0:13:40  iter: 219  total_loss: 22.28  loss_ce: 0.5928  loss_mask: 0.1962  loss_dice: 1.269  loss_ce_0: 0.6553  loss_mask_0: 0.2313  loss_dice_0: 1.568  loss_ce_1: 0.78  loss_mask_1: 0.2189  loss_dice_1: 1.462  loss_ce_2: 0.6797  loss_mask_2: 0.2088  loss_dice_2: 1.401  loss_ce_3: 0.6197  loss_mask_3: 0.1992  loss_dice_3: 1.338  loss_ce_4: 0.6179  loss_mask_4: 0.1978  loss_dice_4: 1.345  loss_ce_5: 0.5822  loss_mask_5: 0.1962  loss_dice_5: 1.332  loss_ce_6: 0.5897  loss_mask_6: 0.1994  loss_dice_6: 1.304  loss_ce_7: 0.5865  loss_mask_7: 0.1982  loss_dice_7: 1.32  loss_ce_8: 0.5814  loss_mask_8: 0.1988  loss_dice_8: 1.338    time: 1.0606  last_time: 1.0364  data_time: 0.0671  last_data_time: 0.0762   lr: 0.0001  max_mem: 32651M
[10/08 15:15:27] d2.utils.events INFO:  eta: 0:13:21  iter: 239  total_loss: 21.62  loss_ce: 0.5406  loss_mask: 0.1875  loss_dice: 1.333  loss_ce_0: 0.6255  loss_mask_0: 0.2214  loss_dice_0: 1.589  loss_ce_1: 0.7765  loss_mask_1: 0.2053  loss_dice_1: 1.454  loss_ce_2: 0.6813  loss_mask_2: 0.1927  loss_dice_2: 1.407  loss_ce_3: 0.609  loss_mask_3: 0.1922  loss_dice_3: 1.343  loss_ce_4: 0.6041  loss_mask_4: 0.191  loss_dice_4: 1.327  loss_ce_5: 0.5617  loss_mask_5: 0.1897  loss_dice_5: 1.332  loss_ce_6: 0.5549  loss_mask_6: 0.1893  loss_dice_6: 1.333  loss_ce_7: 0.5417  loss_mask_7: 0.1916  loss_dice_7: 1.323  loss_ce_8: 0.5579  loss_mask_8: 0.1882  loss_dice_8: 1.338    time: 1.0613  last_time: 1.0635  data_time: 0.0686  last_data_time: 0.0673   lr: 0.0001  max_mem: 32651M
[10/08 15:15:48] d2.utils.events INFO:  eta: 0:12:59  iter: 259  total_loss: 21.2  loss_ce: 0.5777  loss_mask: 0.1796  loss_dice: 1.302  loss_ce_0: 0.6432  loss_mask_0: 0.2051  loss_dice_0: 1.526  loss_ce_1: 0.7443  loss_mask_1: 0.1905  loss_dice_1: 1.401  loss_ce_2: 0.6682  loss_mask_2: 0.1824  loss_dice_2: 1.355  loss_ce_3: 0.6118  loss_mask_3: 0.179  loss_dice_3: 1.279  loss_ce_4: 0.6013  loss_mask_4: 0.1818  loss_dice_4: 1.298  loss_ce_5: 0.5783  loss_mask_5: 0.1818  loss_dice_5: 1.315  loss_ce_6: 0.5482  loss_mask_6: 0.1779  loss_dice_6: 1.283  loss_ce_7: 0.5636  loss_mask_7: 0.1786  loss_dice_7: 1.342  loss_ce_8: 0.5528  loss_mask_8: 0.179  loss_dice_8: 1.337    time: 1.0608  last_time: 1.0916  data_time: 0.0669  last_data_time: 0.0601   lr: 0.0001  max_mem: 32651M
[10/08 15:16:09] d2.utils.events INFO:  eta: 0:12:38  iter: 279  total_loss: 21.12  loss_ce: 0.5575  loss_mask: 0.1751  loss_dice: 1.289  loss_ce_0: 0.6348  loss_mask_0: 0.1927  loss_dice_0: 1.515  loss_ce_1: 0.7093  loss_mask_1: 0.1827  loss_dice_1: 1.42  loss_ce_2: 0.6669  loss_mask_2: 0.177  loss_dice_2: 1.334  loss_ce_3: 0.632  loss_mask_3: 0.175  loss_dice_3: 1.283  loss_ce_4: 0.5943  loss_mask_4: 0.1714  loss_dice_4: 1.266  loss_ce_5: 0.5698  loss_mask_5: 0.1717  loss_dice_5: 1.257  loss_ce_6: 0.5613  loss_mask_6: 0.1751  loss_dice_6: 1.279  loss_ce_7: 0.583  loss_mask_7: 0.1733  loss_dice_7: 1.253  loss_ce_8: 0.5538  loss_mask_8: 0.1755  loss_dice_8: 1.275    time: 1.0606  last_time: 1.0808  data_time: 0.0680  last_data_time: 0.0694   lr: 0.0001  max_mem: 32651M
[10/08 15:16:30] d2.utils.events INFO:  eta: 0:12:15  iter: 299  total_loss: 20.85  loss_ce: 0.5362  loss_mask: 0.1932  loss_dice: 1.217  loss_ce_0: 0.6281  loss_mask_0: 0.2173  loss_dice_0: 1.479  loss_ce_1: 0.6888  loss_mask_1: 0.2096  loss_dice_1: 1.413  loss_ce_2: 0.629  loss_mask_2: 0.1951  loss_dice_2: 1.295  loss_ce_3: 0.6178  loss_mask_3: 0.1981  loss_dice_3: 1.252  loss_ce_4: 0.5847  loss_mask_4: 0.2002  loss_dice_4: 1.24  loss_ce_5: 0.5285  loss_mask_5: 0.1983  loss_dice_5: 1.264  loss_ce_6: 0.5258  loss_mask_6: 0.196  loss_dice_6: 1.251  loss_ce_7: 0.5886  loss_mask_7: 0.1943  loss_dice_7: 1.229  loss_ce_8: 0.5374  loss_mask_8: 0.1935  loss_dice_8: 1.267    time: 1.0610  last_time: 1.0447  data_time: 0.0801  last_data_time: 0.0688   lr: 0.0001  max_mem: 32651M
[10/08 15:16:51] d2.utils.events INFO:  eta: 0:11:54  iter: 319  total_loss: 20.31  loss_ce: 0.5379  loss_mask: 0.1821  loss_dice: 1.207  loss_ce_0: 0.6632  loss_mask_0: 0.2051  loss_dice_0: 1.478  loss_ce_1: 0.7191  loss_mask_1: 0.195  loss_dice_1: 1.349  loss_ce_2: 0.6484  loss_mask_2: 0.1841  loss_dice_2: 1.276  loss_ce_3: 0.6028  loss_mask_3: 0.1841  loss_dice_3: 1.209  loss_ce_4: 0.5808  loss_mask_4: 0.1853  loss_dice_4: 1.267  loss_ce_5: 0.5763  loss_mask_5: 0.1859  loss_dice_5: 1.211  loss_ce_6: 0.5691  loss_mask_6: 0.1842  loss_dice_6: 1.243  loss_ce_7: 0.5467  loss_mask_7: 0.1817  loss_dice_7: 1.194  loss_ce_8: 0.5403  loss_mask_8: 0.1821  loss_dice_8: 1.238    time: 1.0602  last_time: 1.0023  data_time: 0.0660  last_data_time: 0.0633   lr: 0.0001  max_mem: 32651M
[10/08 15:17:13] d2.utils.events INFO:  eta: 0:11:33  iter: 339  total_loss: 20.9  loss_ce: 0.51  loss_mask: 0.1963  loss_dice: 1.262  loss_ce_0: 0.5925  loss_mask_0: 0.2196  loss_dice_0: 1.505  loss_ce_1: 0.6619  loss_mask_1: 0.1988  loss_dice_1: 1.38  loss_ce_2: 0.591  loss_mask_2: 0.1908  loss_dice_2: 1.365  loss_ce_3: 0.5319  loss_mask_3: 0.1921  loss_dice_3: 1.325  loss_ce_4: 0.5403  loss_mask_4: 0.197  loss_dice_4: 1.293  loss_ce_5: 0.54  loss_mask_5: 0.1947  loss_dice_5: 1.3  loss_ce_6: 0.5463  loss_mask_6: 0.1983  loss_dice_6: 1.287  loss_ce_7: 0.5035  loss_mask_7: 0.1965  loss_dice_7: 1.237  loss_ce_8: 0.5149  loss_mask_8: 0.1975  loss_dice_8: 1.279    time: 1.0610  last_time: 1.0148  data_time: 0.0700  last_data_time: 0.0763   lr: 0.0001  max_mem: 32651M
[10/08 15:17:34] d2.utils.events INFO:  eta: 0:11:13  iter: 359  total_loss: 21.44  loss_ce: 0.559  loss_mask: 0.1801  loss_dice: 1.304  loss_ce_0: 0.6697  loss_mask_0: 0.2039  loss_dice_0: 1.546  loss_ce_1: 0.7131  loss_mask_1: 0.1887  loss_dice_1: 1.448  loss_ce_2: 0.6432  loss_mask_2: 0.1833  loss_dice_2: 1.379  loss_ce_3: 0.6071  loss_mask_3: 0.1786  loss_dice_3: 1.347  loss_ce_4: 0.5935  loss_mask_4: 0.1784  loss_dice_4: 1.307  loss_ce_5: 0.5883  loss_mask_5: 0.1797  loss_dice_5: 1.285  loss_ce_6: 0.589  loss_mask_6: 0.1788  loss_dice_6: 1.285  loss_ce_7: 0.5455  loss_mask_7: 0.1805  loss_dice_7: 1.282  loss_ce_8: 0.5615  loss_mask_8: 0.1786  loss_dice_8: 1.311    time: 1.0614  last_time: 1.0405  data_time: 0.0702  last_data_time: 0.0658   lr: 0.0001  max_mem: 32651M
[10/08 15:17:56] d2.utils.events INFO:  eta: 0:10:53  iter: 379  total_loss: 22.22  loss_ce: 0.6098  loss_mask: 0.1874  loss_dice: 1.268  loss_ce_0: 0.6442  loss_mask_0: 0.2077  loss_dice_0: 1.595  loss_ce_1: 0.7356  loss_mask_1: 0.1966  loss_dice_1: 1.469  loss_ce_2: 0.6907  loss_mask_2: 0.1885  loss_dice_2: 1.389  loss_ce_3: 0.6274  loss_mask_3: 0.1851  loss_dice_3: 1.334  loss_ce_4: 0.599  loss_mask_4: 0.189  loss_dice_4: 1.326  loss_ce_5: 0.5745  loss_mask_5: 0.1896  loss_dice_5: 1.306  loss_ce_6: 0.6288  loss_mask_6: 0.1857  loss_dice_6: 1.273  loss_ce_7: 0.5972  loss_mask_7: 0.1871  loss_dice_7: 1.315  loss_ce_8: 0.5742  loss_mask_8: 0.1862  loss_dice_8: 1.316    time: 1.0633  last_time: 1.0446  data_time: 0.0754  last_data_time: 0.0675   lr: 0.0001  max_mem: 32651M
[10/08 15:18:19] d2.utils.events INFO:  eta: 0:10:33  iter: 399  total_loss: 21.24  loss_ce: 0.5205  loss_mask: 0.1707  loss_dice: 1.279  loss_ce_0: 0.6344  loss_mask_0: 0.2006  loss_dice_0: 1.605  loss_ce_1: 0.7044  loss_mask_1: 0.1867  loss_dice_1: 1.418  loss_ce_2: 0.6218  loss_mask_2: 0.1783  loss_dice_2: 1.436  loss_ce_3: 0.5781  loss_mask_3: 0.1789  loss_dice_3: 1.363  loss_ce_4: 0.5709  loss_mask_4: 0.1758  loss_dice_4: 1.338  loss_ce_5: 0.5582  loss_mask_5: 0.1754  loss_dice_5: 1.319  loss_ce_6: 0.5348  loss_mask_6: 0.1735  loss_dice_6: 1.307  loss_ce_7: 0.553  loss_mask_7: 0.1738  loss_dice_7: 1.313  loss_ce_8: 0.5209  loss_mask_8: 0.1751  loss_dice_8: 1.328    time: 1.0667  last_time: 1.1653  data_time: 0.0903  last_data_time: 0.0854   lr: 0.0001  max_mem: 32651M
[10/08 15:18:41] d2.utils.events INFO:  eta: 0:10:14  iter: 419  total_loss: 19.22  loss_ce: 0.4456  loss_mask: 0.1945  loss_dice: 1.195  loss_ce_0: 0.5512  loss_mask_0: 0.2229  loss_dice_0: 1.416  loss_ce_1: 0.5918  loss_mask_1: 0.2021  loss_dice_1: 1.343  loss_ce_2: 0.5647  loss_mask_2: 0.197  loss_dice_2: 1.284  loss_ce_3: 0.49  loss_mask_3: 0.1938  loss_dice_3: 1.236  loss_ce_4: 0.4581  loss_mask_4: 0.1961  loss_dice_4: 1.233  loss_ce_5: 0.4643  loss_mask_5: 0.1958  loss_dice_5: 1.2  loss_ce_6: 0.4491  loss_mask_6: 0.1959  loss_dice_6: 1.235  loss_ce_7: 0.4521  loss_mask_7: 0.196  loss_dice_7: 1.206  loss_ce_8: 0.4616  loss_mask_8: 0.1963  loss_dice_8: 1.218    time: 1.0686  last_time: 1.0906  data_time: 0.0793  last_data_time: 0.0578   lr: 0.0001  max_mem: 32651M
[10/08 15:19:04] d2.utils.events INFO:  eta: 0:09:54  iter: 439  total_loss: 21.13  loss_ce: 0.5447  loss_mask: 0.1826  loss_dice: 1.28  loss_ce_0: 0.601  loss_mask_0: 0.2027  loss_dice_0: 1.542  loss_ce_1: 0.6661  loss_mask_1: 0.1976  loss_dice_1: 1.412  loss_ce_2: 0.6709  loss_mask_2: 0.187  loss_dice_2: 1.337  loss_ce_3: 0.581  loss_mask_3: 0.1867  loss_dice_3: 1.301  loss_ce_4: 0.5638  loss_mask_4: 0.1861  loss_dice_4: 1.275  loss_ce_5: 0.5873  loss_mask_5: 0.182  loss_dice_5: 1.272  loss_ce_6: 0.5184  loss_mask_6: 0.1835  loss_dice_6: 1.301  loss_ce_7: 0.5368  loss_mask_7: 0.1815  loss_dice_7: 1.284  loss_ce_8: 0.5363  loss_mask_8: 0.1844  loss_dice_8: 1.27    time: 1.0727  last_time: 1.1392  data_time: 0.0938  last_data_time: 0.1011   lr: 0.0001  max_mem: 32651M
[10/08 15:19:27] d2.utils.events INFO:  eta: 0:09:34  iter: 459  total_loss: 19.93  loss_ce: 0.5007  loss_mask: 0.1848  loss_dice: 1.228  loss_ce_0: 0.6201  loss_mask_0: 0.2135  loss_dice_0: 1.421  loss_ce_1: 0.6595  loss_mask_1: 0.2025  loss_dice_1: 1.372  loss_ce_2: 0.5941  loss_mask_2: 0.191  loss_dice_2: 1.273  loss_ce_3: 0.5476  loss_mask_3: 0.1882  loss_dice_3: 1.257  loss_ce_4: 0.5164  loss_mask_4: 0.1868  loss_dice_4: 1.265  loss_ce_5: 0.5337  loss_mask_5: 0.1868  loss_dice_5: 1.229  loss_ce_6: 0.534  loss_mask_6: 0.1854  loss_dice_6: 1.248  loss_ce_7: 0.5104  loss_mask_7: 0.1851  loss_dice_7: 1.242  loss_ce_8: 0.5041  loss_mask_8: 0.1827  loss_dice_8: 1.234    time: 1.0755  last_time: 1.1588  data_time: 0.0918  last_data_time: 0.0870   lr: 0.0001  max_mem: 32651M
[10/08 15:19:50] d2.utils.events INFO:  eta: 0:09:13  iter: 479  total_loss: 20.36  loss_ce: 0.5026  loss_mask: 0.183  loss_dice: 1.252  loss_ce_0: 0.5678  loss_mask_0: 0.2168  loss_dice_0: 1.501  loss_ce_1: 0.6645  loss_mask_1: 0.1947  loss_dice_1: 1.41  loss_ce_2: 0.6189  loss_mask_2: 0.1894  loss_dice_2: 1.347  loss_ce_3: 0.5815  loss_mask_3: 0.1857  loss_dice_3: 1.287  loss_ce_4: 0.5535  loss_mask_4: 0.1858  loss_dice_4: 1.253  loss_ce_5: 0.5567  loss_mask_5: 0.183  loss_dice_5: 1.241  loss_ce_6: 0.5398  loss_mask_6: 0.1839  loss_dice_6: 1.241  loss_ce_7: 0.5133  loss_mask_7: 0.1811  loss_dice_7: 1.248  loss_ce_8: 0.5123  loss_mask_8: 0.1841  loss_dice_8: 1.245    time: 1.0774  last_time: 1.0743  data_time: 0.0871  last_data_time: 0.0800   lr: 0.0001  max_mem: 32651M
[10/08 15:20:12] d2.utils.events INFO:  eta: 0:08:53  iter: 499  total_loss: 19.88  loss_ce: 0.526  loss_mask: 0.1791  loss_dice: 1.178  loss_ce_0: 0.604  loss_mask_0: 0.2102  loss_dice_0: 1.434  loss_ce_1: 0.6726  loss_mask_1: 0.1912  loss_dice_1: 1.333  loss_ce_2: 0.6045  loss_mask_2: 0.1819  loss_dice_2: 1.277  loss_ce_3: 0.553  loss_mask_3: 0.1841  loss_dice_3: 1.241  loss_ce_4: 0.545  loss_mask_4: 0.1844  loss_dice_4: 1.21  loss_ce_5: 0.541  loss_mask_5: 0.1831  loss_dice_5: 1.203  loss_ce_6: 0.5309  loss_mask_6: 0.1793  loss_dice_6: 1.189  loss_ce_7: 0.5162  loss_mask_7: 0.1788  loss_dice_7: 1.183  loss_ce_8: 0.5128  loss_mask_8: 0.1799  loss_dice_8: 1.196    time: 1.0794  last_time: 1.1404  data_time: 0.0832  last_data_time: 0.1211   lr: 0.0001  max_mem: 32651M
[10/08 15:20:35] d2.utils.events INFO:  eta: 0:08:33  iter: 519  total_loss: 20.98  loss_ce: 0.5652  loss_mask: 0.1781  loss_dice: 1.275  loss_ce_0: 0.576  loss_mask_0: 0.2093  loss_dice_0: 1.563  loss_ce_1: 0.6725  loss_mask_1: 0.189  loss_dice_1: 1.386  loss_ce_2: 0.6386  loss_mask_2: 0.1807  loss_dice_2: 1.389  loss_ce_3: 0.5731  loss_mask_3: 0.1814  loss_dice_3: 1.302  loss_ce_4: 0.5312  loss_mask_4: 0.1805  loss_dice_4: 1.313  loss_ce_5: 0.5497  loss_mask_5: 0.181  loss_dice_5: 1.337  loss_ce_6: 0.5613  loss_mask_6: 0.179  loss_dice_6: 1.28  loss_ce_7: 0.556  loss_mask_7: 0.1802  loss_dice_7: 1.268  loss_ce_8: 0.5191  loss_mask_8: 0.1762  loss_dice_8: 1.281    time: 1.0822  last_time: 1.1792  data_time: 0.0960  last_data_time: 0.1123   lr: 0.0001  max_mem: 32651M
[10/08 15:20:58] d2.utils.events INFO:  eta: 0:08:13  iter: 539  total_loss: 20.54  loss_ce: 0.5033  loss_mask: 0.1726  loss_dice: 1.274  loss_ce_0: 0.5679  loss_mask_0: 0.2039  loss_dice_0: 1.55  loss_ce_1: 0.665  loss_mask_1: 0.1874  loss_dice_1: 1.455  loss_ce_2: 0.573  loss_mask_2: 0.1746  loss_dice_2: 1.364  loss_ce_3: 0.5663  loss_mask_3: 0.1767  loss_dice_3: 1.326  loss_ce_4: 0.5164  loss_mask_4: 0.1769  loss_dice_4: 1.354  loss_ce_5: 0.5113  loss_mask_5: 0.178  loss_dice_5: 1.358  loss_ce_6: 0.5309  loss_mask_6: 0.1723  loss_dice_6: 1.279  loss_ce_7: 0.4748  loss_mask_7: 0.1735  loss_dice_7: 1.32  loss_ce_8: 0.4943  loss_mask_8: 0.1722  loss_dice_8: 1.292    time: 1.0844  last_time: 1.1327  data_time: 0.0888  last_data_time: 0.0981   lr: 0.0001  max_mem: 32651M
[10/08 15:21:21] d2.utils.events INFO:  eta: 0:07:53  iter: 559  total_loss: 20.9  loss_ce: 0.5332  loss_mask: 0.177  loss_dice: 1.251  loss_ce_0: 0.6244  loss_mask_0: 0.2048  loss_dice_0: 1.505  loss_ce_1: 0.7072  loss_mask_1: 0.1924  loss_dice_1: 1.462  loss_ce_2: 0.6068  loss_mask_2: 0.1873  loss_dice_2: 1.359  loss_ce_3: 0.591  loss_mask_3: 0.18  loss_dice_3: 1.323  loss_ce_4: 0.5631  loss_mask_4: 0.1772  loss_dice_4: 1.31  loss_ce_5: 0.5701  loss_mask_5: 0.1744  loss_dice_5: 1.293  loss_ce_6: 0.5554  loss_mask_6: 0.1764  loss_dice_6: 1.272  loss_ce_7: 0.505  loss_mask_7: 0.1788  loss_dice_7: 1.293  loss_ce_8: 0.5226  loss_mask_8: 0.1769  loss_dice_8: 1.288    time: 1.0858  last_time: 1.0635  data_time: 0.0889  last_data_time: 0.0805   lr: 0.0001  max_mem: 32651M
[10/08 15:21:43] d2.utils.events INFO:  eta: 0:07:31  iter: 579  total_loss: 20.93  loss_ce: 0.4879  loss_mask: 0.1884  loss_dice: 1.264  loss_ce_0: 0.5873  loss_mask_0: 0.2172  loss_dice_0: 1.512  loss_ce_1: 0.6566  loss_mask_1: 0.2053  loss_dice_1: 1.443  loss_ce_2: 0.6057  loss_mask_2: 0.1949  loss_dice_2: 1.369  loss_ce_3: 0.5473  loss_mask_3: 0.1926  loss_dice_3: 1.3  loss_ce_4: 0.5291  loss_mask_4: 0.1911  loss_dice_4: 1.319  loss_ce_5: 0.5032  loss_mask_5: 0.1891  loss_dice_5: 1.299  loss_ce_6: 0.5061  loss_mask_6: 0.1856  loss_dice_6: 1.315  loss_ce_7: 0.4886  loss_mask_7: 0.1845  loss_dice_7: 1.281  loss_ce_8: 0.4936  loss_mask_8: 0.1872  loss_dice_8: 1.254    time: 1.0863  last_time: 1.0731  data_time: 0.0782  last_data_time: 0.0889   lr: 0.0001  max_mem: 32651M
[10/08 15:22:06] d2.utils.events INFO:  eta: 0:07:11  iter: 599  total_loss: 21.56  loss_ce: 0.5587  loss_mask: 0.1899  loss_dice: 1.267  loss_ce_0: 0.6304  loss_mask_0: 0.2088  loss_dice_0: 1.534  loss_ce_1: 0.7224  loss_mask_1: 0.2057  loss_dice_1: 1.411  loss_ce_2: 0.6769  loss_mask_2: 0.1905  loss_dice_2: 1.362  loss_ce_3: 0.6492  loss_mask_3: 0.1949  loss_dice_3: 1.329  loss_ce_4: 0.6019  loss_mask_4: 0.1914  loss_dice_4: 1.309  loss_ce_5: 0.5974  loss_mask_5: 0.1911  loss_dice_5: 1.3  loss_ce_6: 0.5679  loss_mask_6: 0.1883  loss_dice_6: 1.272  loss_ce_7: 0.6029  loss_mask_7: 0.1873  loss_dice_7: 1.285  loss_ce_8: 0.59  loss_mask_8: 0.1874  loss_dice_8: 1.291    time: 1.0878  last_time: 1.0349  data_time: 0.0896  last_data_time: 0.0720   lr: 0.0001  max_mem: 32651M
[10/08 15:22:28] d2.utils.events INFO:  eta: 0:06:50  iter: 619  total_loss: 20.7  loss_ce: 0.5018  loss_mask: 0.1709  loss_dice: 1.295  loss_ce_0: 0.6226  loss_mask_0: 0.2032  loss_dice_0: 1.543  loss_ce_1: 0.6982  loss_mask_1: 0.1818  loss_dice_1: 1.434  loss_ce_2: 0.6641  loss_mask_2: 0.1721  loss_dice_2: 1.344  loss_ce_3: 0.5873  loss_mask_3: 0.1732  loss_dice_3: 1.339  loss_ce_4: 0.5496  loss_mask_4: 0.1693  loss_dice_4: 1.307  loss_ce_5: 0.553  loss_mask_5: 0.171  loss_dice_5: 1.357  loss_ce_6: 0.5335  loss_mask_6: 0.1711  loss_dice_6: 1.28  loss_ce_7: 0.5518  loss_mask_7: 0.17  loss_dice_7: 1.316  loss_ce_8: 0.5351  loss_mask_8: 0.1684  loss_dice_8: 1.299    time: 1.0890  last_time: 1.1771  data_time: 0.0800  last_data_time: 0.0902   lr: 0.0001  max_mem: 32651M
[10/08 15:22:50] d2.utils.events INFO:  eta: 0:06:29  iter: 639  total_loss: 20.73  loss_ce: 0.4876  loss_mask: 0.1762  loss_dice: 1.311  loss_ce_0: 0.5674  loss_mask_0: 0.2059  loss_dice_0: 1.513  loss_ce_1: 0.6082  loss_mask_1: 0.1899  loss_dice_1: 1.435  loss_ce_2: 0.5912  loss_mask_2: 0.1873  loss_dice_2: 1.384  loss_ce_3: 0.5102  loss_mask_3: 0.1798  loss_dice_3: 1.32  loss_ce_4: 0.5196  loss_mask_4: 0.1755  loss_dice_4: 1.314  loss_ce_5: 0.5122  loss_mask_5: 0.1773  loss_dice_5: 1.307  loss_ce_6: 0.4717  loss_mask_6: 0.1765  loss_dice_6: 1.283  loss_ce_7: 0.4792  loss_mask_7: 0.1754  loss_dice_7: 1.312  loss_ce_8: 0.4899  loss_mask_8: 0.1739  loss_dice_8: 1.288    time: 1.0892  last_time: 1.0880  data_time: 0.0795  last_data_time: 0.0677   lr: 0.0001  max_mem: 32651M
[10/08 15:23:12] d2.utils.events INFO:  eta: 0:06:07  iter: 659  total_loss: 20.62  loss_ce: 0.5075  loss_mask: 0.1809  loss_dice: 1.264  loss_ce_0: 0.5726  loss_mask_0: 0.2042  loss_dice_0: 1.493  loss_ce_1: 0.6617  loss_mask_1: 0.192  loss_dice_1: 1.398  loss_ce_2: 0.6219  loss_mask_2: 0.1827  loss_dice_2: 1.331  loss_ce_3: 0.5534  loss_mask_3: 0.1823  loss_dice_3: 1.288  loss_ce_4: 0.5325  loss_mask_4: 0.1817  loss_dice_4: 1.291  loss_ce_5: 0.5385  loss_mask_5: 0.1803  loss_dice_5: 1.285  loss_ce_6: 0.5218  loss_mask_6: 0.1804  loss_dice_6: 1.27  loss_ce_7: 0.5233  loss_mask_7: 0.1793  loss_dice_7: 1.291  loss_ce_8: 0.5333  loss_mask_8: 0.1794  loss_dice_8: 1.236    time: 1.0899  last_time: 1.1625  data_time: 0.0815  last_data_time: 0.0967   lr: 0.0001  max_mem: 32651M
[10/08 15:23:35] d2.utils.events INFO:  eta: 0:05:46  iter: 679  total_loss: 21.94  loss_ce: 0.5198  loss_mask: 0.1738  loss_dice: 1.329  loss_ce_0: 0.5934  loss_mask_0: 0.1972  loss_dice_0: 1.554  loss_ce_1: 0.6651  loss_mask_1: 0.1862  loss_dice_1: 1.454  loss_ce_2: 0.6476  loss_mask_2: 0.1763  loss_dice_2: 1.412  loss_ce_3: 0.571  loss_mask_3: 0.1729  loss_dice_3: 1.373  loss_ce_4: 0.5394  loss_mask_4: 0.1739  loss_dice_4: 1.359  loss_ce_5: 0.5573  loss_mask_5: 0.1711  loss_dice_5: 1.358  loss_ce_6: 0.5323  loss_mask_6: 0.171  loss_dice_6: 1.323  loss_ce_7: 0.5222  loss_mask_7: 0.1723  loss_dice_7: 1.322  loss_ce_8: 0.551  loss_mask_8: 0.1721  loss_dice_8: 1.331    time: 1.0908  last_time: 1.1358  data_time: 0.0800  last_data_time: 0.0941   lr: 0.0001  max_mem: 32651M
[10/08 15:23:58] d2.utils.events INFO:  eta: 0:05:25  iter: 699  total_loss: 21.59  loss_ce: 0.5635  loss_mask: 0.1908  loss_dice: 1.282  loss_ce_0: 0.6357  loss_mask_0: 0.2248  loss_dice_0: 1.562  loss_ce_1: 0.7314  loss_mask_1: 0.2118  loss_dice_1: 1.433  loss_ce_2: 0.6816  loss_mask_2: 0.1978  loss_dice_2: 1.382  loss_ce_3: 0.5973  loss_mask_3: 0.1893  loss_dice_3: 1.324  loss_ce_4: 0.5733  loss_mask_4: 0.1921  loss_dice_4: 1.302  loss_ce_5: 0.5648  loss_mask_5: 0.1869  loss_dice_5: 1.28  loss_ce_6: 0.5679  loss_mask_6: 0.1878  loss_dice_6: 1.289  loss_ce_7: 0.5733  loss_mask_7: 0.1942  loss_dice_7: 1.292  loss_ce_8: 0.5921  loss_mask_8: 0.1905  loss_dice_8: 1.283    time: 1.0920  last_time: 1.1054  data_time: 0.0857  last_data_time: 0.0610   lr: 0.0001  max_mem: 32651M
[10/08 15:24:20] d2.utils.events INFO:  eta: 0:05:04  iter: 719  total_loss: 20.7  loss_ce: 0.551  loss_mask: 0.1717  loss_dice: 1.251  loss_ce_0: 0.6152  loss_mask_0: 0.1996  loss_dice_0: 1.511  loss_ce_1: 0.6938  loss_mask_1: 0.1799  loss_dice_1: 1.382  loss_ce_2: 0.6398  loss_mask_2: 0.1732  loss_dice_2: 1.368  loss_ce_3: 0.5742  loss_mask_3: 0.1744  loss_dice_3: 1.322  loss_ce_4: 0.5655  loss_mask_4: 0.175  loss_dice_4: 1.283  loss_ce_5: 0.5682  loss_mask_5: 0.1725  loss_dice_5: 1.278  loss_ce_6: 0.5402  loss_mask_6: 0.1725  loss_dice_6: 1.289  loss_ce_7: 0.5462  loss_mask_7: 0.172  loss_dice_7: 1.266  loss_ce_8: 0.5375  loss_mask_8: 0.1735  loss_dice_8: 1.258    time: 1.0931  last_time: 1.1543  data_time: 0.0863  last_data_time: 0.0878   lr: 0.0001  max_mem: 32651M
[10/08 15:24:43] d2.utils.events INFO:  eta: 0:04:43  iter: 739  total_loss: 19.87  loss_ce: 0.4952  loss_mask: 0.1817  loss_dice: 1.196  loss_ce_0: 0.5506  loss_mask_0: 0.2024  loss_dice_0: 1.46  loss_ce_1: 0.629  loss_mask_1: 0.1913  loss_dice_1: 1.359  loss_ce_2: 0.5927  loss_mask_2: 0.1824  loss_dice_2: 1.337  loss_ce_3: 0.5238  loss_mask_3: 0.1822  loss_dice_3: 1.259  loss_ce_4: 0.5293  loss_mask_4: 0.1806  loss_dice_4: 1.247  loss_ce_5: 0.5147  loss_mask_5: 0.1819  loss_dice_5: 1.235  loss_ce_6: 0.4857  loss_mask_6: 0.1804  loss_dice_6: 1.22  loss_ce_7: 0.4893  loss_mask_7: 0.1811  loss_dice_7: 1.212  loss_ce_8: 0.4787  loss_mask_8: 0.1825  loss_dice_8: 1.221    time: 1.0947  last_time: 1.1167  data_time: 0.0880  last_data_time: 0.0839   lr: 0.0001  max_mem: 32651M
[10/08 15:25:06] d2.utils.events INFO:  eta: 0:04:21  iter: 759  total_loss: 19.91  loss_ce: 0.4852  loss_mask: 0.1692  loss_dice: 1.247  loss_ce_0: 0.5547  loss_mask_0: 0.1916  loss_dice_0: 1.461  loss_ce_1: 0.6021  loss_mask_1: 0.1793  loss_dice_1: 1.355  loss_ce_2: 0.57  loss_mask_2: 0.1718  loss_dice_2: 1.338  loss_ce_3: 0.5243  loss_mask_3: 0.1735  loss_dice_3: 1.283  loss_ce_4: 0.5313  loss_mask_4: 0.1717  loss_dice_4: 1.246  loss_ce_5: 0.4866  loss_mask_5: 0.1728  loss_dice_5: 1.252  loss_ce_6: 0.5014  loss_mask_6: 0.1711  loss_dice_6: 1.244  loss_ce_7: 0.5111  loss_mask_7: 0.1696  loss_dice_7: 1.214  loss_ce_8: 0.4925  loss_mask_8: 0.1683  loss_dice_8: 1.231    time: 1.0958  last_time: 1.0809  data_time: 0.0925  last_data_time: 0.0754   lr: 0.0001  max_mem: 32651M
[10/08 15:25:28] d2.utils.events INFO:  eta: 0:03:59  iter: 779  total_loss: 19.18  loss_ce: 0.4698  loss_mask: 0.1695  loss_dice: 1.221  loss_ce_0: 0.5531  loss_mask_0: 0.1936  loss_dice_0: 1.443  loss_ce_1: 0.6388  loss_mask_1: 0.1814  loss_dice_1: 1.304  loss_ce_2: 0.5547  loss_mask_2: 0.1765  loss_dice_2: 1.259  loss_ce_3: 0.4761  loss_mask_3: 0.1719  loss_dice_3: 1.216  loss_ce_4: 0.5073  loss_mask_4: 0.1726  loss_dice_4: 1.22  loss_ce_5: 0.4935  loss_mask_5: 0.1693  loss_dice_5: 1.215  loss_ce_6: 0.459  loss_mask_6: 0.1702  loss_dice_6: 1.192  loss_ce_7: 0.4757  loss_mask_7: 0.1726  loss_dice_7: 1.189  loss_ce_8: 0.4758  loss_mask_8: 0.171  loss_dice_8: 1.18    time: 1.0962  last_time: 1.1905  data_time: 0.0803  last_data_time: 0.0807   lr: 0.0001  max_mem: 32651M
[10/08 15:25:51] d2.utils.events INFO:  eta: 0:03:38  iter: 799  total_loss: 20.22  loss_ce: 0.4901  loss_mask: 0.1804  loss_dice: 1.2  loss_ce_0: 0.5693  loss_mask_0: 0.2134  loss_dice_0: 1.51  loss_ce_1: 0.6755  loss_mask_1: 0.1916  loss_dice_1: 1.335  loss_ce_2: 0.5915  loss_mask_2: 0.1868  loss_dice_2: 1.293  loss_ce_3: 0.5268  loss_mask_3: 0.1863  loss_dice_3: 1.287  loss_ce_4: 0.5353  loss_mask_4: 0.1826  loss_dice_4: 1.269  loss_ce_5: 0.527  loss_mask_5: 0.1822  loss_dice_5: 1.247  loss_ce_6: 0.4841  loss_mask_6: 0.1826  loss_dice_6: 1.233  loss_ce_7: 0.4858  loss_mask_7: 0.1841  loss_dice_7: 1.225  loss_ce_8: 0.4911  loss_mask_8: 0.1801  loss_dice_8: 1.222    time: 1.0974  last_time: 1.1103  data_time: 0.0846  last_data_time: 0.0736   lr: 0.0001  max_mem: 32651M
[10/08 15:26:14] d2.utils.events INFO:  eta: 0:03:16  iter: 819  total_loss: 20.18  loss_ce: 0.4766  loss_mask: 0.1755  loss_dice: 1.241  loss_ce_0: 0.6032  loss_mask_0: 0.1916  loss_dice_0: 1.468  loss_ce_1: 0.6549  loss_mask_1: 0.1891  loss_dice_1: 1.352  loss_ce_2: 0.5805  loss_mask_2: 0.1896  loss_dice_2: 1.305  loss_ce_3: 0.5326  loss_mask_3: 0.181  loss_dice_3: 1.237  loss_ce_4: 0.5219  loss_mask_4: 0.1772  loss_dice_4: 1.239  loss_ce_5: 0.4916  loss_mask_5: 0.1762  loss_dice_5: 1.283  loss_ce_6: 0.5173  loss_mask_6: 0.1742  loss_dice_6: 1.213  loss_ce_7: 0.497  loss_mask_7: 0.1759  loss_dice_7: 1.235  loss_ce_8: 0.4864  loss_mask_8: 0.1763  loss_dice_8: 1.229    time: 1.0986  last_time: 1.2159  data_time: 0.0880  last_data_time: 0.0927   lr: 0.0001  max_mem: 32651M
[10/08 15:26:37] d2.utils.events INFO:  eta: 0:02:55  iter: 839  total_loss: 19.76  loss_ce: 0.5143  loss_mask: 0.1579  loss_dice: 1.195  loss_ce_0: 0.6105  loss_mask_0: 0.1822  loss_dice_0: 1.45  loss_ce_1: 0.6324  loss_mask_1: 0.183  loss_dice_1: 1.352  loss_ce_2: 0.5785  loss_mask_2: 0.1666  loss_dice_2: 1.293  loss_ce_3: 0.5323  loss_mask_3: 0.1634  loss_dice_3: 1.246  loss_ce_4: 0.4976  loss_mask_4: 0.1618  loss_dice_4: 1.189  loss_ce_5: 0.529  loss_mask_5: 0.159  loss_dice_5: 1.216  loss_ce_6: 0.5106  loss_mask_6: 0.1587  loss_dice_6: 1.187  loss_ce_7: 0.4848  loss_mask_7: 0.1581  loss_dice_7: 1.186  loss_ce_8: 0.5223  loss_mask_8: 0.1578  loss_dice_8: 1.178    time: 1.0998  last_time: 1.1523  data_time: 0.0903  last_data_time: 0.0820   lr: 0.0001  max_mem: 32651M
[10/08 15:26:59] d2.utils.events INFO:  eta: 0:02:33  iter: 859  total_loss: 20.27  loss_ce: 0.4809  loss_mask: 0.1775  loss_dice: 1.239  loss_ce_0: 0.567  loss_mask_0: 0.1914  loss_dice_0: 1.474  loss_ce_1: 0.6697  loss_mask_1: 0.1853  loss_dice_1: 1.356  loss_ce_2: 0.5715  loss_mask_2: 0.1774  loss_dice_2: 1.308  loss_ce_3: 0.515  loss_mask_3: 0.1755  loss_dice_3: 1.289  loss_ce_4: 0.5107  loss_mask_4: 0.1762  loss_dice_4: 1.242  loss_ce_5: 0.4993  loss_mask_5: 0.1751  loss_dice_5: 1.252  loss_ce_6: 0.515  loss_mask_6: 0.1757  loss_dice_6: 1.216  loss_ce_7: 0.5118  loss_mask_7: 0.1755  loss_dice_7: 1.209  loss_ce_8: 0.5048  loss_mask_8: 0.176  loss_dice_8: 1.225    time: 1.0998  last_time: 1.0652  data_time: 0.0825  last_data_time: 0.0622   lr: 0.0001  max_mem: 32651M
[10/08 15:27:22] d2.utils.events INFO:  eta: 0:02:11  iter: 879  total_loss: 19.37  loss_ce: 0.4456  loss_mask: 0.1795  loss_dice: 1.162  loss_ce_0: 0.5938  loss_mask_0: 0.214  loss_dice_0: 1.408  loss_ce_1: 0.6026  loss_mask_1: 0.2009  loss_dice_1: 1.31  loss_ce_2: 0.5649  loss_mask_2: 0.1864  loss_dice_2: 1.263  loss_ce_3: 0.482  loss_mask_3: 0.1813  loss_dice_3: 1.232  loss_ce_4: 0.4821  loss_mask_4: 0.1824  loss_dice_4: 1.157  loss_ce_5: 0.4644  loss_mask_5: 0.1801  loss_dice_5: 1.169  loss_ce_6: 0.4612  loss_mask_6: 0.179  loss_dice_6: 1.19  loss_ce_7: 0.4445  loss_mask_7: 0.1813  loss_dice_7: 1.175  loss_ce_8: 0.4698  loss_mask_8: 0.1809  loss_dice_8: 1.182    time: 1.1002  last_time: 1.1625  data_time: 0.0871  last_data_time: 0.1672   lr: 0.0001  max_mem: 32651M
[10/08 15:27:45] d2.utils.events INFO:  eta: 0:01:49  iter: 899  total_loss: 20.08  loss_ce: 0.5316  loss_mask: 0.1716  loss_dice: 1.223  loss_ce_0: 0.5541  loss_mask_0: 0.2015  loss_dice_0: 1.47  loss_ce_1: 0.6472  loss_mask_1: 0.1843  loss_dice_1: 1.394  loss_ce_2: 0.5916  loss_mask_2: 0.1773  loss_dice_2: 1.312  loss_ce_3: 0.5311  loss_mask_3: 0.1746  loss_dice_3: 1.268  loss_ce_4: 0.5101  loss_mask_4: 0.1704  loss_dice_4: 1.295  loss_ce_5: 0.4925  loss_mask_5: 0.1737  loss_dice_5: 1.259  loss_ce_6: 0.4858  loss_mask_6: 0.1715  loss_dice_6: 1.245  loss_ce_7: 0.4985  loss_mask_7: 0.173  loss_dice_7: 1.211  loss_ce_8: 0.5342  loss_mask_8: 0.1701  loss_dice_8: 1.211    time: 1.1012  last_time: 1.1968  data_time: 0.0886  last_data_time: 0.0909   lr: 0.0001  max_mem: 32651M
[10/08 15:28:07] d2.utils.events INFO:  eta: 0:01:27  iter: 919  total_loss: 20.58  loss_ce: 0.4826  loss_mask: 0.1631  loss_dice: 1.207  loss_ce_0: 0.5702  loss_mask_0: 0.1976  loss_dice_0: 1.45  loss_ce_1: 0.6743  loss_mask_1: 0.1783  loss_dice_1: 1.397  loss_ce_2: 0.6174  loss_mask_2: 0.1673  loss_dice_2: 1.348  loss_ce_3: 0.5524  loss_mask_3: 0.1709  loss_dice_3: 1.254  loss_ce_4: 0.5395  loss_mask_4: 0.1677  loss_dice_4: 1.276  loss_ce_5: 0.5296  loss_mask_5: 0.1628  loss_dice_5: 1.255  loss_ce_6: 0.5392  loss_mask_6: 0.1655  loss_dice_6: 1.253  loss_ce_7: 0.5405  loss_mask_7: 0.1646  loss_dice_7: 1.25  loss_ce_8: 0.4886  loss_mask_8: 0.1649  loss_dice_8: 1.225    time: 1.1014  last_time: 1.1527  data_time: 0.0869  last_data_time: 0.0865   lr: 0.0001  max_mem: 32651M
[10/08 15:28:30] d2.utils.events INFO:  eta: 0:01:05  iter: 939  total_loss: 20.37  loss_ce: 0.5092  loss_mask: 0.1529  loss_dice: 1.199  loss_ce_0: 0.5514  loss_mask_0: 0.179  loss_dice_0: 1.515  loss_ce_1: 0.655  loss_mask_1: 0.1685  loss_dice_1: 1.36  loss_ce_2: 0.6343  loss_mask_2: 0.161  loss_dice_2: 1.329  loss_ce_3: 0.5516  loss_mask_3: 0.1599  loss_dice_3: 1.247  loss_ce_4: 0.4985  loss_mask_4: 0.1582  loss_dice_4: 1.256  loss_ce_5: 0.5038  loss_mask_5: 0.1542  loss_dice_5: 1.256  loss_ce_6: 0.522  loss_mask_6: 0.1539  loss_dice_6: 1.215  loss_ce_7: 0.4925  loss_mask_7: 0.1554  loss_dice_7: 1.239  loss_ce_8: 0.5013  loss_mask_8: 0.1538  loss_dice_8: 1.197    time: 1.1022  last_time: 1.0637  data_time: 0.0909  last_data_time: 0.0676   lr: 0.0001  max_mem: 32651M
[10/08 15:28:53] d2.utils.events INFO:  eta: 0:00:44  iter: 959  total_loss: 20.15  loss_ce: 0.4976  loss_mask: 0.16  loss_dice: 1.26  loss_ce_0: 0.5637  loss_mask_0: 0.1877  loss_dice_0: 1.501  loss_ce_1: 0.6299  loss_mask_1: 0.1725  loss_dice_1: 1.396  loss_ce_2: 0.5786  loss_mask_2: 0.1633  loss_dice_2: 1.33  loss_ce_3: 0.5298  loss_mask_3: 0.1645  loss_dice_3: 1.305  loss_ce_4: 0.495  loss_mask_4: 0.1633  loss_dice_4: 1.301  loss_ce_5: 0.5185  loss_mask_5: 0.1591  loss_dice_5: 1.317  loss_ce_6: 0.4773  loss_mask_6: 0.1597  loss_dice_6: 1.294  loss_ce_7: 0.4874  loss_mask_7: 0.1583  loss_dice_7: 1.292  loss_ce_8: 0.4769  loss_mask_8: 0.1587  loss_dice_8: 1.264    time: 1.1030  last_time: 1.2412  data_time: 0.0842  last_data_time: 0.0877   lr: 0.0001  max_mem: 32651M
[10/08 15:29:16] d2.utils.events INFO:  eta: 0:00:22  iter: 979  total_loss: 21.27  loss_ce: 0.526  loss_mask: 0.1669  loss_dice: 1.236  loss_ce_0: 0.5951  loss_mask_0: 0.1898  loss_dice_0: 1.529  loss_ce_1: 0.669  loss_mask_1: 0.1799  loss_dice_1: 1.458  loss_ce_2: 0.658  loss_mask_2: 0.167  loss_dice_2: 1.361  loss_ce_3: 0.5489  loss_mask_3: 0.169  loss_dice_3: 1.332  loss_ce_4: 0.5321  loss_mask_4: 0.1663  loss_dice_4: 1.322  loss_ce_5: 0.5637  loss_mask_5: 0.1643  loss_dice_5: 1.348  loss_ce_6: 0.5126  loss_mask_6: 0.1662  loss_dice_6: 1.314  loss_ce_7: 0.5362  loss_mask_7: 0.1654  loss_dice_7: 1.283  loss_ce_8: 0.5256  loss_mask_8: 0.1684  loss_dice_8: 1.278    time: 1.1039  last_time: 1.1706  data_time: 0.0883  last_data_time: 0.0898   lr: 0.0001  max_mem: 32651M
[10/08 15:29:39] fvcore.common.checkpoint INFO: Saving checkpoint to /tsi/hi-paris/GB/segmentation/results/In_vocab/r50_008_1000_19_a_decoder/model_final.pth
[10/08 15:29:42] d2.utils.events INFO:  eta: 0:00:00  iter: 999  total_loss: 20.33  loss_ce: 0.5094  loss_mask: 0.1614  loss_dice: 1.267  loss_ce_0: 0.5835  loss_mask_0: 0.2017  loss_dice_0: 1.481  loss_ce_1: 0.6813  loss_mask_1: 0.1805  loss_dice_1: 1.415  loss_ce_2: 0.5956  loss_mask_2: 0.1603  loss_dice_2: 1.336  loss_ce_3: 0.5533  loss_mask_3: 0.162  loss_dice_3: 1.286  loss_ce_4: 0.533  loss_mask_4: 0.166  loss_dice_4: 1.315  loss_ce_5: 0.5  loss_mask_5: 0.1652  loss_dice_5: 1.317  loss_ce_6: 0.5069  loss_mask_6: 0.1674  loss_dice_6: 1.254  loss_ce_7: 0.511  loss_mask_7: 0.1656  loss_dice_7: 1.239  loss_ce_8: 0.498  loss_mask_8: 0.1642  loss_dice_8: 1.283    time: 1.1048  last_time: 1.1677  data_time: 0.0872  last_data_time: 0.0873   lr: 0.0001  max_mem: 32651M
[10/08 15:29:42] d2.engine.hooks INFO: Overall training speed: 998 iterations in 0:18:22 (1.1048 s / it)
[10/08 15:29:42] d2.engine.hooks INFO: Total training time: 0:18:28 (0:00:05 on hooks)
[10/08 15:29:42] fcclip.data.datasets.register_cityscapes_panoptic INFO: 3 cities found in 'datasets/cityscapes/leftImg8bit/val'.
[10/08 15:29:42] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(1024, 1024), max_size=2560, sample_style='choice')]
[10/08 15:29:42] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/08 15:29:42] d2.data.common INFO: Serializing 500 elements to byte tensors and concatenating them all ...
[10/08 15:29:42] d2.data.common INFO: Serialized dataset takes 0.74 MiB
[10/08 15:29:42] d2.evaluation.evaluator INFO: Start inference on 500 batches
[10/08 15:42:51] detectron2 INFO: Rank of current process: 0. World size: 1
[10/08 15:42:52] detectron2 INFO: Environment info:
-------------------------------  -------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.8.19 | packaged by conda-forge | (default, Mar 20 2024, 12:47:35) [GCC 12.3.0]
numpy                            1.24.4
detectron2                       0.6 @/home/ids/gbrison/segmentation/segmentation/detectron2/detectron2
Compiler                         GCC 11.4
CUDA compiler                    CUDA 12.5
detectron2 arch flags            8.9
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.0+cu121 @/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0,1                          NVIDIA L40S (arch=8.9)
Driver version                   555.42.06
CUDA_HOME                        /usr/local/cuda
Pillow                           9.4.0
torchvision                      0.19.0+cu121 @/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torchvision
torchvision arch flags           5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.10.0
-------------------------------  -------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX512
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.0, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/08 15:42:52] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco/panoptic-segmentation/fcclip/r50_exp_1000_a_decoder.yaml', dist_url='tcp://127.0.0.1:51163', eval_only=False, machine_rank=0, num_gpus=1, num_machines=1, opts=[], resume=False)
[10/08 15:42:52] detectron2 INFO: Contents of args.config_file=configs/coco/panoptic-segmentation/fcclip/r50_exp_1000_a_decoder.yaml:
_BASE_: ./fcclip_convnext_large_eval_ade20k_r50.yaml

INPUT:
  MIN_SIZE_TEST: 1024
  MAX_SIZE_TEST: 2560

MODEL:
  SEM_SEG_HEAD:
    NUM_CLASSES: 19
  WEIGHTS: /tsi/hi-paris/GB/segmentation/models/fcclip_cocopan_r50.pth
OUTPUT_DIR: /tsi/hi-paris/GB/segmentation/results/In_vocab/r50_008_1000_19_a_decoder
DATASETS:
  TRAIN: ("openvocab_cityscapes_fine_panoptic_train",)
  TEST: ("openvocab_cityscapes_fine_panoptic_val",)
SOLVER:
  IMS_PER_BATCH: 8
  MAX_ITER: 1000
TEST:
  EVAL_PERIOD: 1000


[10/08 15:42:53] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - openvocab_cityscapes_fine_panoptic_val
  TRAIN:
  - openvocab_cityscapes_fine_panoptic_train
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: false
  CROP:
    ENABLED: false
    MINIMUM_INST_AREA: 1
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  DATASET_MAPPER_NAME: coco_panoptic_lsj
  FORMAT: RGB
  IMAGE_SIZE: 1024
  MASK_FORMAT: polygon
  MAX_SCALE: 2.0
  MAX_SIZE_TEST: 2560
  MAX_SIZE_TRAIN: 1333
  MIN_SCALE: 0.1
  MIN_SIZE_TEST: 1024
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: -1
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 0
    NAME: CLIP
  DEVICE: cuda
  FC_CLIP:
    CLIP_MODEL_NAME: RN50
    CLIP_PRETRAINED_WEIGHTS: openai
    EMBED_DIM: 1024
    ENSEMBLE_ON_VALID_MASK: true
    GEOMETRIC_ENSEMBLE_ALPHA: 0.4
    GEOMETRIC_ENSEMBLE_BETA: 0.8
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_FORMER:
    CLASS_WEIGHT: 2.0
    DEC_LAYERS: 10
    DEEP_SUPERVISION: true
    DICE_WEIGHT: 5.0
    DIM_FEEDFORWARD: 2048
    DROPOUT: 0.0
    ENC_LAYERS: 0
    ENFORCE_INPUT_PROJ: false
    HIDDEN_DIM: 256
    IMPORTANCE_SAMPLE_RATIO: 0.75
    MASK_WEIGHT: 5.0
    NHEADS: 8
    NO_OBJECT_WEIGHT: 0.1
    NUM_OBJECT_QUERIES: 250
    OVERSAMPLE_RATIO: 3.0
    PRE_NORM: false
    SIZE_DIVISIBILITY: 32
    TEST:
      INSTANCE_ON: true
      OBJECT_MASK_THRESHOLD: 0.0
      OVERLAP_THRESHOLD: 0.8
      PANOPTIC_ON: true
      SEMANTIC_ON: true
      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: false
    TRAIN_NUM_POINTS: 12544
    TRANSFORMER_DECODER_NAME: MultiScaleMaskedTransformerDecoder
    TRANSFORMER_IN_FEATURE: multi_scale_pixel_decoder
  MASK_ON: false
  META_ARCHITECTURE: FCCLIP
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 122.7709383
  - 116.7460125
  - 104.09373615
  PIXEL_STD:
  - 68.5005327
  - 66.6321579
  - 70.32316305
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 1
    - 1
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: basic
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 256
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES:
    - res3
    - res4
    - res5
    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8
    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    MASK_DIM: 256
    NAME: FCCLIPHead
    NORM: GN
    NUM_CLASSES: 19
    PIXEL_DECODER_NAME: MSDeformAttnPixelDecoder
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    TRANSFORMER_ENC_LAYERS: 6
    USE_DEPTHWISE_SEPARABLE_CONV: false
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    USE_CHECKPOINT: false
    WINDOW_SIZE: 7
  WEIGHTS: /tsi/hi-paris/GB/segmentation/models/fcclip_cocopan_r50.pth
OUTPUT_DIR: /tsi/hi-paris/GB/segmentation/results/In_vocab/r50_008_1000_19_a_decoder
SEED: -1
SOLVER:
  AMP:
    ENABLED: true
  BACKBONE_MULTIPLIER: 0.1
  BASE_LR: 0.0001
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 5000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.01
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 8
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 1000
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 327778
  - 355092
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 1000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[10/08 15:42:53] detectron2 INFO: Full config saved to /tsi/hi-paris/GB/segmentation/results/In_vocab/r50_008_1000_19_a_decoder/config.yaml
[10/08 15:42:53] d2.utils.env INFO: Using a generated random seed 54212037
[10/08 15:42:56] d2.engine.defaults INFO: Model:
FCCLIP(
  (backbone): CLIP(
    (clip_model): CLIP(
      (visual): ModifiedResNet(
        (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
        (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        (layer1): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
            (downsample): Sequential(
              (-1): AvgPool2d(kernel_size=1, stride=1, padding=0)
              (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
        )
        (layer2): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
            (downsample): Sequential(
              (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (3): Bottleneck(
            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
        )
        (layer3): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
            (downsample): Sequential(
              (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
              (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (3): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (4): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (5): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
        )
        (layer4): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
            (downsample): Sequential(
              (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
              (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
        )
        (attnpool): AttentionPool2d(
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (c_proj): Linear(in_features=2048, out_features=1024, bias=True)
        )
      )
      (transformer): Transformer(
        (resblocks): ModuleList(
          (0-11): 12 x ResidualAttentionBlock(
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ls_2): Identity()
          )
        )
      )
      (token_embedding): Embedding(49408, 512)
      (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
  )
  (sem_seg_head): FCCLIPHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0-8): 9 x SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0-8): 9 x CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0-8): 9 x FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(250, 256)
      (query_embed): Embedding(250, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0-2): 3 x Sequential()
      )
      (mask_embed): MLP(
        (layers): ModuleList(
          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (mask_pooling): MaskPooling()
      (_mask_pooling_proj): Sequential(
        (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
      )
      (class_embed): MLP(
        (layers): ModuleList(
          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=1024, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 19
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
  (conv1): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv2): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU()
  (mask_pooling): MaskPooling()
  (decoder_adapter): DecoderAdapter(
    (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
    (conv2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    (norm1): LayerNorm((64, 1, 1), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((256, 1, 1), eps=1e-05, elementwise_affine=True)
    (relu): ReLU()
  )
  (void_embedding): Embedding(1, 1024)
)
[10/08 15:42:56] fcclip.data.dataset_mappers.mask_former_semantic_dataset_mapper INFO: [MaskFormerPanopticDatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(800,), max_size=1333, sample_style='choice'), RandomFlip()]
[10/08 15:42:57] fcclip.data.datasets.register_cityscapes_panoptic INFO: 18 cities found in 'datasets/cityscapes/leftImg8bit/train'.
[10/08 15:42:57] d2.data.build INFO: Using training sampler TrainingSampler
[10/08 15:42:57] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/08 15:42:57] d2.data.common INFO: Serializing 2975 elements to byte tensors and concatenating them all ...
[10/08 15:42:57] d2.data.common INFO: Serialized dataset takes 4.12 MiB
[10/08 15:42:57] d2.data.build INFO: Making batched data loader with batch_size=8
[10/08 15:42:57] d2.solver.build WARNING: SOLVER.STEPS contains values larger than SOLVER.MAX_ITER. These values will be ignored.
[10/08 15:42:57] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /tsi/hi-paris/GB/segmentation/models/fcclip_cocopan_r50.pth ...
[10/08 15:42:57] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /tsi/hi-paris/GB/segmentation/models/fcclip_cocopan_r50.pth ...
[10/08 15:42:57] fvcore.common.checkpoint WARNING: Skip loading parameter 'criterion.empty_weight' to the model due to incompatible shapes: (134,) in the checkpoint but (20,) in the model! You might want to double check if this is expected.
[10/08 15:42:57] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.clip_model.ln_final.{bias, weight}[0m
[34mbackbone.clip_model.token_embedding.weight[0m
[34mbackbone.clip_model.transformer.resblocks.0.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.attnpool.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.attnpool.k_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.attnpool.positional_embedding[0m
[34mbackbone.clip_model.visual.attnpool.q_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.attnpool.v_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.conv1.weight[0m
[34mbackbone.clip_model.visual.conv2.weight[0m
[34mbackbone.clip_model.visual.conv3.weight[0m
[34mbackbone.clip_model.visual.layer1.0.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.0.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.0.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.0.conv1.weight[0m
[34mbackbone.clip_model.visual.layer1.0.conv2.weight[0m
[34mbackbone.clip_model.visual.layer1.0.conv3.weight[0m
[34mbackbone.clip_model.visual.layer1.0.downsample.0.weight[0m
[34mbackbone.clip_model.visual.layer1.0.downsample.1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.1.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.1.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.1.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.1.conv1.weight[0m
[34mbackbone.clip_model.visual.layer1.1.conv2.weight[0m
[34mbackbone.clip_model.visual.layer1.1.conv3.weight[0m
[34mbackbone.clip_model.visual.layer1.2.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.2.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.2.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.2.conv1.weight[0m
[34mbackbone.clip_model.visual.layer1.2.conv2.weight[0m
[34mbackbone.clip_model.visual.layer1.2.conv3.weight[0m
[34mbackbone.clip_model.visual.layer2.0.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.0.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.0.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.0.conv1.weight[0m
[34mbackbone.clip_model.visual.layer2.0.conv2.weight[0m
[34mbackbone.clip_model.visual.layer2.0.conv3.weight[0m
[34mbackbone.clip_model.visual.layer2.0.downsample.0.weight[0m
[34mbackbone.clip_model.visual.layer2.0.downsample.1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.1.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.1.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.1.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.1.conv1.weight[0m
[34mbackbone.clip_model.visual.layer2.1.conv2.weight[0m
[34mbackbone.clip_model.visual.layer2.1.conv3.weight[0m
[34mbackbone.clip_model.visual.layer2.2.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.2.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.2.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.2.conv1.weight[0m
[34mbackbone.clip_model.visual.layer2.2.conv2.weight[0m
[34mbackbone.clip_model.visual.layer2.2.conv3.weight[0m
[34mbackbone.clip_model.visual.layer2.3.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.3.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.3.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.3.conv1.weight[0m
[34mbackbone.clip_model.visual.layer2.3.conv2.weight[0m
[34mbackbone.clip_model.visual.layer2.3.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.0.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.0.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.0.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.0.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.0.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.0.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.0.downsample.0.weight[0m
[34mbackbone.clip_model.visual.layer3.0.downsample.1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.1.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.1.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.1.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.1.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.1.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.1.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.2.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.2.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.2.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.2.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.2.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.2.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.3.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.3.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.3.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.3.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.3.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.3.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.4.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.4.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.4.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.4.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.4.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.4.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.5.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.5.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.5.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.5.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.5.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.5.conv3.weight[0m
[34mbackbone.clip_model.visual.layer4.0.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.0.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.0.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.0.conv1.weight[0m
[34mbackbone.clip_model.visual.layer4.0.conv2.weight[0m
[34mbackbone.clip_model.visual.layer4.0.conv3.weight[0m
[34mbackbone.clip_model.visual.layer4.0.downsample.0.weight[0m
[34mbackbone.clip_model.visual.layer4.0.downsample.1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.1.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.1.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.1.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.1.conv1.weight[0m
[34mbackbone.clip_model.visual.layer4.1.conv2.weight[0m
[34mbackbone.clip_model.visual.layer4.1.conv3.weight[0m
[34mbackbone.clip_model.visual.layer4.2.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.2.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.2.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.2.conv1.weight[0m
[34mbackbone.clip_model.visual.layer4.2.conv2.weight[0m
[34mbackbone.clip_model.visual.layer4.2.conv3.weight[0m
[34mbackbone.clip_model.{logit_scale, positional_embedding, text_projection}[0m
[34mbn1.{bias, running_mean, running_var, weight}[0m
[34mbn2.{bias, running_mean, running_var, weight}[0m
[34mconv1.{bias, weight}[0m
[34mconv2.{bias, weight}[0m
[34mcriterion.empty_weight[0m
[34mdecoder_adapter.conv1.{bias, weight}[0m
[34mdecoder_adapter.conv2.{bias, weight}[0m
[34mdecoder_adapter.norm1.{bias, weight}[0m
[34mdecoder_adapter.norm2.{bias, weight}[0m
[10/08 15:42:57] d2.engine.train_loop INFO: Starting training from iteration 0
[10/08 15:43:21] d2.utils.events INFO:  eta: 0:16:43  iter: 19  total_loss: 30.47  loss_ce: 1.127  loss_mask: 0.2928  loss_dice: 1.448  loss_ce_0: 1.406  loss_mask_0: 0.3155  loss_dice_0: 1.737  loss_ce_1: 1.428  loss_mask_1: 0.3057  loss_dice_1: 1.592  loss_ce_2: 1.277  loss_mask_2: 0.3006  loss_dice_2: 1.536  loss_ce_3: 1.23  loss_mask_3: 0.2944  loss_dice_3: 1.483  loss_ce_4: 1.175  loss_mask_4: 0.2985  loss_dice_4: 1.479  loss_ce_5: 1.166  loss_mask_5: 0.2968  loss_dice_5: 1.42  loss_ce_6: 1.155  loss_mask_6: 0.2921  loss_dice_6: 1.428  loss_ce_7: 1.112  loss_mask_7: 0.2906  loss_dice_7: 1.451  loss_ce_8: 1.144  loss_mask_8: 0.2926  loss_dice_8: 1.427    time: 1.0438  last_time: 1.0026  data_time: 0.1089  last_data_time: 0.0513   lr: 0.0001  max_mem: 30783M
[10/08 15:43:42] d2.utils.events INFO:  eta: 0:16:31  iter: 39  total_loss: 24.84  loss_ce: 0.7353  loss_mask: 0.2261  loss_dice: 1.379  loss_ce_0: 0.8439  loss_mask_0: 0.2632  loss_dice_0: 1.705  loss_ce_1: 0.9702  loss_mask_1: 0.2455  loss_dice_1: 1.548  loss_ce_2: 0.9011  loss_mask_2: 0.2359  loss_dice_2: 1.464  loss_ce_3: 0.8364  loss_mask_3: 0.2298  loss_dice_3: 1.396  loss_ce_4: 0.7691  loss_mask_4: 0.2296  loss_dice_4: 1.39  loss_ce_5: 0.7461  loss_mask_5: 0.2301  loss_dice_5: 1.4  loss_ce_6: 0.7485  loss_mask_6: 0.2281  loss_dice_6: 1.4  loss_ce_7: 0.7445  loss_mask_7: 0.2288  loss_dice_7: 1.394  loss_ce_8: 0.7463  loss_mask_8: 0.2301  loss_dice_8: 1.398    time: 1.0476  last_time: 1.0555  data_time: 0.0693  last_data_time: 0.0906   lr: 0.0001  max_mem: 31843M
[10/08 15:44:03] d2.utils.events INFO:  eta: 0:16:23  iter: 59  total_loss: 24.24  loss_ce: 0.7117  loss_mask: 0.1981  loss_dice: 1.385  loss_ce_0: 0.7309  loss_mask_0: 0.2416  loss_dice_0: 1.786  loss_ce_1: 0.8931  loss_mask_1: 0.2273  loss_dice_1: 1.552  loss_ce_2: 0.8078  loss_mask_2: 0.2138  loss_dice_2: 1.481  loss_ce_3: 0.7569  loss_mask_3: 0.2052  loss_dice_3: 1.413  loss_ce_4: 0.7078  loss_mask_4: 0.2027  loss_dice_4: 1.442  loss_ce_5: 0.6986  loss_mask_5: 0.1997  loss_dice_5: 1.417  loss_ce_6: 0.71  loss_mask_6: 0.1989  loss_dice_6: 1.395  loss_ce_7: 0.7016  loss_mask_7: 0.2001  loss_dice_7: 1.388  loss_ce_8: 0.6851  loss_mask_8: 0.2023  loss_dice_8: 1.398    time: 1.0497  last_time: 1.0290  data_time: 0.0698  last_data_time: 0.0678   lr: 0.0001  max_mem: 31843M
[10/08 15:44:24] d2.utils.events INFO:  eta: 0:16:01  iter: 79  total_loss: 23.55  loss_ce: 0.6431  loss_mask: 0.2186  loss_dice: 1.312  loss_ce_0: 0.7208  loss_mask_0: 0.2407  loss_dice_0: 1.632  loss_ce_1: 0.8701  loss_mask_1: 0.2289  loss_dice_1: 1.491  loss_ce_2: 0.763  loss_mask_2: 0.2202  loss_dice_2: 1.434  loss_ce_3: 0.6982  loss_mask_3: 0.2177  loss_dice_3: 1.37  loss_ce_4: 0.6804  loss_mask_4: 0.2181  loss_dice_4: 1.326  loss_ce_5: 0.6642  loss_mask_5: 0.2222  loss_dice_5: 1.359  loss_ce_6: 0.6763  loss_mask_6: 0.2167  loss_dice_6: 1.311  loss_ce_7: 0.6379  loss_mask_7: 0.2197  loss_dice_7: 1.325  loss_ce_8: 0.6891  loss_mask_8: 0.2202  loss_dice_8: 1.323    time: 1.0492  last_time: 1.1412  data_time: 0.0677  last_data_time: 0.0855   lr: 0.0001  max_mem: 32049M
[10/08 15:44:47] d2.utils.events INFO:  eta: 0:15:45  iter: 99  total_loss: 23.08  loss_ce: 0.6189  loss_mask: 0.2138  loss_dice: 1.389  loss_ce_0: 0.6945  loss_mask_0: 0.2492  loss_dice_0: 1.614  loss_ce_1: 0.7945  loss_mask_1: 0.2343  loss_dice_1: 1.529  loss_ce_2: 0.7263  loss_mask_2: 0.2197  loss_dice_2: 1.472  loss_ce_3: 0.6753  loss_mask_3: 0.2184  loss_dice_3: 1.41  loss_ce_4: 0.6497  loss_mask_4: 0.2206  loss_dice_4: 1.362  loss_ce_5: 0.6259  loss_mask_5: 0.2153  loss_dice_5: 1.398  loss_ce_6: 0.6543  loss_mask_6: 0.2142  loss_dice_6: 1.337  loss_ce_7: 0.6447  loss_mask_7: 0.2149  loss_dice_7: 1.364  loss_ce_8: 0.63  loss_mask_8: 0.2171  loss_dice_8: 1.361    time: 1.0621  last_time: 1.1187  data_time: 0.0812  last_data_time: 0.0930   lr: 0.0001  max_mem: 33079M
[10/08 15:45:09] d2.utils.events INFO:  eta: 0:15:29  iter: 119  total_loss: 22.8  loss_ce: 0.6406  loss_mask: 0.191  loss_dice: 1.277  loss_ce_0: 0.7172  loss_mask_0: 0.2165  loss_dice_0: 1.568  loss_ce_1: 0.8887  loss_mask_1: 0.2104  loss_dice_1: 1.477  loss_ce_2: 0.7467  loss_mask_2: 0.1994  loss_dice_2: 1.404  loss_ce_3: 0.6909  loss_mask_3: 0.1941  loss_dice_3: 1.285  loss_ce_4: 0.6776  loss_mask_4: 0.1998  loss_dice_4: 1.309  loss_ce_5: 0.6884  loss_mask_5: 0.1935  loss_dice_5: 1.323  loss_ce_6: 0.6576  loss_mask_6: 0.1952  loss_dice_6: 1.313  loss_ce_7: 0.6596  loss_mask_7: 0.1964  loss_dice_7: 1.267  loss_ce_8: 0.6578  loss_mask_8: 0.1955  loss_dice_8: 1.289    time: 1.0707  last_time: 1.1401  data_time: 0.0864  last_data_time: 0.0674   lr: 0.0001  max_mem: 33079M
[10/08 15:45:31] d2.utils.events INFO:  eta: 0:15:17  iter: 139  total_loss: 21.84  loss_ce: 0.5988  loss_mask: 0.1917  loss_dice: 1.281  loss_ce_0: 0.6737  loss_mask_0: 0.2149  loss_dice_0: 1.591  loss_ce_1: 0.7747  loss_mask_1: 0.1972  loss_dice_1: 1.432  loss_ce_2: 0.6922  loss_mask_2: 0.1925  loss_dice_2: 1.432  loss_ce_3: 0.6363  loss_mask_3: 0.1904  loss_dice_3: 1.334  loss_ce_4: 0.6008  loss_mask_4: 0.1943  loss_dice_4: 1.337  loss_ce_5: 0.602  loss_mask_5: 0.187  loss_dice_5: 1.295  loss_ce_6: 0.6157  loss_mask_6: 0.1887  loss_dice_6: 1.289  loss_ce_7: 0.5792  loss_mask_7: 0.1897  loss_dice_7: 1.252  loss_ce_8: 0.5995  loss_mask_8: 0.1867  loss_dice_8: 1.288    time: 1.0761  last_time: 1.0722  data_time: 0.0829  last_data_time: 0.0721   lr: 0.0001  max_mem: 33079M
[10/08 15:45:53] d2.utils.events INFO:  eta: 0:15:02  iter: 159  total_loss: 21.05  loss_ce: 0.5505  loss_mask: 0.2015  loss_dice: 1.213  loss_ce_0: 0.6149  loss_mask_0: 0.235  loss_dice_0: 1.492  loss_ce_1: 0.7347  loss_mask_1: 0.2156  loss_dice_1: 1.388  loss_ce_2: 0.6618  loss_mask_2: 0.2002  loss_dice_2: 1.321  loss_ce_3: 0.6359  loss_mask_3: 0.2071  loss_dice_3: 1.248  loss_ce_4: 0.5885  loss_mask_4: 0.2057  loss_dice_4: 1.27  loss_ce_5: 0.5795  loss_mask_5: 0.201  loss_dice_5: 1.264  loss_ce_6: 0.5387  loss_mask_6: 0.202  loss_dice_6: 1.222  loss_ce_7: 0.5616  loss_mask_7: 0.1997  loss_dice_7: 1.215  loss_ce_8: 0.5608  loss_mask_8: 0.2008  loss_dice_8: 1.23    time: 1.0813  last_time: 1.1789  data_time: 0.0836  last_data_time: 0.0858   lr: 0.0001  max_mem: 33079M
[10/08 15:46:15] d2.utils.events INFO:  eta: 0:14:41  iter: 179  total_loss: 21.97  loss_ce: 0.5718  loss_mask: 0.2102  loss_dice: 1.315  loss_ce_0: 0.6476  loss_mask_0: 0.2534  loss_dice_0: 1.57  loss_ce_1: 0.7545  loss_mask_1: 0.2173  loss_dice_1: 1.464  loss_ce_2: 0.6508  loss_mask_2: 0.2122  loss_dice_2: 1.404  loss_ce_3: 0.6176  loss_mask_3: 0.2139  loss_dice_3: 1.343  loss_ce_4: 0.6002  loss_mask_4: 0.2162  loss_dice_4: 1.375  loss_ce_5: 0.6013  loss_mask_5: 0.2104  loss_dice_5: 1.343  loss_ce_6: 0.5792  loss_mask_6: 0.2101  loss_dice_6: 1.303  loss_ce_7: 0.5627  loss_mask_7: 0.2106  loss_dice_7: 1.306  loss_ce_8: 0.5627  loss_mask_8: 0.2091  loss_dice_8: 1.338    time: 1.0831  last_time: 1.0652  data_time: 0.0810  last_data_time: 0.0692   lr: 0.0001  max_mem: 33079M
[10/08 15:46:38] d2.utils.events INFO:  eta: 0:14:24  iter: 199  total_loss: 22.03  loss_ce: 0.5627  loss_mask: 0.1939  loss_dice: 1.334  loss_ce_0: 0.6394  loss_mask_0: 0.2337  loss_dice_0: 1.577  loss_ce_1: 0.7325  loss_mask_1: 0.2053  loss_dice_1: 1.497  loss_ce_2: 0.6677  loss_mask_2: 0.1969  loss_dice_2: 1.404  loss_ce_3: 0.6026  loss_mask_3: 0.1999  loss_dice_3: 1.376  loss_ce_4: 0.5472  loss_mask_4: 0.194  loss_dice_4: 1.349  loss_ce_5: 0.5754  loss_mask_5: 0.1971  loss_dice_5: 1.34  loss_ce_6: 0.5843  loss_mask_6: 0.1938  loss_dice_6: 1.336  loss_ce_7: 0.566  loss_mask_7: 0.1959  loss_dice_7: 1.304  loss_ce_8: 0.5613  loss_mask_8: 0.192  loss_dice_8: 1.328    time: 1.0851  last_time: 1.1876  data_time: 0.0802  last_data_time: 0.0860   lr: 0.0001  max_mem: 33079M
[10/08 15:47:00] d2.utils.events INFO:  eta: 0:14:10  iter: 219  total_loss: 21.33  loss_ce: 0.576  loss_mask: 0.1792  loss_dice: 1.269  loss_ce_0: 0.6598  loss_mask_0: 0.2098  loss_dice_0: 1.542  loss_ce_1: 0.7336  loss_mask_1: 0.2012  loss_dice_1: 1.46  loss_ce_2: 0.6806  loss_mask_2: 0.1898  loss_dice_2: 1.349  loss_ce_3: 0.5866  loss_mask_3: 0.1819  loss_dice_3: 1.313  loss_ce_4: 0.5381  loss_mask_4: 0.1876  loss_dice_4: 1.312  loss_ce_5: 0.5631  loss_mask_5: 0.1883  loss_dice_5: 1.299  loss_ce_6: 0.5644  loss_mask_6: 0.184  loss_dice_6: 1.286  loss_ce_7: 0.5504  loss_mask_7: 0.1836  loss_dice_7: 1.253  loss_ce_8: 0.5599  loss_mask_8: 0.1797  loss_dice_8: 1.298    time: 1.0894  last_time: 1.1531  data_time: 0.0875  last_data_time: 0.0892   lr: 0.0001  max_mem: 33079M
[10/08 15:47:23] d2.utils.events INFO:  eta: 0:13:53  iter: 239  total_loss: 22.47  loss_ce: 0.6284  loss_mask: 0.1813  loss_dice: 1.351  loss_ce_0: 0.646  loss_mask_0: 0.2199  loss_dice_0: 1.573  loss_ce_1: 0.7719  loss_mask_1: 0.1952  loss_dice_1: 1.483  loss_ce_2: 0.7  loss_mask_2: 0.1918  loss_dice_2: 1.433  loss_ce_3: 0.6453  loss_mask_3: 0.1869  loss_dice_3: 1.381  loss_ce_4: 0.6015  loss_mask_4: 0.1856  loss_dice_4: 1.361  loss_ce_5: 0.6199  loss_mask_5: 0.1867  loss_dice_5: 1.359  loss_ce_6: 0.608  loss_mask_6: 0.1829  loss_dice_6: 1.348  loss_ce_7: 0.602  loss_mask_7: 0.1857  loss_dice_7: 1.347  loss_ce_8: 0.6163  loss_mask_8: 0.1791  loss_dice_8: 1.35    time: 1.0931  last_time: 1.0842  data_time: 0.0923  last_data_time: 0.0945   lr: 0.0001  max_mem: 33079M
[10/08 15:47:46] d2.utils.events INFO:  eta: 0:13:35  iter: 259  total_loss: 22.52  loss_ce: 0.5918  loss_mask: 0.1961  loss_dice: 1.329  loss_ce_0: 0.6868  loss_mask_0: 0.2238  loss_dice_0: 1.579  loss_ce_1: 0.7432  loss_mask_1: 0.2157  loss_dice_1: 1.495  loss_ce_2: 0.6748  loss_mask_2: 0.2043  loss_dice_2: 1.406  loss_ce_3: 0.624  loss_mask_3: 0.1984  loss_dice_3: 1.361  loss_ce_4: 0.5785  loss_mask_4: 0.1958  loss_dice_4: 1.336  loss_ce_5: 0.5903  loss_mask_5: 0.1957  loss_dice_5: 1.319  loss_ce_6: 0.597  loss_mask_6: 0.1959  loss_dice_6: 1.333  loss_ce_7: 0.5881  loss_mask_7: 0.1946  loss_dice_7: 1.314  loss_ce_8: 0.5804  loss_mask_8: 0.1954  loss_dice_8: 1.307    time: 1.0956  last_time: 1.0900  data_time: 0.0851  last_data_time: 0.0857   lr: 0.0001  max_mem: 33079M
[10/08 15:48:09] d2.utils.events INFO:  eta: 0:13:16  iter: 279  total_loss: 21.43  loss_ce: 0.5899  loss_mask: 0.1976  loss_dice: 1.268  loss_ce_0: 0.6297  loss_mask_0: 0.2236  loss_dice_0: 1.53  loss_ce_1: 0.7304  loss_mask_1: 0.2113  loss_dice_1: 1.454  loss_ce_2: 0.6968  loss_mask_2: 0.2029  loss_dice_2: 1.369  loss_ce_3: 0.6183  loss_mask_3: 0.208  loss_dice_3: 1.318  loss_ce_4: 0.584  loss_mask_4: 0.1966  loss_dice_4: 1.284  loss_ce_5: 0.604  loss_mask_5: 0.1987  loss_dice_5: 1.308  loss_ce_6: 0.6097  loss_mask_6: 0.1984  loss_dice_6: 1.258  loss_ce_7: 0.5831  loss_mask_7: 0.1988  loss_dice_7: 1.301  loss_ce_8: 0.5448  loss_mask_8: 0.1981  loss_dice_8: 1.257    time: 1.1000  last_time: 1.0688  data_time: 0.0914  last_data_time: 0.0904   lr: 0.0001  max_mem: 33079M
[10/08 15:48:32] d2.utils.events INFO:  eta: 0:12:56  iter: 299  total_loss: 22.31  loss_ce: 0.5635  loss_mask: 0.1819  loss_dice: 1.331  loss_ce_0: 0.6828  loss_mask_0: 0.2099  loss_dice_0: 1.589  loss_ce_1: 0.7896  loss_mask_1: 0.1979  loss_dice_1: 1.476  loss_ce_2: 0.6903  loss_mask_2: 0.1861  loss_dice_2: 1.443  loss_ce_3: 0.6423  loss_mask_3: 0.1858  loss_dice_3: 1.382  loss_ce_4: 0.6264  loss_mask_4: 0.1848  loss_dice_4: 1.331  loss_ce_5: 0.6072  loss_mask_5: 0.1817  loss_dice_5: 1.341  loss_ce_6: 0.6051  loss_mask_6: 0.1818  loss_dice_6: 1.33  loss_ce_7: 0.567  loss_mask_7: 0.1791  loss_dice_7: 1.341  loss_ce_8: 0.5706  loss_mask_8: 0.1809  loss_dice_8: 1.328    time: 1.1034  last_time: 1.1150  data_time: 0.1009  last_data_time: 0.0595   lr: 0.0001  max_mem: 33079M
[10/08 15:48:54] d2.utils.events INFO:  eta: 0:12:34  iter: 319  total_loss: 20.85  loss_ce: 0.5791  loss_mask: 0.1769  loss_dice: 1.26  loss_ce_0: 0.623  loss_mask_0: 0.2013  loss_dice_0: 1.522  loss_ce_1: 0.6959  loss_mask_1: 0.1898  loss_dice_1: 1.371  loss_ce_2: 0.654  loss_mask_2: 0.1833  loss_dice_2: 1.371  loss_ce_3: 0.5994  loss_mask_3: 0.1851  loss_dice_3: 1.279  loss_ce_4: 0.5901  loss_mask_4: 0.1769  loss_dice_4: 1.248  loss_ce_5: 0.558  loss_mask_5: 0.1771  loss_dice_5: 1.272  loss_ce_6: 0.5634  loss_mask_6: 0.176  loss_dice_6: 1.27  loss_ce_7: 0.5281  loss_mask_7: 0.1767  loss_dice_7: 1.25  loss_ce_8: 0.5377  loss_mask_8: 0.1764  loss_dice_8: 1.27    time: 1.1042  last_time: 1.0839  data_time: 0.0848  last_data_time: 0.0778   lr: 0.0001  max_mem: 33079M
[10/08 15:49:17] d2.utils.events INFO:  eta: 0:12:14  iter: 339  total_loss: 22.24  loss_ce: 0.6036  loss_mask: 0.1777  loss_dice: 1.269  loss_ce_0: 0.6514  loss_mask_0: 0.2061  loss_dice_0: 1.541  loss_ce_1: 0.7487  loss_mask_1: 0.1852  loss_dice_1: 1.466  loss_ce_2: 0.6688  loss_mask_2: 0.1833  loss_dice_2: 1.363  loss_ce_3: 0.6437  loss_mask_3: 0.1839  loss_dice_3: 1.355  loss_ce_4: 0.5923  loss_mask_4: 0.1807  loss_dice_4: 1.364  loss_ce_5: 0.5992  loss_mask_5: 0.1806  loss_dice_5: 1.312  loss_ce_6: 0.5979  loss_mask_6: 0.181  loss_dice_6: 1.293  loss_ce_7: 0.5865  loss_mask_7: 0.1809  loss_dice_7: 1.312  loss_ce_8: 0.6033  loss_mask_8: 0.178  loss_dice_8: 1.273    time: 1.1073  last_time: 1.1461  data_time: 0.0967  last_data_time: 0.0967   lr: 0.0001  max_mem: 33079M
[10/08 15:49:40] d2.utils.events INFO:  eta: 0:11:53  iter: 359  total_loss: 21.65  loss_ce: 0.5782  loss_mask: 0.1859  loss_dice: 1.33  loss_ce_0: 0.6274  loss_mask_0: 0.2083  loss_dice_0: 1.551  loss_ce_1: 0.7452  loss_mask_1: 0.1965  loss_dice_1: 1.481  loss_ce_2: 0.6747  loss_mask_2: 0.1882  loss_dice_2: 1.438  loss_ce_3: 0.6508  loss_mask_3: 0.1854  loss_dice_3: 1.399  loss_ce_4: 0.5928  loss_mask_4: 0.1892  loss_dice_4: 1.362  loss_ce_5: 0.588  loss_mask_5: 0.1851  loss_dice_5: 1.353  loss_ce_6: 0.5899  loss_mask_6: 0.1854  loss_dice_6: 1.339  loss_ce_7: 0.5672  loss_mask_7: 0.1886  loss_dice_7: 1.346  loss_ce_8: 0.5674  loss_mask_8: 0.184  loss_dice_8: 1.352    time: 1.1088  last_time: 1.1861  data_time: 0.0878  last_data_time: 0.0881   lr: 0.0001  max_mem: 33079M
[10/08 15:50:03] d2.utils.events INFO:  eta: 0:11:31  iter: 379  total_loss: 21.29  loss_ce: 0.5629  loss_mask: 0.1942  loss_dice: 1.245  loss_ce_0: 0.6357  loss_mask_0: 0.2325  loss_dice_0: 1.509  loss_ce_1: 0.7057  loss_mask_1: 0.2059  loss_dice_1: 1.393  loss_ce_2: 0.6885  loss_mask_2: 0.2027  loss_dice_2: 1.352  loss_ce_3: 0.6291  loss_mask_3: 0.1949  loss_dice_3: 1.314  loss_ce_4: 0.5903  loss_mask_4: 0.1928  loss_dice_4: 1.286  loss_ce_5: 0.5922  loss_mask_5: 0.1974  loss_dice_5: 1.287  loss_ce_6: 0.5741  loss_mask_6: 0.1941  loss_dice_6: 1.25  loss_ce_7: 0.5538  loss_mask_7: 0.1959  loss_dice_7: 1.231  loss_ce_8: 0.5469  loss_mask_8: 0.1952  loss_dice_8: 1.236    time: 1.1094  last_time: 1.1408  data_time: 0.0818  last_data_time: 0.0791   lr: 0.0001  max_mem: 33079M
[10/08 15:50:25] d2.utils.events INFO:  eta: 0:11:10  iter: 399  total_loss: 22  loss_ce: 0.5538  loss_mask: 0.1914  loss_dice: 1.321  loss_ce_0: 0.6266  loss_mask_0: 0.2108  loss_dice_0: 1.567  loss_ce_1: 0.7252  loss_mask_1: 0.2046  loss_dice_1: 1.479  loss_ce_2: 0.6761  loss_mask_2: 0.1974  loss_dice_2: 1.412  loss_ce_3: 0.6297  loss_mask_3: 0.1954  loss_dice_3: 1.393  loss_ce_4: 0.591  loss_mask_4: 0.1959  loss_dice_4: 1.374  loss_ce_5: 0.5973  loss_mask_5: 0.1938  loss_dice_5: 1.356  loss_ce_6: 0.5747  loss_mask_6: 0.189  loss_dice_6: 1.322  loss_ce_7: 0.5681  loss_mask_7: 0.1928  loss_dice_7: 1.328  loss_ce_8: 0.5569  loss_mask_8: 0.1922  loss_dice_8: 1.338    time: 1.1110  last_time: 1.0470  data_time: 0.0853  last_data_time: 0.0666   lr: 0.0001  max_mem: 33079M
[10/08 15:50:48] d2.utils.events INFO:  eta: 0:10:48  iter: 419  total_loss: 20.88  loss_ce: 0.5586  loss_mask: 0.1709  loss_dice: 1.249  loss_ce_0: 0.5771  loss_mask_0: 0.2034  loss_dice_0: 1.521  loss_ce_1: 0.6765  loss_mask_1: 0.1867  loss_dice_1: 1.404  loss_ce_2: 0.6209  loss_mask_2: 0.1797  loss_dice_2: 1.327  loss_ce_3: 0.5568  loss_mask_3: 0.1803  loss_dice_3: 1.296  loss_ce_4: 0.5626  loss_mask_4: 0.1736  loss_dice_4: 1.328  loss_ce_5: 0.5364  loss_mask_5: 0.1771  loss_dice_5: 1.305  loss_ce_6: 0.5336  loss_mask_6: 0.1744  loss_dice_6: 1.238  loss_ce_7: 0.5481  loss_mask_7: 0.1734  loss_dice_7: 1.251  loss_ce_8: 0.5349  loss_mask_8: 0.1735  loss_dice_8: 1.244    time: 1.1117  last_time: 1.1275  data_time: 0.0842  last_data_time: 0.1006   lr: 0.0001  max_mem: 33079M
[10/08 15:51:11] d2.utils.events INFO:  eta: 0:10:26  iter: 439  total_loss: 21.18  loss_ce: 0.5465  loss_mask: 0.1727  loss_dice: 1.272  loss_ce_0: 0.5948  loss_mask_0: 0.2045  loss_dice_0: 1.494  loss_ce_1: 0.6704  loss_mask_1: 0.1893  loss_dice_1: 1.421  loss_ce_2: 0.6482  loss_mask_2: 0.1831  loss_dice_2: 1.361  loss_ce_3: 0.5864  loss_mask_3: 0.1757  loss_dice_3: 1.272  loss_ce_4: 0.5539  loss_mask_4: 0.1751  loss_dice_4: 1.289  loss_ce_5: 0.5444  loss_mask_5: 0.1764  loss_dice_5: 1.296  loss_ce_6: 0.5406  loss_mask_6: 0.1733  loss_dice_6: 1.26  loss_ce_7: 0.5683  loss_mask_7: 0.1753  loss_dice_7: 1.23  loss_ce_8: 0.5586  loss_mask_8: 0.1713  loss_dice_8: 1.231    time: 1.1128  last_time: 1.1327  data_time: 0.1003  last_data_time: 0.0817   lr: 0.0001  max_mem: 33079M
[10/08 15:51:34] d2.utils.events INFO:  eta: 0:10:04  iter: 459  total_loss: 20.13  loss_ce: 0.523  loss_mask: 0.1715  loss_dice: 1.193  loss_ce_0: 0.6107  loss_mask_0: 0.2019  loss_dice_0: 1.448  loss_ce_1: 0.696  loss_mask_1: 0.1893  loss_dice_1: 1.346  loss_ce_2: 0.6035  loss_mask_2: 0.1779  loss_dice_2: 1.27  loss_ce_3: 0.5668  loss_mask_3: 0.1745  loss_dice_3: 1.217  loss_ce_4: 0.5215  loss_mask_4: 0.1763  loss_dice_4: 1.226  loss_ce_5: 0.5412  loss_mask_5: 0.1739  loss_dice_5: 1.204  loss_ce_6: 0.5371  loss_mask_6: 0.1721  loss_dice_6: 1.2  loss_ce_7: 0.501  loss_mask_7: 0.1718  loss_dice_7: 1.198  loss_ce_8: 0.5085  loss_mask_8: 0.1715  loss_dice_8: 1.187    time: 1.1139  last_time: 1.1145  data_time: 0.0891  last_data_time: 0.0781   lr: 0.0001  max_mem: 33079M
[10/08 15:51:56] d2.utils.events INFO:  eta: 0:09:42  iter: 479  total_loss: 21.4  loss_ce: 0.5635  loss_mask: 0.1808  loss_dice: 1.313  loss_ce_0: 0.6417  loss_mask_0: 0.2134  loss_dice_0: 1.601  loss_ce_1: 0.7232  loss_mask_1: 0.2007  loss_dice_1: 1.49  loss_ce_2: 0.676  loss_mask_2: 0.187  loss_dice_2: 1.369  loss_ce_3: 0.5742  loss_mask_3: 0.1855  loss_dice_3: 1.358  loss_ce_4: 0.5921  loss_mask_4: 0.1845  loss_dice_4: 1.352  loss_ce_5: 0.5696  loss_mask_5: 0.1834  loss_dice_5: 1.365  loss_ce_6: 0.5749  loss_mask_6: 0.1796  loss_dice_6: 1.335  loss_ce_7: 0.5891  loss_mask_7: 0.1801  loss_dice_7: 1.297  loss_ce_8: 0.585  loss_mask_8: 0.1796  loss_dice_8: 1.335    time: 1.1142  last_time: 1.1244  data_time: 0.0824  last_data_time: 0.0711   lr: 0.0001  max_mem: 33079M
[10/08 15:52:19] d2.utils.events INFO:  eta: 0:09:19  iter: 499  total_loss: 19.86  loss_ce: 0.5073  loss_mask: 0.1792  loss_dice: 1.19  loss_ce_0: 0.5814  loss_mask_0: 0.2112  loss_dice_0: 1.53  loss_ce_1: 0.6537  loss_mask_1: 0.1941  loss_dice_1: 1.333  loss_ce_2: 0.5748  loss_mask_2: 0.1859  loss_dice_2: 1.27  loss_ce_3: 0.5538  loss_mask_3: 0.1872  loss_dice_3: 1.221  loss_ce_4: 0.5003  loss_mask_4: 0.1825  loss_dice_4: 1.233  loss_ce_5: 0.4844  loss_mask_5: 0.1815  loss_dice_5: 1.202  loss_ce_6: 0.519  loss_mask_6: 0.1792  loss_dice_6: 1.185  loss_ce_7: 0.5219  loss_mask_7: 0.179  loss_dice_7: 1.206  loss_ce_8: 0.4946  loss_mask_8: 0.1788  loss_dice_8: 1.204    time: 1.1145  last_time: 1.1366  data_time: 0.0789  last_data_time: 0.0971   lr: 0.0001  max_mem: 33079M
[10/08 15:52:41] d2.utils.events INFO:  eta: 0:08:58  iter: 519  total_loss: 20.35  loss_ce: 0.5191  loss_mask: 0.1552  loss_dice: 1.285  loss_ce_0: 0.5712  loss_mask_0: 0.1785  loss_dice_0: 1.56  loss_ce_1: 0.6568  loss_mask_1: 0.1707  loss_dice_1: 1.398  loss_ce_2: 0.6321  loss_mask_2: 0.1591  loss_dice_2: 1.332  loss_ce_3: 0.5909  loss_mask_3: 0.1559  loss_dice_3: 1.261  loss_ce_4: 0.5429  loss_mask_4: 0.1571  loss_dice_4: 1.288  loss_ce_5: 0.5369  loss_mask_5: 0.1535  loss_dice_5: 1.259  loss_ce_6: 0.5057  loss_mask_6: 0.1559  loss_dice_6: 1.248  loss_ce_7: 0.5207  loss_mask_7: 0.1549  loss_dice_7: 1.284  loss_ce_8: 0.5038  loss_mask_8: 0.1551  loss_dice_8: 1.283    time: 1.1155  last_time: 1.1781  data_time: 0.0865  last_data_time: 0.1164   lr: 0.0001  max_mem: 33079M
[10/08 15:53:04] d2.utils.events INFO:  eta: 0:08:36  iter: 539  total_loss: 21.27  loss_ce: 0.4849  loss_mask: 0.1899  loss_dice: 1.267  loss_ce_0: 0.6154  loss_mask_0: 0.2218  loss_dice_0: 1.458  loss_ce_1: 0.6485  loss_mask_1: 0.204  loss_dice_1: 1.417  loss_ce_2: 0.6096  loss_mask_2: 0.1908  loss_dice_2: 1.327  loss_ce_3: 0.584  loss_mask_3: 0.1983  loss_dice_3: 1.286  loss_ce_4: 0.5291  loss_mask_4: 0.1917  loss_dice_4: 1.266  loss_ce_5: 0.5167  loss_mask_5: 0.19  loss_dice_5: 1.292  loss_ce_6: 0.5666  loss_mask_6: 0.1891  loss_dice_6: 1.271  loss_ce_7: 0.4963  loss_mask_7: 0.1901  loss_dice_7: 1.253  loss_ce_8: 0.5018  loss_mask_8: 0.1887  loss_dice_8: 1.295    time: 1.1160  last_time: 1.1359  data_time: 0.0849  last_data_time: 0.1073   lr: 0.0001  max_mem: 33079M
[10/08 15:53:27] d2.utils.events INFO:  eta: 0:08:14  iter: 559  total_loss: 21.55  loss_ce: 0.5452  loss_mask: 0.1729  loss_dice: 1.279  loss_ce_0: 0.5946  loss_mask_0: 0.2031  loss_dice_0: 1.552  loss_ce_1: 0.6621  loss_mask_1: 0.1822  loss_dice_1: 1.484  loss_ce_2: 0.6199  loss_mask_2: 0.173  loss_dice_2: 1.396  loss_ce_3: 0.6209  loss_mask_3: 0.1718  loss_dice_3: 1.348  loss_ce_4: 0.5879  loss_mask_4: 0.1687  loss_dice_4: 1.334  loss_ce_5: 0.5844  loss_mask_5: 0.167  loss_dice_5: 1.315  loss_ce_6: 0.5497  loss_mask_6: 0.1739  loss_dice_6: 1.307  loss_ce_7: 0.5421  loss_mask_7: 0.1717  loss_dice_7: 1.299  loss_ce_8: 0.5422  loss_mask_8: 0.1712  loss_dice_8: 1.319    time: 1.1169  last_time: 1.2186  data_time: 0.0867  last_data_time: 0.0813   lr: 0.0001  max_mem: 33079M
[10/08 15:53:49] d2.utils.events INFO:  eta: 0:07:51  iter: 579  total_loss: 20.64  loss_ce: 0.5128  loss_mask: 0.1844  loss_dice: 1.246  loss_ce_0: 0.609  loss_mask_0: 0.2095  loss_dice_0: 1.482  loss_ce_1: 0.6399  loss_mask_1: 0.1985  loss_dice_1: 1.442  loss_ce_2: 0.5993  loss_mask_2: 0.1891  loss_dice_2: 1.338  loss_ce_3: 0.5268  loss_mask_3: 0.1885  loss_dice_3: 1.294  loss_ce_4: 0.5419  loss_mask_4: 0.1846  loss_dice_4: 1.301  loss_ce_5: 0.5165  loss_mask_5: 0.1849  loss_dice_5: 1.263  loss_ce_6: 0.5106  loss_mask_6: 0.186  loss_dice_6: 1.267  loss_ce_7: 0.5237  loss_mask_7: 0.184  loss_dice_7: 1.255  loss_ce_8: 0.4929  loss_mask_8: 0.1868  loss_dice_8: 1.263    time: 1.1167  last_time: 1.1340  data_time: 0.0826  last_data_time: 0.1003   lr: 0.0001  max_mem: 33079M
[10/08 15:54:11] d2.utils.events INFO:  eta: 0:07:28  iter: 599  total_loss: 20.4  loss_ce: 0.4861  loss_mask: 0.1885  loss_dice: 1.241  loss_ce_0: 0.6011  loss_mask_0: 0.2197  loss_dice_0: 1.531  loss_ce_1: 0.643  loss_mask_1: 0.2047  loss_dice_1: 1.349  loss_ce_2: 0.5999  loss_mask_2: 0.1916  loss_dice_2: 1.362  loss_ce_3: 0.5658  loss_mask_3: 0.1906  loss_dice_3: 1.269  loss_ce_4: 0.5153  loss_mask_4: 0.1824  loss_dice_4: 1.317  loss_ce_5: 0.5053  loss_mask_5: 0.1872  loss_dice_5: 1.282  loss_ce_6: 0.4895  loss_mask_6: 0.1889  loss_dice_6: 1.258  loss_ce_7: 0.495  loss_mask_7: 0.1914  loss_dice_7: 1.275  loss_ce_8: 0.5038  loss_mask_8: 0.1898  loss_dice_8: 1.288    time: 1.1163  last_time: 1.1099  data_time: 0.0751  last_data_time: 0.0805   lr: 0.0001  max_mem: 33079M
[10/08 15:54:33] d2.utils.events INFO:  eta: 0:07:05  iter: 619  total_loss: 21.17  loss_ce: 0.5422  loss_mask: 0.1849  loss_dice: 1.235  loss_ce_0: 0.6016  loss_mask_0: 0.2084  loss_dice_0: 1.471  loss_ce_1: 0.7255  loss_mask_1: 0.2002  loss_dice_1: 1.426  loss_ce_2: 0.6091  loss_mask_2: 0.1861  loss_dice_2: 1.341  loss_ce_3: 0.5835  loss_mask_3: 0.1854  loss_dice_3: 1.27  loss_ce_4: 0.537  loss_mask_4: 0.188  loss_dice_4: 1.262  loss_ce_5: 0.5499  loss_mask_5: 0.1875  loss_dice_5: 1.272  loss_ce_6: 0.5455  loss_mask_6: 0.1899  loss_dice_6: 1.25  loss_ce_7: 0.5494  loss_mask_7: 0.1885  loss_dice_7: 1.259  loss_ce_8: 0.5503  loss_mask_8: 0.1858  loss_dice_8: 1.27    time: 1.1146  last_time: 1.0383  data_time: 0.0694  last_data_time: 0.0700   lr: 0.0001  max_mem: 33079M
[10/08 15:54:54] d2.utils.events INFO:  eta: 0:06:42  iter: 639  total_loss: 20.47  loss_ce: 0.53  loss_mask: 0.1676  loss_dice: 1.227  loss_ce_0: 0.5824  loss_mask_0: 0.2006  loss_dice_0: 1.424  loss_ce_1: 0.6693  loss_mask_1: 0.1851  loss_dice_1: 1.381  loss_ce_2: 0.6085  loss_mask_2: 0.1681  loss_dice_2: 1.315  loss_ce_3: 0.5658  loss_mask_3: 0.1683  loss_dice_3: 1.259  loss_ce_4: 0.5214  loss_mask_4: 0.1732  loss_dice_4: 1.286  loss_ce_5: 0.5422  loss_mask_5: 0.1702  loss_dice_5: 1.317  loss_ce_6: 0.5086  loss_mask_6: 0.1669  loss_dice_6: 1.241  loss_ce_7: 0.5239  loss_mask_7: 0.1695  loss_dice_7: 1.248  loss_ce_8: 0.5116  loss_mask_8: 0.1695  loss_dice_8: 1.263    time: 1.1138  last_time: 1.1150  data_time: 0.0728  last_data_time: 0.0654   lr: 0.0001  max_mem: 33079M
[10/08 15:55:16] d2.utils.events INFO:  eta: 0:06:19  iter: 659  total_loss: 20.04  loss_ce: 0.5296  loss_mask: 0.1721  loss_dice: 1.217  loss_ce_0: 0.5984  loss_mask_0: 0.2102  loss_dice_0: 1.505  loss_ce_1: 0.7084  loss_mask_1: 0.1876  loss_dice_1: 1.341  loss_ce_2: 0.6278  loss_mask_2: 0.1786  loss_dice_2: 1.289  loss_ce_3: 0.6213  loss_mask_3: 0.179  loss_dice_3: 1.193  loss_ce_4: 0.5754  loss_mask_4: 0.1767  loss_dice_4: 1.243  loss_ce_5: 0.579  loss_mask_5: 0.1743  loss_dice_5: 1.226  loss_ce_6: 0.5364  loss_mask_6: 0.1754  loss_dice_6: 1.212  loss_ce_7: 0.5381  loss_mask_7: 0.1718  loss_dice_7: 1.232  loss_ce_8: 0.5193  loss_mask_8: 0.1722  loss_dice_8: 1.208    time: 1.1118  last_time: 1.0385  data_time: 0.0672  last_data_time: 0.0773   lr: 0.0001  max_mem: 33079M
[10/08 15:55:37] d2.utils.events INFO:  eta: 0:05:56  iter: 679  total_loss: 20.69  loss_ce: 0.4892  loss_mask: 0.1588  loss_dice: 1.26  loss_ce_0: 0.5796  loss_mask_0: 0.1968  loss_dice_0: 1.513  loss_ce_1: 0.6622  loss_mask_1: 0.1744  loss_dice_1: 1.426  loss_ce_2: 0.6134  loss_mask_2: 0.1667  loss_dice_2: 1.343  loss_ce_3: 0.5601  loss_mask_3: 0.1663  loss_dice_3: 1.28  loss_ce_4: 0.5442  loss_mask_4: 0.1633  loss_dice_4: 1.272  loss_ce_5: 0.5298  loss_mask_5: 0.1598  loss_dice_5: 1.293  loss_ce_6: 0.5177  loss_mask_6: 0.1591  loss_dice_6: 1.274  loss_ce_7: 0.5072  loss_mask_7: 0.1608  loss_dice_7: 1.265  loss_ce_8: 0.5244  loss_mask_8: 0.1594  loss_dice_8: 1.273    time: 1.1105  last_time: 1.0477  data_time: 0.0685  last_data_time: 0.0736   lr: 0.0001  max_mem: 33079M
[10/08 15:55:59] d2.utils.events INFO:  eta: 0:05:33  iter: 699  total_loss: 21.52  loss_ce: 0.5258  loss_mask: 0.1697  loss_dice: 1.283  loss_ce_0: 0.6055  loss_mask_0: 0.1986  loss_dice_0: 1.596  loss_ce_1: 0.6984  loss_mask_1: 0.1924  loss_dice_1: 1.432  loss_ce_2: 0.6503  loss_mask_2: 0.1791  loss_dice_2: 1.384  loss_ce_3: 0.5557  loss_mask_3: 0.1768  loss_dice_3: 1.357  loss_ce_4: 0.5402  loss_mask_4: 0.1762  loss_dice_4: 1.369  loss_ce_5: 0.5388  loss_mask_5: 0.174  loss_dice_5: 1.353  loss_ce_6: 0.5463  loss_mask_6: 0.1709  loss_dice_6: 1.301  loss_ce_7: 0.5357  loss_mask_7: 0.1746  loss_dice_7: 1.296  loss_ce_8: 0.5018  loss_mask_8: 0.1713  loss_dice_8: 1.332    time: 1.1097  last_time: 1.0715  data_time: 0.0721  last_data_time: 0.0630   lr: 0.0001  max_mem: 33079M
[10/08 15:56:20] d2.utils.events INFO:  eta: 0:05:11  iter: 719  total_loss: 21.37  loss_ce: 0.5114  loss_mask: 0.1727  loss_dice: 1.313  loss_ce_0: 0.6207  loss_mask_0: 0.2027  loss_dice_0: 1.513  loss_ce_1: 0.7022  loss_mask_1: 0.1901  loss_dice_1: 1.437  loss_ce_2: 0.6312  loss_mask_2: 0.1777  loss_dice_2: 1.366  loss_ce_3: 0.5718  loss_mask_3: 0.1771  loss_dice_3: 1.325  loss_ce_4: 0.5333  loss_mask_4: 0.1764  loss_dice_4: 1.379  loss_ce_5: 0.5036  loss_mask_5: 0.1742  loss_dice_5: 1.347  loss_ce_6: 0.5354  loss_mask_6: 0.1724  loss_dice_6: 1.302  loss_ce_7: 0.5357  loss_mask_7: 0.175  loss_dice_7: 1.329  loss_ce_8: 0.5285  loss_mask_8: 0.1742  loss_dice_8: 1.313    time: 1.1084  last_time: 1.0523  data_time: 0.0714  last_data_time: 0.0548   lr: 0.0001  max_mem: 33079M
[10/08 15:56:41] d2.utils.events INFO:  eta: 0:04:48  iter: 739  total_loss: 20.52  loss_ce: 0.517  loss_mask: 0.1889  loss_dice: 1.261  loss_ce_0: 0.5893  loss_mask_0: 0.2216  loss_dice_0: 1.49  loss_ce_1: 0.6691  loss_mask_1: 0.2029  loss_dice_1: 1.403  loss_ce_2: 0.5762  loss_mask_2: 0.1908  loss_dice_2: 1.332  loss_ce_3: 0.5207  loss_mask_3: 0.1908  loss_dice_3: 1.261  loss_ce_4: 0.5381  loss_mask_4: 0.1888  loss_dice_4: 1.324  loss_ce_5: 0.5507  loss_mask_5: 0.188  loss_dice_5: 1.265  loss_ce_6: 0.5204  loss_mask_6: 0.1872  loss_dice_6: 1.255  loss_ce_7: 0.5029  loss_mask_7: 0.1883  loss_dice_7: 1.25  loss_ce_8: 0.5372  loss_mask_8: 0.189  loss_dice_8: 1.267    time: 1.1067  last_time: 1.0762  data_time: 0.0641  last_data_time: 0.0677   lr: 0.0001  max_mem: 33079M
[10/08 15:57:02] d2.utils.events INFO:  eta: 0:04:25  iter: 759  total_loss: 20.63  loss_ce: 0.5359  loss_mask: 0.1695  loss_dice: 1.246  loss_ce_0: 0.5767  loss_mask_0: 0.1967  loss_dice_0: 1.49  loss_ce_1: 0.6584  loss_mask_1: 0.1846  loss_dice_1: 1.424  loss_ce_2: 0.5964  loss_mask_2: 0.173  loss_dice_2: 1.344  loss_ce_3: 0.5558  loss_mask_3: 0.1754  loss_dice_3: 1.296  loss_ce_4: 0.574  loss_mask_4: 0.1743  loss_dice_4: 1.299  loss_ce_5: 0.5229  loss_mask_5: 0.1732  loss_dice_5: 1.277  loss_ce_6: 0.5215  loss_mask_6: 0.1734  loss_dice_6: 1.264  loss_ce_7: 0.5155  loss_mask_7: 0.1725  loss_dice_7: 1.248  loss_ce_8: 0.4998  loss_mask_8: 0.1723  loss_dice_8: 1.295    time: 1.1054  last_time: 1.0716  data_time: 0.0656  last_data_time: 0.0583   lr: 0.0001  max_mem: 33079M
[10/08 15:57:24] d2.utils.events INFO:  eta: 0:04:03  iter: 779  total_loss: 21.14  loss_ce: 0.5094  loss_mask: 0.173  loss_dice: 1.295  loss_ce_0: 0.5674  loss_mask_0: 0.2069  loss_dice_0: 1.552  loss_ce_1: 0.6668  loss_mask_1: 0.1822  loss_dice_1: 1.461  loss_ce_2: 0.5901  loss_mask_2: 0.1773  loss_dice_2: 1.359  loss_ce_3: 0.5723  loss_mask_3: 0.1753  loss_dice_3: 1.354  loss_ce_4: 0.5422  loss_mask_4: 0.1773  loss_dice_4: 1.355  loss_ce_5: 0.5336  loss_mask_5: 0.1751  loss_dice_5: 1.345  loss_ce_6: 0.531  loss_mask_6: 0.1746  loss_dice_6: 1.31  loss_ce_7: 0.5194  loss_mask_7: 0.1751  loss_dice_7: 1.31  loss_ce_8: 0.5195  loss_mask_8: 0.1723  loss_dice_8: 1.335    time: 1.1044  last_time: 1.1082  data_time: 0.0684  last_data_time: 0.0599   lr: 0.0001  max_mem: 33079M
[10/08 15:57:45] d2.utils.events INFO:  eta: 0:03:40  iter: 799  total_loss: 20.32  loss_ce: 0.4933  loss_mask: 0.1756  loss_dice: 1.263  loss_ce_0: 0.5917  loss_mask_0: 0.1946  loss_dice_0: 1.449  loss_ce_1: 0.6276  loss_mask_1: 0.1815  loss_dice_1: 1.41  loss_ce_2: 0.571  loss_mask_2: 0.1754  loss_dice_2: 1.336  loss_ce_3: 0.5473  loss_mask_3: 0.1759  loss_dice_3: 1.299  loss_ce_4: 0.4994  loss_mask_4: 0.1725  loss_dice_4: 1.311  loss_ce_5: 0.4887  loss_mask_5: 0.1725  loss_dice_5: 1.281  loss_ce_6: 0.4906  loss_mask_6: 0.1738  loss_dice_6: 1.26  loss_ce_7: 0.5166  loss_mask_7: 0.1755  loss_dice_7: 1.28  loss_ce_8: 0.4725  loss_mask_8: 0.1724  loss_dice_8: 1.278    time: 1.1036  last_time: 1.0818  data_time: 0.0714  last_data_time: 0.0676   lr: 0.0001  max_mem: 33079M
[10/08 15:58:06] d2.utils.events INFO:  eta: 0:03:18  iter: 819  total_loss: 19.96  loss_ce: 0.4933  loss_mask: 0.1765  loss_dice: 1.226  loss_ce_0: 0.5801  loss_mask_0: 0.1972  loss_dice_0: 1.439  loss_ce_1: 0.6336  loss_mask_1: 0.1871  loss_dice_1: 1.365  loss_ce_2: 0.5939  loss_mask_2: 0.1772  loss_dice_2: 1.315  loss_ce_3: 0.5403  loss_mask_3: 0.1772  loss_dice_3: 1.284  loss_ce_4: 0.5167  loss_mask_4: 0.1755  loss_dice_4: 1.264  loss_ce_5: 0.5039  loss_mask_5: 0.1755  loss_dice_5: 1.223  loss_ce_6: 0.4936  loss_mask_6: 0.1789  loss_dice_6: 1.257  loss_ce_7: 0.4883  loss_mask_7: 0.1781  loss_dice_7: 1.225  loss_ce_8: 0.5035  loss_mask_8: 0.1749  loss_dice_8: 1.229    time: 1.1026  last_time: 1.0464  data_time: 0.0677  last_data_time: 0.0760   lr: 0.0001  max_mem: 33079M
[10/08 15:58:28] d2.utils.events INFO:  eta: 0:02:56  iter: 839  total_loss: 20.26  loss_ce: 0.4546  loss_mask: 0.1578  loss_dice: 1.244  loss_ce_0: 0.5448  loss_mask_0: 0.1851  loss_dice_0: 1.485  loss_ce_1: 0.6577  loss_mask_1: 0.1696  loss_dice_1: 1.369  loss_ce_2: 0.6199  loss_mask_2: 0.1602  loss_dice_2: 1.303  loss_ce_3: 0.5225  loss_mask_3: 0.1606  loss_dice_3: 1.275  loss_ce_4: 0.4838  loss_mask_4: 0.1588  loss_dice_4: 1.272  loss_ce_5: 0.4789  loss_mask_5: 0.1588  loss_dice_5: 1.266  loss_ce_6: 0.466  loss_mask_6: 0.1556  loss_dice_6: 1.231  loss_ce_7: 0.4628  loss_mask_7: 0.1567  loss_dice_7: 1.245  loss_ce_8: 0.5072  loss_mask_8: 0.1583  loss_dice_8: 1.232    time: 1.1017  last_time: 1.0609  data_time: 0.0674  last_data_time: 0.0623   lr: 0.0001  max_mem: 33079M
[10/08 15:58:49] d2.utils.events INFO:  eta: 0:02:33  iter: 859  total_loss: 20.77  loss_ce: 0.5287  loss_mask: 0.17  loss_dice: 1.247  loss_ce_0: 0.6162  loss_mask_0: 0.2015  loss_dice_0: 1.509  loss_ce_1: 0.6562  loss_mask_1: 0.1874  loss_dice_1: 1.436  loss_ce_2: 0.6404  loss_mask_2: 0.1744  loss_dice_2: 1.364  loss_ce_3: 0.5564  loss_mask_3: 0.1742  loss_dice_3: 1.301  loss_ce_4: 0.5258  loss_mask_4: 0.1708  loss_dice_4: 1.276  loss_ce_5: 0.5426  loss_mask_5: 0.1715  loss_dice_5: 1.269  loss_ce_6: 0.4976  loss_mask_6: 0.1693  loss_dice_6: 1.279  loss_ce_7: 0.5212  loss_mask_7: 0.1698  loss_dice_7: 1.259  loss_ce_8: 0.5235  loss_mask_8: 0.1718  loss_dice_8: 1.234    time: 1.1009  last_time: 1.1425  data_time: 0.0679  last_data_time: 0.0768   lr: 0.0001  max_mem: 33079M
[10/08 15:59:10] d2.utils.events INFO:  eta: 0:02:11  iter: 879  total_loss: 19.39  loss_ce: 0.4465  loss_mask: 0.1687  loss_dice: 1.202  loss_ce_0: 0.5779  loss_mask_0: 0.1963  loss_dice_0: 1.45  loss_ce_1: 0.6456  loss_mask_1: 0.1794  loss_dice_1: 1.35  loss_ce_2: 0.5949  loss_mask_2: 0.1708  loss_dice_2: 1.27  loss_ce_3: 0.5266  loss_mask_3: 0.1722  loss_dice_3: 1.206  loss_ce_4: 0.4927  loss_mask_4: 0.1723  loss_dice_4: 1.223  loss_ce_5: 0.503  loss_mask_5: 0.1706  loss_dice_5: 1.176  loss_ce_6: 0.4719  loss_mask_6: 0.1709  loss_dice_6: 1.184  loss_ce_7: 0.501  loss_mask_7: 0.1705  loss_dice_7: 1.188  loss_ce_8: 0.4586  loss_mask_8: 0.1691  loss_dice_8: 1.199    time: 1.0998  last_time: 1.0794  data_time: 0.0667  last_data_time: 0.0693   lr: 0.0001  max_mem: 33079M
[10/08 15:59:31] d2.utils.events INFO:  eta: 0:01:49  iter: 899  total_loss: 18.9  loss_ce: 0.4219  loss_mask: 0.1759  loss_dice: 1.179  loss_ce_0: 0.5669  loss_mask_0: 0.2025  loss_dice_0: 1.393  loss_ce_1: 0.5978  loss_mask_1: 0.1931  loss_dice_1: 1.306  loss_ce_2: 0.5565  loss_mask_2: 0.1854  loss_dice_2: 1.234  loss_ce_3: 0.4883  loss_mask_3: 0.1827  loss_dice_3: 1.191  loss_ce_4: 0.4781  loss_mask_4: 0.1792  loss_dice_4: 1.185  loss_ce_5: 0.4571  loss_mask_5: 0.1832  loss_dice_5: 1.208  loss_ce_6: 0.4339  loss_mask_6: 0.1831  loss_dice_6: 1.192  loss_ce_7: 0.4393  loss_mask_7: 0.1772  loss_dice_7: 1.181  loss_ce_8: 0.4459  loss_mask_8: 0.1764  loss_dice_8: 1.175    time: 1.0990  last_time: 1.0500  data_time: 0.0675  last_data_time: 0.0643   lr: 0.0001  max_mem: 33079M
[10/08 15:59:53] d2.utils.events INFO:  eta: 0:01:27  iter: 919  total_loss: 20.3  loss_ce: 0.5068  loss_mask: 0.1676  loss_dice: 1.228  loss_ce_0: 0.5888  loss_mask_0: 0.1969  loss_dice_0: 1.455  loss_ce_1: 0.6769  loss_mask_1: 0.1797  loss_dice_1: 1.361  loss_ce_2: 0.5965  loss_mask_2: 0.1745  loss_dice_2: 1.333  loss_ce_3: 0.5231  loss_mask_3: 0.1681  loss_dice_3: 1.269  loss_ce_4: 0.4918  loss_mask_4: 0.1708  loss_dice_4: 1.242  loss_ce_5: 0.4864  loss_mask_5: 0.1711  loss_dice_5: 1.273  loss_ce_6: 0.4724  loss_mask_6: 0.1697  loss_dice_6: 1.234  loss_ce_7: 0.5004  loss_mask_7: 0.1712  loss_dice_7: 1.249  loss_ce_8: 0.4844  loss_mask_8: 0.1703  loss_dice_8: 1.225    time: 1.0985  last_time: 1.0302  data_time: 0.0739  last_data_time: 0.0562   lr: 0.0001  max_mem: 33079M
[10/08 16:00:15] d2.utils.events INFO:  eta: 0:01:05  iter: 939  total_loss: 20.82  loss_ce: 0.4972  loss_mask: 0.1617  loss_dice: 1.281  loss_ce_0: 0.5899  loss_mask_0: 0.1932  loss_dice_0: 1.544  loss_ce_1: 0.6847  loss_mask_1: 0.1737  loss_dice_1: 1.425  loss_ce_2: 0.6425  loss_mask_2: 0.1672  loss_dice_2: 1.372  loss_ce_3: 0.5589  loss_mask_3: 0.1653  loss_dice_3: 1.312  loss_ce_4: 0.5326  loss_mask_4: 0.1654  loss_dice_4: 1.32  loss_ce_5: 0.5067  loss_mask_5: 0.1644  loss_dice_5: 1.303  loss_ce_6: 0.5281  loss_mask_6: 0.161  loss_dice_6: 1.265  loss_ce_7: 0.477  loss_mask_7: 0.1604  loss_dice_7: 1.287  loss_ce_8: 0.503  loss_mask_8: 0.1622  loss_dice_8: 1.306    time: 1.0980  last_time: 1.1056  data_time: 0.0712  last_data_time: 0.0770   lr: 0.0001  max_mem: 33079M
[10/08 16:00:36] d2.utils.events INFO:  eta: 0:00:43  iter: 959  total_loss: 19.83  loss_ce: 0.4936  loss_mask: 0.1837  loss_dice: 1.191  loss_ce_0: 0.53  loss_mask_0: 0.2151  loss_dice_0: 1.451  loss_ce_1: 0.6062  loss_mask_1: 0.2002  loss_dice_1: 1.356  loss_ce_2: 0.5823  loss_mask_2: 0.1956  loss_dice_2: 1.319  loss_ce_3: 0.5175  loss_mask_3: 0.1878  loss_dice_3: 1.245  loss_ce_4: 0.4984  loss_mask_4: 0.1865  loss_dice_4: 1.23  loss_ce_5: 0.5173  loss_mask_5: 0.1852  loss_dice_5: 1.258  loss_ce_6: 0.4946  loss_mask_6: 0.1839  loss_dice_6: 1.216  loss_ce_7: 0.5178  loss_mask_7: 0.1828  loss_dice_7: 1.192  loss_ce_8: 0.5211  loss_mask_8: 0.1812  loss_dice_8: 1.184    time: 1.0972  last_time: 1.0799  data_time: 0.0650  last_data_time: 0.0819   lr: 0.0001  max_mem: 33079M
[10/08 16:00:57] d2.utils.events INFO:  eta: 0:00:21  iter: 979  total_loss: 20.46  loss_ce: 0.4511  loss_mask: 0.1661  loss_dice: 1.25  loss_ce_0: 0.5726  loss_mask_0: 0.1931  loss_dice_0: 1.521  loss_ce_1: 0.627  loss_mask_1: 0.1784  loss_dice_1: 1.408  loss_ce_2: 0.5511  loss_mask_2: 0.1709  loss_dice_2: 1.342  loss_ce_3: 0.523  loss_mask_3: 0.1685  loss_dice_3: 1.297  loss_ce_4: 0.5047  loss_mask_4: 0.1661  loss_dice_4: 1.3  loss_ce_5: 0.4648  loss_mask_5: 0.1647  loss_dice_5: 1.296  loss_ce_6: 0.4916  loss_mask_6: 0.1658  loss_dice_6: 1.262  loss_ce_7: 0.4373  loss_mask_7: 0.1645  loss_dice_7: 1.243  loss_ce_8: 0.4514  loss_mask_8: 0.164  loss_dice_8: 1.235    time: 1.0966  last_time: 1.0509  data_time: 0.0664  last_data_time: 0.0760   lr: 0.0001  max_mem: 33079M
[10/08 16:01:19] fvcore.common.checkpoint INFO: Saving checkpoint to /tsi/hi-paris/GB/segmentation/results/In_vocab/r50_008_1000_19_a_decoder/model_final.pth
[10/08 16:01:22] d2.utils.events INFO:  eta: 0:00:00  iter: 999  total_loss: 20.47  loss_ce: 0.5574  loss_mask: 0.1721  loss_dice: 1.237  loss_ce_0: 0.5717  loss_mask_0: 0.206  loss_dice_0: 1.515  loss_ce_1: 0.687  loss_mask_1: 0.2001  loss_dice_1: 1.368  loss_ce_2: 0.6266  loss_mask_2: 0.177  loss_dice_2: 1.298  loss_ce_3: 0.5766  loss_mask_3: 0.1767  loss_dice_3: 1.291  loss_ce_4: 0.5517  loss_mask_4: 0.1746  loss_dice_4: 1.263  loss_ce_5: 0.5486  loss_mask_5: 0.1738  loss_dice_5: 1.275  loss_ce_6: 0.5524  loss_mask_6: 0.1738  loss_dice_6: 1.228  loss_ce_7: 0.5392  loss_mask_7: 0.1764  loss_dice_7: 1.224  loss_ce_8: 0.541  loss_mask_8: 0.1719  loss_dice_8: 1.223    time: 1.0960  last_time: 1.0989  data_time: 0.0679  last_data_time: 0.0712   lr: 0.0001  max_mem: 33079M
[10/08 16:01:22] d2.engine.hooks INFO: Overall training speed: 998 iterations in 0:18:13 (1.0960 s / it)
[10/08 16:01:22] d2.engine.hooks INFO: Total training time: 0:18:19 (0:00:05 on hooks)
[10/08 16:01:22] fcclip.data.datasets.register_cityscapes_panoptic INFO: 3 cities found in 'datasets/cityscapes/leftImg8bit/val'.
[10/08 16:01:22] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(1024, 1024), max_size=2560, sample_style='choice')]
[10/08 16:01:22] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/08 16:01:22] d2.data.common INFO: Serializing 500 elements to byte tensors and concatenating them all ...
[10/08 16:01:22] d2.data.common INFO: Serialized dataset takes 0.74 MiB
[10/08 16:01:22] d2.evaluation.evaluator INFO: Start inference on 500 batches
[10/08 16:08:07] detectron2 INFO: Rank of current process: 0. World size: 1
[10/08 16:08:08] detectron2 INFO: Environment info:
-------------------------------  -------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.8.19 | packaged by conda-forge | (default, Mar 20 2024, 12:47:35) [GCC 12.3.0]
numpy                            1.24.4
detectron2                       0.6 @/home/ids/gbrison/segmentation/segmentation/detectron2/detectron2
Compiler                         GCC 11.4
CUDA compiler                    CUDA 12.5
detectron2 arch flags            8.9
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.0+cu121 @/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0,1                          NVIDIA L40S (arch=8.9)
Driver version                   555.42.06
CUDA_HOME                        /usr/local/cuda
Pillow                           9.4.0
torchvision                      0.19.0+cu121 @/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torchvision
torchvision arch flags           5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.10.0
-------------------------------  -------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX512
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.0, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/08 16:08:08] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco/panoptic-segmentation/fcclip/r50_exp_1000_a_decoder.yaml', dist_url='tcp://127.0.0.1:51163', eval_only=False, machine_rank=0, num_gpus=1, num_machines=1, opts=[], resume=False)
[10/08 16:08:08] detectron2 INFO: Contents of args.config_file=configs/coco/panoptic-segmentation/fcclip/r50_exp_1000_a_decoder.yaml:
_BASE_: ./fcclip_convnext_large_eval_ade20k_r50.yaml

INPUT:
  MIN_SIZE_TEST: 1024
  MAX_SIZE_TEST: 2560

MODEL:
  SEM_SEG_HEAD:
    NUM_CLASSES: 19
  WEIGHTS: /tsi/hi-paris/GB/segmentation/models/fcclip_cocopan_r50.pth
OUTPUT_DIR: /tsi/hi-paris/GB/segmentation/results/In_vocab/r50_008_1000_19_a_decoder
DATASETS:
  TRAIN: ("openvocab_cityscapes_fine_panoptic_train",)
  TEST: ("openvocab_cityscapes_fine_panoptic_val",)
SOLVER:
  IMS_PER_BATCH: 8
  MAX_ITER: 1000
TEST:
  EVAL_PERIOD: 1000


[10/08 16:08:08] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - openvocab_cityscapes_fine_panoptic_val
  TRAIN:
  - openvocab_cityscapes_fine_panoptic_train
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: false
  CROP:
    ENABLED: false
    MINIMUM_INST_AREA: 1
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  DATASET_MAPPER_NAME: coco_panoptic_lsj
  FORMAT: RGB
  IMAGE_SIZE: 1024
  MASK_FORMAT: polygon
  MAX_SCALE: 2.0
  MAX_SIZE_TEST: 2560
  MAX_SIZE_TRAIN: 1333
  MIN_SCALE: 0.1
  MIN_SIZE_TEST: 1024
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: -1
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 0
    NAME: CLIP
  DEVICE: cuda
  FC_CLIP:
    CLIP_MODEL_NAME: RN50
    CLIP_PRETRAINED_WEIGHTS: openai
    EMBED_DIM: 1024
    ENSEMBLE_ON_VALID_MASK: true
    GEOMETRIC_ENSEMBLE_ALPHA: 0.4
    GEOMETRIC_ENSEMBLE_BETA: 0.8
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_FORMER:
    CLASS_WEIGHT: 2.0
    DEC_LAYERS: 10
    DEEP_SUPERVISION: true
    DICE_WEIGHT: 5.0
    DIM_FEEDFORWARD: 2048
    DROPOUT: 0.0
    ENC_LAYERS: 0
    ENFORCE_INPUT_PROJ: false
    HIDDEN_DIM: 256
    IMPORTANCE_SAMPLE_RATIO: 0.75
    MASK_WEIGHT: 5.0
    NHEADS: 8
    NO_OBJECT_WEIGHT: 0.1
    NUM_OBJECT_QUERIES: 250
    OVERSAMPLE_RATIO: 3.0
    PRE_NORM: false
    SIZE_DIVISIBILITY: 32
    TEST:
      INSTANCE_ON: true
      OBJECT_MASK_THRESHOLD: 0.0
      OVERLAP_THRESHOLD: 0.8
      PANOPTIC_ON: true
      SEMANTIC_ON: true
      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: false
    TRAIN_NUM_POINTS: 12544
    TRANSFORMER_DECODER_NAME: MultiScaleMaskedTransformerDecoder
    TRANSFORMER_IN_FEATURE: multi_scale_pixel_decoder
  MASK_ON: false
  META_ARCHITECTURE: FCCLIP
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 122.7709383
  - 116.7460125
  - 104.09373615
  PIXEL_STD:
  - 68.5005327
  - 66.6321579
  - 70.32316305
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 1
    - 1
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: basic
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 256
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES:
    - res3
    - res4
    - res5
    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8
    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    MASK_DIM: 256
    NAME: FCCLIPHead
    NORM: GN
    NUM_CLASSES: 19
    PIXEL_DECODER_NAME: MSDeformAttnPixelDecoder
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    TRANSFORMER_ENC_LAYERS: 6
    USE_DEPTHWISE_SEPARABLE_CONV: false
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    USE_CHECKPOINT: false
    WINDOW_SIZE: 7
  WEIGHTS: /tsi/hi-paris/GB/segmentation/models/fcclip_cocopan_r50.pth
OUTPUT_DIR: /tsi/hi-paris/GB/segmentation/results/In_vocab/r50_008_1000_19_a_decoder
SEED: -1
SOLVER:
  AMP:
    ENABLED: true
  BACKBONE_MULTIPLIER: 0.1
  BASE_LR: 0.0001
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 5000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.01
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 8
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 1000
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 327778
  - 355092
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 1000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[10/08 16:08:08] detectron2 INFO: Full config saved to /tsi/hi-paris/GB/segmentation/results/In_vocab/r50_008_1000_19_a_decoder/config.yaml
[10/08 16:08:08] d2.utils.env INFO: Using a generated random seed 9491182
[10/08 16:08:12] d2.engine.defaults INFO: Model:
FCCLIP(
  (backbone): CLIP(
    (clip_model): CLIP(
      (visual): ModifiedResNet(
        (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
        (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        (layer1): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
            (downsample): Sequential(
              (-1): AvgPool2d(kernel_size=1, stride=1, padding=0)
              (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
        )
        (layer2): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
            (downsample): Sequential(
              (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (3): Bottleneck(
            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
        )
        (layer3): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
            (downsample): Sequential(
              (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
              (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (3): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (4): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (5): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
        )
        (layer4): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
            (downsample): Sequential(
              (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
              (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
        )
        (attnpool): AttentionPool2d(
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (c_proj): Linear(in_features=2048, out_features=1024, bias=True)
        )
      )
      (transformer): Transformer(
        (resblocks): ModuleList(
          (0-11): 12 x ResidualAttentionBlock(
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ls_2): Identity()
          )
        )
      )
      (token_embedding): Embedding(49408, 512)
      (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
  )
  (sem_seg_head): FCCLIPHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0-8): 9 x SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0-8): 9 x CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0-8): 9 x FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(250, 256)
      (query_embed): Embedding(250, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0-2): 3 x Sequential()
      )
      (mask_embed): MLP(
        (layers): ModuleList(
          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (mask_pooling): MaskPooling()
      (_mask_pooling_proj): Sequential(
        (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
      )
      (class_embed): MLP(
        (layers): ModuleList(
          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=1024, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 19
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
  (conv1): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv2): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU()
  (mask_pooling): MaskPooling()
  (decoder_adapter): DecoderAdapter(
    (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
    (conv2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    (norm1): LayerNorm((64, 1, 1), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((256, 1, 1), eps=1e-05, elementwise_affine=True)
    (relu): ReLU()
  )
  (void_embedding): Embedding(1, 1024)
)
[10/08 16:08:12] fcclip.data.dataset_mappers.mask_former_semantic_dataset_mapper INFO: [MaskFormerPanopticDatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(800,), max_size=1333, sample_style='choice'), RandomFlip()]
[10/08 16:08:12] fcclip.data.datasets.register_cityscapes_panoptic INFO: 18 cities found in 'datasets/cityscapes/leftImg8bit/train'.
[10/08 16:08:12] d2.data.build INFO: Using training sampler TrainingSampler
[10/08 16:08:12] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/08 16:08:12] d2.data.common INFO: Serializing 2975 elements to byte tensors and concatenating them all ...
[10/08 16:08:12] d2.data.common INFO: Serialized dataset takes 4.12 MiB
[10/08 16:08:12] d2.data.build INFO: Making batched data loader with batch_size=8
[10/08 16:08:12] d2.solver.build WARNING: SOLVER.STEPS contains values larger than SOLVER.MAX_ITER. These values will be ignored.
[10/08 16:08:12] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /tsi/hi-paris/GB/segmentation/models/fcclip_cocopan_r50.pth ...
[10/08 16:08:12] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /tsi/hi-paris/GB/segmentation/models/fcclip_cocopan_r50.pth ...
[10/08 16:08:12] fvcore.common.checkpoint WARNING: Skip loading parameter 'criterion.empty_weight' to the model due to incompatible shapes: (134,) in the checkpoint but (20,) in the model! You might want to double check if this is expected.
[10/08 16:08:12] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.clip_model.ln_final.{bias, weight}[0m
[34mbackbone.clip_model.token_embedding.weight[0m
[34mbackbone.clip_model.transformer.resblocks.0.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.attnpool.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.attnpool.k_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.attnpool.positional_embedding[0m
[34mbackbone.clip_model.visual.attnpool.q_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.attnpool.v_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.conv1.weight[0m
[34mbackbone.clip_model.visual.conv2.weight[0m
[34mbackbone.clip_model.visual.conv3.weight[0m
[34mbackbone.clip_model.visual.layer1.0.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.0.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.0.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.0.conv1.weight[0m
[34mbackbone.clip_model.visual.layer1.0.conv2.weight[0m
[34mbackbone.clip_model.visual.layer1.0.conv3.weight[0m
[34mbackbone.clip_model.visual.layer1.0.downsample.0.weight[0m
[34mbackbone.clip_model.visual.layer1.0.downsample.1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.1.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.1.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.1.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.1.conv1.weight[0m
[34mbackbone.clip_model.visual.layer1.1.conv2.weight[0m
[34mbackbone.clip_model.visual.layer1.1.conv3.weight[0m
[34mbackbone.clip_model.visual.layer1.2.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.2.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.2.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.2.conv1.weight[0m
[34mbackbone.clip_model.visual.layer1.2.conv2.weight[0m
[34mbackbone.clip_model.visual.layer1.2.conv3.weight[0m
[34mbackbone.clip_model.visual.layer2.0.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.0.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.0.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.0.conv1.weight[0m
[34mbackbone.clip_model.visual.layer2.0.conv2.weight[0m
[34mbackbone.clip_model.visual.layer2.0.conv3.weight[0m
[34mbackbone.clip_model.visual.layer2.0.downsample.0.weight[0m
[34mbackbone.clip_model.visual.layer2.0.downsample.1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.1.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.1.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.1.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.1.conv1.weight[0m
[34mbackbone.clip_model.visual.layer2.1.conv2.weight[0m
[34mbackbone.clip_model.visual.layer2.1.conv3.weight[0m
[34mbackbone.clip_model.visual.layer2.2.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.2.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.2.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.2.conv1.weight[0m
[34mbackbone.clip_model.visual.layer2.2.conv2.weight[0m
[34mbackbone.clip_model.visual.layer2.2.conv3.weight[0m
[34mbackbone.clip_model.visual.layer2.3.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.3.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.3.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.3.conv1.weight[0m
[34mbackbone.clip_model.visual.layer2.3.conv2.weight[0m
[34mbackbone.clip_model.visual.layer2.3.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.0.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.0.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.0.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.0.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.0.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.0.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.0.downsample.0.weight[0m
[34mbackbone.clip_model.visual.layer3.0.downsample.1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.1.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.1.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.1.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.1.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.1.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.1.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.2.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.2.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.2.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.2.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.2.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.2.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.3.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.3.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.3.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.3.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.3.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.3.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.4.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.4.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.4.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.4.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.4.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.4.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.5.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.5.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.5.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.5.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.5.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.5.conv3.weight[0m
[34mbackbone.clip_model.visual.layer4.0.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.0.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.0.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.0.conv1.weight[0m
[34mbackbone.clip_model.visual.layer4.0.conv2.weight[0m
[34mbackbone.clip_model.visual.layer4.0.conv3.weight[0m
[34mbackbone.clip_model.visual.layer4.0.downsample.0.weight[0m
[34mbackbone.clip_model.visual.layer4.0.downsample.1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.1.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.1.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.1.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.1.conv1.weight[0m
[34mbackbone.clip_model.visual.layer4.1.conv2.weight[0m
[34mbackbone.clip_model.visual.layer4.1.conv3.weight[0m
[34mbackbone.clip_model.visual.layer4.2.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.2.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.2.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.2.conv1.weight[0m
[34mbackbone.clip_model.visual.layer4.2.conv2.weight[0m
[34mbackbone.clip_model.visual.layer4.2.conv3.weight[0m
[34mbackbone.clip_model.{logit_scale, positional_embedding, text_projection}[0m
[34mbn1.{bias, running_mean, running_var, weight}[0m
[34mbn2.{bias, running_mean, running_var, weight}[0m
[34mconv1.{bias, weight}[0m
[34mconv2.{bias, weight}[0m
[34mcriterion.empty_weight[0m
[34mdecoder_adapter.conv1.{bias, weight}[0m
[34mdecoder_adapter.conv2.{bias, weight}[0m
[34mdecoder_adapter.norm1.{bias, weight}[0m
[34mdecoder_adapter.norm2.{bias, weight}[0m
[10/08 16:08:12] d2.engine.train_loop INFO: Starting training from iteration 0
[10/08 16:08:36] d2.utils.events INFO:  eta: 0:17:05  iter: 19  total_loss: 31.04  loss_ce: 1.165  loss_mask: 0.2791  loss_dice: 1.525  loss_ce_0: 1.334  loss_mask_0: 0.3121  loss_dice_0: 1.848  loss_ce_1: 1.383  loss_mask_1: 0.3047  loss_dice_1: 1.681  loss_ce_2: 1.291  loss_mask_2: 0.2871  loss_dice_2: 1.609  loss_ce_3: 1.254  loss_mask_3: 0.2835  loss_dice_3: 1.554  loss_ce_4: 1.175  loss_mask_4: 0.2841  loss_dice_4: 1.556  loss_ce_5: 1.266  loss_mask_5: 0.2804  loss_dice_5: 1.54  loss_ce_6: 1.159  loss_mask_6: 0.2826  loss_dice_6: 1.558  loss_ce_7: 1.181  loss_mask_7: 0.2803  loss_dice_7: 1.557  loss_ce_8: 1.238  loss_mask_8: 0.282  loss_dice_8: 1.525    time: 1.0556  last_time: 1.0140  data_time: 0.1038  last_data_time: 0.0718   lr: 0.0001  max_mem: 31701M
[10/08 16:08:57] d2.utils.events INFO:  eta: 0:16:33  iter: 39  total_loss: 25.07  loss_ce: 0.7768  loss_mask: 0.2251  loss_dice: 1.39  loss_ce_0: 0.8817  loss_mask_0: 0.2665  loss_dice_0: 1.742  loss_ce_1: 0.9201  loss_mask_1: 0.2552  loss_dice_1: 1.573  loss_ce_2: 0.8375  loss_mask_2: 0.2364  loss_dice_2: 1.508  loss_ce_3: 0.8243  loss_mask_3: 0.2262  loss_dice_3: 1.443  loss_ce_4: 0.7909  loss_mask_4: 0.2262  loss_dice_4: 1.472  loss_ce_5: 0.7811  loss_mask_5: 0.231  loss_dice_5: 1.436  loss_ce_6: 0.7987  loss_mask_6: 0.2271  loss_dice_6: 1.397  loss_ce_7: 0.7897  loss_mask_7: 0.2256  loss_dice_7: 1.385  loss_ce_8: 0.778  loss_mask_8: 0.2288  loss_dice_8: 1.39    time: 1.0436  last_time: 1.0371  data_time: 0.0657  last_data_time: 0.0790   lr: 0.0001  max_mem: 31701M
[10/08 16:09:18] d2.utils.events INFO:  eta: 0:16:14  iter: 59  total_loss: 23.92  loss_ce: 0.6641  loss_mask: 0.2301  loss_dice: 1.293  loss_ce_0: 0.759  loss_mask_0: 0.2577  loss_dice_0: 1.58  loss_ce_1: 0.7985  loss_mask_1: 0.2449  loss_dice_1: 1.506  loss_ce_2: 0.7203  loss_mask_2: 0.231  loss_dice_2: 1.429  loss_ce_3: 0.69  loss_mask_3: 0.229  loss_dice_3: 1.356  loss_ce_4: 0.6617  loss_mask_4: 0.2292  loss_dice_4: 1.349  loss_ce_5: 0.6563  loss_mask_5: 0.2295  loss_dice_5: 1.366  loss_ce_6: 0.6761  loss_mask_6: 0.2276  loss_dice_6: 1.313  loss_ce_7: 0.6702  loss_mask_7: 0.2333  loss_dice_7: 1.307  loss_ce_8: 0.6298  loss_mask_8: 0.2289  loss_dice_8: 1.341    time: 1.0472  last_time: 1.1013  data_time: 0.0689  last_data_time: 0.0902   lr: 0.0001  max_mem: 31701M
[10/08 16:09:39] d2.utils.events INFO:  eta: 0:15:57  iter: 79  total_loss: 23.85  loss_ce: 0.6407  loss_mask: 0.2128  loss_dice: 1.373  loss_ce_0: 0.7528  loss_mask_0: 0.2374  loss_dice_0: 1.687  loss_ce_1: 0.8373  loss_mask_1: 0.2278  loss_dice_1: 1.569  loss_ce_2: 0.7423  loss_mask_2: 0.2221  loss_dice_2: 1.466  loss_ce_3: 0.6955  loss_mask_3: 0.2133  loss_dice_3: 1.425  loss_ce_4: 0.6587  loss_mask_4: 0.2126  loss_dice_4: 1.407  loss_ce_5: 0.6725  loss_mask_5: 0.2112  loss_dice_5: 1.409  loss_ce_6: 0.675  loss_mask_6: 0.213  loss_dice_6: 1.389  loss_ce_7: 0.6275  loss_mask_7: 0.2165  loss_dice_7: 1.397  loss_ce_8: 0.6509  loss_mask_8: 0.2117  loss_dice_8: 1.365    time: 1.0508  last_time: 1.0777  data_time: 0.0723  last_data_time: 0.0888   lr: 0.0001  max_mem: 31701M
[10/08 16:10:02] d2.utils.events INFO:  eta: 0:15:43  iter: 99  total_loss: 23.68  loss_ce: 0.657  loss_mask: 0.2077  loss_dice: 1.386  loss_ce_0: 0.742  loss_mask_0: 0.2298  loss_dice_0: 1.706  loss_ce_1: 0.7883  loss_mask_1: 0.2143  loss_dice_1: 1.581  loss_ce_2: 0.7053  loss_mask_2: 0.2042  loss_dice_2: 1.48  loss_ce_3: 0.7009  loss_mask_3: 0.2032  loss_dice_3: 1.419  loss_ce_4: 0.6431  loss_mask_4: 0.2025  loss_dice_4: 1.415  loss_ce_5: 0.6628  loss_mask_5: 0.2022  loss_dice_5: 1.414  loss_ce_6: 0.6372  loss_mask_6: 0.2028  loss_dice_6: 1.391  loss_ce_7: 0.6433  loss_mask_7: 0.205  loss_dice_7: 1.374  loss_ce_8: 0.6242  loss_mask_8: 0.2074  loss_dice_8: 1.399    time: 1.0630  last_time: 1.0499  data_time: 0.0809  last_data_time: 0.0657   lr: 0.0001  max_mem: 31772M
[10/08 16:10:23] d2.utils.events INFO:  eta: 0:15:26  iter: 119  total_loss: 22.77  loss_ce: 0.6252  loss_mask: 0.1976  loss_dice: 1.342  loss_ce_0: 0.641  loss_mask_0: 0.2297  loss_dice_0: 1.643  loss_ce_1: 0.7724  loss_mask_1: 0.2095  loss_dice_1: 1.496  loss_ce_2: 0.7138  loss_mask_2: 0.2016  loss_dice_2: 1.438  loss_ce_3: 0.6582  loss_mask_3: 0.2057  loss_dice_3: 1.398  loss_ce_4: 0.6191  loss_mask_4: 0.2106  loss_dice_4: 1.376  loss_ce_5: 0.6189  loss_mask_5: 0.2035  loss_dice_5: 1.366  loss_ce_6: 0.6032  loss_mask_6: 0.1982  loss_dice_6: 1.369  loss_ce_7: 0.6291  loss_mask_7: 0.198  loss_dice_7: 1.356  loss_ce_8: 0.5988  loss_mask_8: 0.1983  loss_dice_8: 1.338    time: 1.0637  last_time: 0.9899  data_time: 0.0758  last_data_time: 0.0706   lr: 0.0001  max_mem: 31772M
[10/08 16:10:45] d2.utils.events INFO:  eta: 0:15:08  iter: 139  total_loss: 22.73  loss_ce: 0.6473  loss_mask: 0.2062  loss_dice: 1.335  loss_ce_0: 0.7524  loss_mask_0: 0.236  loss_dice_0: 1.622  loss_ce_1: 0.8027  loss_mask_1: 0.2231  loss_dice_1: 1.496  loss_ce_2: 0.7442  loss_mask_2: 0.2105  loss_dice_2: 1.442  loss_ce_3: 0.6773  loss_mask_3: 0.2111  loss_dice_3: 1.384  loss_ce_4: 0.6241  loss_mask_4: 0.2109  loss_dice_4: 1.363  loss_ce_5: 0.6479  loss_mask_5: 0.2068  loss_dice_5: 1.365  loss_ce_6: 0.6232  loss_mask_6: 0.2056  loss_dice_6: 1.336  loss_ce_7: 0.6516  loss_mask_7: 0.2049  loss_dice_7: 1.327  loss_ce_8: 0.6318  loss_mask_8: 0.2046  loss_dice_8: 1.356    time: 1.0703  last_time: 1.1577  data_time: 0.0799  last_data_time: 0.0813   lr: 0.0001  max_mem: 31959M
[10/08 16:11:08] d2.utils.events INFO:  eta: 0:14:55  iter: 159  total_loss: 22.83  loss_ce: 0.6158  loss_mask: 0.1906  loss_dice: 1.351  loss_ce_0: 0.7026  loss_mask_0: 0.2186  loss_dice_0: 1.582  loss_ce_1: 0.7397  loss_mask_1: 0.2095  loss_dice_1: 1.52  loss_ce_2: 0.6956  loss_mask_2: 0.1973  loss_dice_2: 1.438  loss_ce_3: 0.6562  loss_mask_3: 0.1923  loss_dice_3: 1.382  loss_ce_4: 0.6318  loss_mask_4: 0.1919  loss_dice_4: 1.349  loss_ce_5: 0.6071  loss_mask_5: 0.1928  loss_dice_5: 1.378  loss_ce_6: 0.6119  loss_mask_6: 0.1936  loss_dice_6: 1.337  loss_ce_7: 0.6261  loss_mask_7: 0.1929  loss_dice_7: 1.305  loss_ce_8: 0.5848  loss_mask_8: 0.1889  loss_dice_8: 1.326    time: 1.0764  last_time: 1.1472  data_time: 0.0811  last_data_time: 0.0863   lr: 0.0001  max_mem: 31959M
[10/08 16:11:30] d2.utils.events INFO:  eta: 0:14:40  iter: 179  total_loss: 21.42  loss_ce: 0.5889  loss_mask: 0.2087  loss_dice: 1.214  loss_ce_0: 0.6559  loss_mask_0: 0.2353  loss_dice_0: 1.481  loss_ce_1: 0.7415  loss_mask_1: 0.2105  loss_dice_1: 1.418  loss_ce_2: 0.66  loss_mask_2: 0.2019  loss_dice_2: 1.329  loss_ce_3: 0.6296  loss_mask_3: 0.1992  loss_dice_3: 1.28  loss_ce_4: 0.5677  loss_mask_4: 0.2004  loss_dice_4: 1.275  loss_ce_5: 0.5738  loss_mask_5: 0.2014  loss_dice_5: 1.286  loss_ce_6: 0.5276  loss_mask_6: 0.2009  loss_dice_6: 1.257  loss_ce_7: 0.5338  loss_mask_7: 0.2095  loss_dice_7: 1.234  loss_ce_8: 0.5306  loss_mask_8: 0.2079  loss_dice_8: 1.23    time: 1.0784  last_time: 1.0158  data_time: 0.0784  last_data_time: 0.0628   lr: 0.0001  max_mem: 31959M
[10/08 16:11:52] d2.utils.events INFO:  eta: 0:14:22  iter: 199  total_loss: 22.54  loss_ce: 0.5821  loss_mask: 0.1896  loss_dice: 1.3  loss_ce_0: 0.6874  loss_mask_0: 0.2144  loss_dice_0: 1.575  loss_ce_1: 0.7473  loss_mask_1: 0.1974  loss_dice_1: 1.487  loss_ce_2: 0.6475  loss_mask_2: 0.1919  loss_dice_2: 1.395  loss_ce_3: 0.6248  loss_mask_3: 0.1925  loss_dice_3: 1.34  loss_ce_4: 0.6326  loss_mask_4: 0.1935  loss_dice_4: 1.351  loss_ce_5: 0.6105  loss_mask_5: 0.1917  loss_dice_5: 1.312  loss_ce_6: 0.6128  loss_mask_6: 0.1867  loss_dice_6: 1.32  loss_ce_7: 0.5757  loss_mask_7: 0.1873  loss_dice_7: 1.323  loss_ce_8: 0.5837  loss_mask_8: 0.1904  loss_dice_8: 1.294    time: 1.0825  last_time: 1.0692  data_time: 0.0780  last_data_time: 0.0846   lr: 0.0001  max_mem: 31959M
[10/08 16:12:14] d2.utils.events INFO:  eta: 0:14:04  iter: 219  total_loss: 22.11  loss_ce: 0.59  loss_mask: 0.2087  loss_dice: 1.279  loss_ce_0: 0.6519  loss_mask_0: 0.2344  loss_dice_0: 1.581  loss_ce_1: 0.7211  loss_mask_1: 0.2071  loss_dice_1: 1.467  loss_ce_2: 0.6945  loss_mask_2: 0.2036  loss_dice_2: 1.426  loss_ce_3: 0.6293  loss_mask_3: 0.2117  loss_dice_3: 1.348  loss_ce_4: 0.5904  loss_mask_4: 0.2093  loss_dice_4: 1.343  loss_ce_5: 0.6325  loss_mask_5: 0.2054  loss_dice_5: 1.297  loss_ce_6: 0.5939  loss_mask_6: 0.2102  loss_dice_6: 1.293  loss_ce_7: 0.6025  loss_mask_7: 0.2076  loss_dice_7: 1.304  loss_ce_8: 0.5772  loss_mask_8: 0.2039  loss_dice_8: 1.309    time: 1.0847  last_time: 1.1347  data_time: 0.0778  last_data_time: 0.0865   lr: 0.0001  max_mem: 31959M
[10/08 16:12:37] d2.utils.events INFO:  eta: 0:13:43  iter: 239  total_loss: 21.92  loss_ce: 0.5224  loss_mask: 0.1992  loss_dice: 1.331  loss_ce_0: 0.6604  loss_mask_0: 0.2331  loss_dice_0: 1.559  loss_ce_1: 0.6819  loss_mask_1: 0.2114  loss_dice_1: 1.443  loss_ce_2: 0.6575  loss_mask_2: 0.2051  loss_dice_2: 1.412  loss_ce_3: 0.6018  loss_mask_3: 0.2051  loss_dice_3: 1.34  loss_ce_4: 0.57  loss_mask_4: 0.2035  loss_dice_4: 1.342  loss_ce_5: 0.599  loss_mask_5: 0.1985  loss_dice_5: 1.3  loss_ce_6: 0.55  loss_mask_6: 0.1988  loss_dice_6: 1.291  loss_ce_7: 0.5659  loss_mask_7: 0.1978  loss_dice_7: 1.327  loss_ce_8: 0.5392  loss_mask_8: 0.1988  loss_dice_8: 1.296    time: 1.0866  last_time: 1.1416  data_time: 0.0683  last_data_time: 0.0681   lr: 0.0001  max_mem: 31959M
[10/08 16:12:59] d2.utils.events INFO:  eta: 0:13:24  iter: 259  total_loss: 21.79  loss_ce: 0.5536  loss_mask: 0.2075  loss_dice: 1.277  loss_ce_0: 0.6136  loss_mask_0: 0.2319  loss_dice_0: 1.558  loss_ce_1: 0.6917  loss_mask_1: 0.2182  loss_dice_1: 1.438  loss_ce_2: 0.665  loss_mask_2: 0.2126  loss_dice_2: 1.369  loss_ce_3: 0.6085  loss_mask_3: 0.212  loss_dice_3: 1.337  loss_ce_4: 0.5874  loss_mask_4: 0.2067  loss_dice_4: 1.309  loss_ce_5: 0.5978  loss_mask_5: 0.2002  loss_dice_5: 1.313  loss_ce_6: 0.5407  loss_mask_6: 0.2071  loss_dice_6: 1.291  loss_ce_7: 0.5715  loss_mask_7: 0.2048  loss_dice_7: 1.285  loss_ce_8: 0.5743  loss_mask_8: 0.2103  loss_dice_8: 1.265    time: 1.0893  last_time: 1.0778  data_time: 0.0731  last_data_time: 0.0548   lr: 0.0001  max_mem: 31959M
[10/08 16:13:21] d2.utils.events INFO:  eta: 0:13:05  iter: 279  total_loss: 22.1  loss_ce: 0.5824  loss_mask: 0.1919  loss_dice: 1.3  loss_ce_0: 0.6607  loss_mask_0: 0.2244  loss_dice_0: 1.565  loss_ce_1: 0.7057  loss_mask_1: 0.2061  loss_dice_1: 1.477  loss_ce_2: 0.655  loss_mask_2: 0.2026  loss_dice_2: 1.429  loss_ce_3: 0.6218  loss_mask_3: 0.1987  loss_dice_3: 1.334  loss_ce_4: 0.5637  loss_mask_4: 0.1988  loss_dice_4: 1.331  loss_ce_5: 0.5964  loss_mask_5: 0.1957  loss_dice_5: 1.337  loss_ce_6: 0.5572  loss_mask_6: 0.192  loss_dice_6: 1.316  loss_ce_7: 0.6014  loss_mask_7: 0.1926  loss_dice_7: 1.332  loss_ce_8: 0.575  loss_mask_8: 0.1912  loss_dice_8: 1.331    time: 1.0912  last_time: 1.1189  data_time: 0.0710  last_data_time: 0.0609   lr: 0.0001  max_mem: 31959M
[10/08 16:13:44] d2.utils.events INFO:  eta: 0:12:46  iter: 299  total_loss: 21.84  loss_ce: 0.57  loss_mask: 0.1753  loss_dice: 1.321  loss_ce_0: 0.6632  loss_mask_0: 0.204  loss_dice_0: 1.566  loss_ce_1: 0.7431  loss_mask_1: 0.1859  loss_dice_1: 1.446  loss_ce_2: 0.6558  loss_mask_2: 0.1846  loss_dice_2: 1.416  loss_ce_3: 0.5932  loss_mask_3: 0.1777  loss_dice_3: 1.389  loss_ce_4: 0.579  loss_mask_4: 0.1732  loss_dice_4: 1.314  loss_ce_5: 0.5772  loss_mask_5: 0.1735  loss_dice_5: 1.332  loss_ce_6: 0.5676  loss_mask_6: 0.1759  loss_dice_6: 1.308  loss_ce_7: 0.5699  loss_mask_7: 0.1745  loss_dice_7: 1.323  loss_ce_8: 0.5627  loss_mask_8: 0.1742  loss_dice_8: 1.304    time: 1.0938  last_time: 1.1584  data_time: 0.0719  last_data_time: 0.0592   lr: 0.0001  max_mem: 31959M
[10/08 16:14:06] d2.utils.events INFO:  eta: 0:12:25  iter: 319  total_loss: 21.48  loss_ce: 0.5737  loss_mask: 0.1802  loss_dice: 1.289  loss_ce_0: 0.6378  loss_mask_0: 0.2119  loss_dice_0: 1.624  loss_ce_1: 0.7219  loss_mask_1: 0.1911  loss_dice_1: 1.452  loss_ce_2: 0.6528  loss_mask_2: 0.1887  loss_dice_2: 1.384  loss_ce_3: 0.6035  loss_mask_3: 0.1824  loss_dice_3: 1.339  loss_ce_4: 0.5757  loss_mask_4: 0.1852  loss_dice_4: 1.341  loss_ce_5: 0.5492  loss_mask_5: 0.1836  loss_dice_5: 1.354  loss_ce_6: 0.5997  loss_mask_6: 0.1829  loss_dice_6: 1.298  loss_ce_7: 0.5661  loss_mask_7: 0.1818  loss_dice_7: 1.329  loss_ce_8: 0.5649  loss_mask_8: 0.1833  loss_dice_8: 1.327    time: 1.0954  last_time: 1.0571  data_time: 0.0708  last_data_time: 0.0509   lr: 0.0001  max_mem: 31959M
[10/08 16:14:29] d2.utils.events INFO:  eta: 0:12:03  iter: 339  total_loss: 21.84  loss_ce: 0.5706  loss_mask: 0.174  loss_dice: 1.267  loss_ce_0: 0.6542  loss_mask_0: 0.2019  loss_dice_0: 1.582  loss_ce_1: 0.7278  loss_mask_1: 0.1838  loss_dice_1: 1.469  loss_ce_2: 0.6808  loss_mask_2: 0.175  loss_dice_2: 1.389  loss_ce_3: 0.6049  loss_mask_3: 0.176  loss_dice_3: 1.366  loss_ce_4: 0.6011  loss_mask_4: 0.1783  loss_dice_4: 1.331  loss_ce_5: 0.5737  loss_mask_5: 0.1744  loss_dice_5: 1.327  loss_ce_6: 0.5755  loss_mask_6: 0.1772  loss_dice_6: 1.303  loss_ce_7: 0.5828  loss_mask_7: 0.1747  loss_dice_7: 1.293  loss_ce_8: 0.6024  loss_mask_8: 0.1766  loss_dice_8: 1.273    time: 1.0962  last_time: 1.0725  data_time: 0.0670  last_data_time: 0.0749   lr: 0.0001  max_mem: 32477M
[10/08 16:14:51] d2.utils.events INFO:  eta: 0:11:42  iter: 359  total_loss: 20.6  loss_ce: 0.5739  loss_mask: 0.1822  loss_dice: 1.201  loss_ce_0: 0.6072  loss_mask_0: 0.2076  loss_dice_0: 1.438  loss_ce_1: 0.7006  loss_mask_1: 0.2032  loss_dice_1: 1.365  loss_ce_2: 0.6428  loss_mask_2: 0.188  loss_dice_2: 1.281  loss_ce_3: 0.6321  loss_mask_3: 0.189  loss_dice_3: 1.254  loss_ce_4: 0.5906  loss_mask_4: 0.1854  loss_dice_4: 1.236  loss_ce_5: 0.558  loss_mask_5: 0.1881  loss_dice_5: 1.265  loss_ce_6: 0.5426  loss_mask_6: 0.1869  loss_dice_6: 1.247  loss_ce_7: 0.5852  loss_mask_7: 0.1801  loss_dice_7: 1.227  loss_ce_8: 0.5526  loss_mask_8: 0.1824  loss_dice_8: 1.198    time: 1.0969  last_time: 1.0983  data_time: 0.0696  last_data_time: 0.0666   lr: 0.0001  max_mem: 32578M
[10/08 16:15:13] d2.utils.events INFO:  eta: 0:11:20  iter: 379  total_loss: 21.56  loss_ce: 0.5753  loss_mask: 0.1752  loss_dice: 1.264  loss_ce_0: 0.6101  loss_mask_0: 0.2035  loss_dice_0: 1.547  loss_ce_1: 0.7215  loss_mask_1: 0.1934  loss_dice_1: 1.457  loss_ce_2: 0.6514  loss_mask_2: 0.1822  loss_dice_2: 1.365  loss_ce_3: 0.6106  loss_mask_3: 0.1822  loss_dice_3: 1.318  loss_ce_4: 0.5767  loss_mask_4: 0.1788  loss_dice_4: 1.308  loss_ce_5: 0.5825  loss_mask_5: 0.1767  loss_dice_5: 1.324  loss_ce_6: 0.5636  loss_mask_6: 0.1755  loss_dice_6: 1.299  loss_ce_7: 0.5863  loss_mask_7: 0.1758  loss_dice_7: 1.251  loss_ce_8: 0.5683  loss_mask_8: 0.1734  loss_dice_8: 1.254    time: 1.0970  last_time: 1.1106  data_time: 0.0678  last_data_time: 0.0706   lr: 0.0001  max_mem: 32578M
[10/08 16:15:35] d2.utils.events INFO:  eta: 0:10:59  iter: 399  total_loss: 21.52  loss_ce: 0.5351  loss_mask: 0.192  loss_dice: 1.269  loss_ce_0: 0.624  loss_mask_0: 0.2231  loss_dice_0: 1.527  loss_ce_1: 0.7003  loss_mask_1: 0.2037  loss_dice_1: 1.437  loss_ce_2: 0.6375  loss_mask_2: 0.1954  loss_dice_2: 1.395  loss_ce_3: 0.5728  loss_mask_3: 0.1925  loss_dice_3: 1.313  loss_ce_4: 0.5847  loss_mask_4: 0.1909  loss_dice_4: 1.298  loss_ce_5: 0.5461  loss_mask_5: 0.1917  loss_dice_5: 1.326  loss_ce_6: 0.5559  loss_mask_6: 0.1924  loss_dice_6: 1.256  loss_ce_7: 0.5329  loss_mask_7: 0.1926  loss_dice_7: 1.274  loss_ce_8: 0.5437  loss_mask_8: 0.1921  loss_dice_8: 1.282    time: 1.0981  last_time: 1.1598  data_time: 0.0711  last_data_time: 0.0709   lr: 0.0001  max_mem: 32578M
[10/08 16:15:58] d2.utils.events INFO:  eta: 0:10:37  iter: 419  total_loss: 20.86  loss_ce: 0.5323  loss_mask: 0.1936  loss_dice: 1.27  loss_ce_0: 0.6299  loss_mask_0: 0.2158  loss_dice_0: 1.47  loss_ce_1: 0.7037  loss_mask_1: 0.203  loss_dice_1: 1.398  loss_ce_2: 0.6561  loss_mask_2: 0.1975  loss_dice_2: 1.338  loss_ce_3: 0.5962  loss_mask_3: 0.1976  loss_dice_3: 1.292  loss_ce_4: 0.5517  loss_mask_4: 0.1974  loss_dice_4: 1.303  loss_ce_5: 0.5517  loss_mask_5: 0.1947  loss_dice_5: 1.312  loss_ce_6: 0.55  loss_mask_6: 0.1943  loss_dice_6: 1.245  loss_ce_7: 0.5674  loss_mask_7: 0.1943  loss_dice_7: 1.258  loss_ce_8: 0.548  loss_mask_8: 0.1944  loss_dice_8: 1.243    time: 1.0994  last_time: 1.1019  data_time: 0.0823  last_data_time: 0.0559   lr: 0.0001  max_mem: 32578M
[10/08 16:16:20] d2.utils.events INFO:  eta: 0:10:15  iter: 439  total_loss: 20.73  loss_ce: 0.5344  loss_mask: 0.1894  loss_dice: 1.262  loss_ce_0: 0.6019  loss_mask_0: 0.2169  loss_dice_0: 1.496  loss_ce_1: 0.6853  loss_mask_1: 0.1996  loss_dice_1: 1.394  loss_ce_2: 0.5996  loss_mask_2: 0.1895  loss_dice_2: 1.333  loss_ce_3: 0.5692  loss_mask_3: 0.1865  loss_dice_3: 1.278  loss_ce_4: 0.538  loss_mask_4: 0.1883  loss_dice_4: 1.299  loss_ce_5: 0.5285  loss_mask_5: 0.1898  loss_dice_5: 1.298  loss_ce_6: 0.5005  loss_mask_6: 0.1891  loss_dice_6: 1.265  loss_ce_7: 0.4864  loss_mask_7: 0.1899  loss_dice_7: 1.25  loss_ce_8: 0.5345  loss_mask_8: 0.1899  loss_dice_8: 1.254    time: 1.0996  last_time: 1.0719  data_time: 0.0792  last_data_time: 0.0599   lr: 0.0001  max_mem: 32578M
[10/08 16:16:43] d2.utils.events INFO:  eta: 0:09:54  iter: 459  total_loss: 21.82  loss_ce: 0.6068  loss_mask: 0.1889  loss_dice: 1.34  loss_ce_0: 0.6292  loss_mask_0: 0.2227  loss_dice_0: 1.561  loss_ce_1: 0.7183  loss_mask_1: 0.1988  loss_dice_1: 1.505  loss_ce_2: 0.6685  loss_mask_2: 0.1951  loss_dice_2: 1.396  loss_ce_3: 0.6133  loss_mask_3: 0.1901  loss_dice_3: 1.316  loss_ce_4: 0.5965  loss_mask_4: 0.1909  loss_dice_4: 1.329  loss_ce_5: 0.5817  loss_mask_5: 0.1883  loss_dice_5: 1.359  loss_ce_6: 0.561  loss_mask_6: 0.1887  loss_dice_6: 1.308  loss_ce_7: 0.5621  loss_mask_7: 0.1888  loss_dice_7: 1.348  loss_ce_8: 0.5712  loss_mask_8: 0.1893  loss_dice_8: 1.315    time: 1.1021  last_time: 1.1272  data_time: 0.0848  last_data_time: 0.0913   lr: 0.0001  max_mem: 32672M
[10/08 16:17:05] d2.utils.events INFO:  eta: 0:09:32  iter: 479  total_loss: 20.77  loss_ce: 0.5645  loss_mask: 0.1856  loss_dice: 1.236  loss_ce_0: 0.6012  loss_mask_0: 0.2396  loss_dice_0: 1.444  loss_ce_1: 0.6952  loss_mask_1: 0.2209  loss_dice_1: 1.408  loss_ce_2: 0.6199  loss_mask_2: 0.2027  loss_dice_2: 1.319  loss_ce_3: 0.5903  loss_mask_3: 0.1983  loss_dice_3: 1.261  loss_ce_4: 0.5639  loss_mask_4: 0.1949  loss_dice_4: 1.264  loss_ce_5: 0.5186  loss_mask_5: 0.1937  loss_dice_5: 1.272  loss_ce_6: 0.5375  loss_mask_6: 0.1962  loss_dice_6: 1.235  loss_ce_7: 0.5413  loss_mask_7: 0.1932  loss_dice_7: 1.206  loss_ce_8: 0.53  loss_mask_8: 0.1869  loss_dice_8: 1.252    time: 1.1022  last_time: 1.1581  data_time: 0.0776  last_data_time: 0.0906   lr: 0.0001  max_mem: 32672M
[10/08 16:17:27] d2.utils.events INFO:  eta: 0:09:09  iter: 499  total_loss: 19.54  loss_ce: 0.5155  loss_mask: 0.1868  loss_dice: 1.15  loss_ce_0: 0.5622  loss_mask_0: 0.2134  loss_dice_0: 1.387  loss_ce_1: 0.6264  loss_mask_1: 0.2025  loss_dice_1: 1.304  loss_ce_2: 0.5927  loss_mask_2: 0.1952  loss_dice_2: 1.257  loss_ce_3: 0.5408  loss_mask_3: 0.1882  loss_dice_3: 1.201  loss_ce_4: 0.5324  loss_mask_4: 0.1885  loss_dice_4: 1.178  loss_ce_5: 0.5405  loss_mask_5: 0.1861  loss_dice_5: 1.157  loss_ce_6: 0.5224  loss_mask_6: 0.1861  loss_dice_6: 1.208  loss_ce_7: 0.5063  loss_mask_7: 0.1881  loss_dice_7: 1.162  loss_ce_8: 0.4897  loss_mask_8: 0.19  loss_dice_8: 1.165    time: 1.1017  last_time: 1.0169  data_time: 0.0753  last_data_time: 0.0686   lr: 0.0001  max_mem: 32672M
[10/08 16:17:49] d2.utils.events INFO:  eta: 0:08:47  iter: 519  total_loss: 19.64  loss_ce: 0.462  loss_mask: 0.1804  loss_dice: 1.196  loss_ce_0: 0.568  loss_mask_0: 0.2094  loss_dice_0: 1.431  loss_ce_1: 0.6263  loss_mask_1: 0.1943  loss_dice_1: 1.372  loss_ce_2: 0.5824  loss_mask_2: 0.1874  loss_dice_2: 1.321  loss_ce_3: 0.5392  loss_mask_3: 0.182  loss_dice_3: 1.234  loss_ce_4: 0.4813  loss_mask_4: 0.1862  loss_dice_4: 1.258  loss_ce_5: 0.4446  loss_mask_5: 0.1819  loss_dice_5: 1.221  loss_ce_6: 0.4721  loss_mask_6: 0.1822  loss_dice_6: 1.175  loss_ce_7: 0.4808  loss_mask_7: 0.1828  loss_dice_7: 1.175  loss_ce_8: 0.4838  loss_mask_8: 0.1808  loss_dice_8: 1.21    time: 1.1004  last_time: 1.2091  data_time: 0.0696  last_data_time: 0.0649   lr: 0.0001  max_mem: 32672M
[10/08 16:18:10] d2.utils.events INFO:  eta: 0:08:25  iter: 539  total_loss: 20.52  loss_ce: 0.5164  loss_mask: 0.1716  loss_dice: 1.238  loss_ce_0: 0.5647  loss_mask_0: 0.1908  loss_dice_0: 1.507  loss_ce_1: 0.6831  loss_mask_1: 0.1865  loss_dice_1: 1.372  loss_ce_2: 0.6209  loss_mask_2: 0.1724  loss_dice_2: 1.313  loss_ce_3: 0.5845  loss_mask_3: 0.1734  loss_dice_3: 1.308  loss_ce_4: 0.5573  loss_mask_4: 0.1708  loss_dice_4: 1.239  loss_ce_5: 0.5342  loss_mask_5: 0.1701  loss_dice_5: 1.288  loss_ce_6: 0.5374  loss_mask_6: 0.1709  loss_dice_6: 1.261  loss_ce_7: 0.5356  loss_mask_7: 0.171  loss_dice_7: 1.243  loss_ce_8: 0.5105  loss_mask_8: 0.1732  loss_dice_8: 1.257    time: 1.0996  last_time: 1.0622  data_time: 0.0736  last_data_time: 0.0756   lr: 0.0001  max_mem: 32672M
[10/08 16:18:32] d2.utils.events INFO:  eta: 0:08:02  iter: 559  total_loss: 21.61  loss_ce: 0.5287  loss_mask: 0.1771  loss_dice: 1.377  loss_ce_0: 0.5912  loss_mask_0: 0.2114  loss_dice_0: 1.572  loss_ce_1: 0.7168  loss_mask_1: 0.1895  loss_dice_1: 1.506  loss_ce_2: 0.6239  loss_mask_2: 0.1863  loss_dice_2: 1.427  loss_ce_3: 0.5837  loss_mask_3: 0.1769  loss_dice_3: 1.401  loss_ce_4: 0.5793  loss_mask_4: 0.1772  loss_dice_4: 1.372  loss_ce_5: 0.5744  loss_mask_5: 0.1753  loss_dice_5: 1.394  loss_ce_6: 0.5428  loss_mask_6: 0.1755  loss_dice_6: 1.334  loss_ce_7: 0.5305  loss_mask_7: 0.178  loss_dice_7: 1.406  loss_ce_8: 0.5232  loss_mask_8: 0.1764  loss_dice_8: 1.351    time: 1.0988  last_time: 1.0227  data_time: 0.0738  last_data_time: 0.0703   lr: 0.0001  max_mem: 32672M
[10/08 16:18:54] d2.utils.events INFO:  eta: 0:07:40  iter: 579  total_loss: 22.34  loss_ce: 0.5737  loss_mask: 0.1673  loss_dice: 1.388  loss_ce_0: 0.6263  loss_mask_0: 0.1981  loss_dice_0: 1.637  loss_ce_1: 0.7467  loss_mask_1: 0.1842  loss_dice_1: 1.517  loss_ce_2: 0.6829  loss_mask_2: 0.1796  loss_dice_2: 1.445  loss_ce_3: 0.6192  loss_mask_3: 0.1717  loss_dice_3: 1.437  loss_ce_4: 0.6106  loss_mask_4: 0.1714  loss_dice_4: 1.417  loss_ce_5: 0.6167  loss_mask_5: 0.1701  loss_dice_5: 1.426  loss_ce_6: 0.5991  loss_mask_6: 0.1699  loss_dice_6: 1.415  loss_ce_7: 0.5938  loss_mask_7: 0.166  loss_dice_7: 1.383  loss_ce_8: 0.5885  loss_mask_8: 0.167  loss_dice_8: 1.414    time: 1.0985  last_time: 1.0669  data_time: 0.0739  last_data_time: 0.0902   lr: 0.0001  max_mem: 32672M
[10/08 16:19:16] d2.utils.events INFO:  eta: 0:07:18  iter: 599  total_loss: 21.9  loss_ce: 0.5564  loss_mask: 0.1602  loss_dice: 1.316  loss_ce_0: 0.5989  loss_mask_0: 0.1925  loss_dice_0: 1.574  loss_ce_1: 0.7088  loss_mask_1: 0.1662  loss_dice_1: 1.48  loss_ce_2: 0.637  loss_mask_2: 0.1676  loss_dice_2: 1.417  loss_ce_3: 0.6054  loss_mask_3: 0.165  loss_dice_3: 1.349  loss_ce_4: 0.5585  loss_mask_4: 0.1667  loss_dice_4: 1.319  loss_ce_5: 0.5836  loss_mask_5: 0.1623  loss_dice_5: 1.306  loss_ce_6: 0.5407  loss_mask_6: 0.1628  loss_dice_6: 1.295  loss_ce_7: 0.5465  loss_mask_7: 0.164  loss_dice_7: 1.326  loss_ce_8: 0.5378  loss_mask_8: 0.1638  loss_dice_8: 1.319    time: 1.0987  last_time: 1.1178  data_time: 0.0791  last_data_time: 0.0772   lr: 0.0001  max_mem: 32672M
[10/08 16:19:38] d2.utils.events INFO:  eta: 0:06:57  iter: 619  total_loss: 20.86  loss_ce: 0.5424  loss_mask: 0.1707  loss_dice: 1.283  loss_ce_0: 0.6045  loss_mask_0: 0.2011  loss_dice_0: 1.51  loss_ce_1: 0.7274  loss_mask_1: 0.1839  loss_dice_1: 1.387  loss_ce_2: 0.6574  loss_mask_2: 0.1732  loss_dice_2: 1.349  loss_ce_3: 0.6126  loss_mask_3: 0.1728  loss_dice_3: 1.3  loss_ce_4: 0.6031  loss_mask_4: 0.1707  loss_dice_4: 1.275  loss_ce_5: 0.5465  loss_mask_5: 0.1694  loss_dice_5: 1.275  loss_ce_6: 0.5547  loss_mask_6: 0.1689  loss_dice_6: 1.259  loss_ce_7: 0.5061  loss_mask_7: 0.1702  loss_dice_7: 1.25  loss_ce_8: 0.5353  loss_mask_8: 0.1697  loss_dice_8: 1.264    time: 1.0989  last_time: 1.1040  data_time: 0.0823  last_data_time: 0.0958   lr: 0.0001  max_mem: 32672M
[10/08 16:20:01] d2.utils.events INFO:  eta: 0:06:35  iter: 639  total_loss: 20.84  loss_ce: 0.5338  loss_mask: 0.1837  loss_dice: 1.27  loss_ce_0: 0.6217  loss_mask_0: 0.2048  loss_dice_0: 1.519  loss_ce_1: 0.7114  loss_mask_1: 0.1961  loss_dice_1: 1.437  loss_ce_2: 0.6156  loss_mask_2: 0.1866  loss_dice_2: 1.339  loss_ce_3: 0.5648  loss_mask_3: 0.1861  loss_dice_3: 1.302  loss_ce_4: 0.5407  loss_mask_4: 0.1839  loss_dice_4: 1.289  loss_ce_5: 0.5461  loss_mask_5: 0.1814  loss_dice_5: 1.305  loss_ce_6: 0.5323  loss_mask_6: 0.1846  loss_dice_6: 1.325  loss_ce_7: 0.5401  loss_mask_7: 0.1824  loss_dice_7: 1.262  loss_ce_8: 0.5241  loss_mask_8: 0.1826  loss_dice_8: 1.232    time: 1.0999  last_time: 1.1588  data_time: 0.0860  last_data_time: 0.1095   lr: 0.0001  max_mem: 32672M
[10/08 16:20:23] d2.utils.events INFO:  eta: 0:06:13  iter: 659  total_loss: 20.17  loss_ce: 0.5429  loss_mask: 0.1957  loss_dice: 1.242  loss_ce_0: 0.5729  loss_mask_0: 0.2177  loss_dice_0: 1.48  loss_ce_1: 0.6396  loss_mask_1: 0.2038  loss_dice_1: 1.373  loss_ce_2: 0.6151  loss_mask_2: 0.1995  loss_dice_2: 1.307  loss_ce_3: 0.5735  loss_mask_3: 0.199  loss_dice_3: 1.228  loss_ce_4: 0.5413  loss_mask_4: 0.1966  loss_dice_4: 1.232  loss_ce_5: 0.5682  loss_mask_5: 0.196  loss_dice_5: 1.257  loss_ce_6: 0.538  loss_mask_6: 0.197  loss_dice_6: 1.225  loss_ce_7: 0.5519  loss_mask_7: 0.1932  loss_dice_7: 1.208  loss_ce_8: 0.5382  loss_mask_8: 0.1953  loss_dice_8: 1.193    time: 1.0995  last_time: 1.0982  data_time: 0.0768  last_data_time: 0.0928   lr: 0.0001  max_mem: 32672M
[10/08 16:20:45] d2.utils.events INFO:  eta: 0:05:51  iter: 679  total_loss: 21.43  loss_ce: 0.5226  loss_mask: 0.1783  loss_dice: 1.234  loss_ce_0: 0.5881  loss_mask_0: 0.2117  loss_dice_0: 1.466  loss_ce_1: 0.6641  loss_mask_1: 0.1935  loss_dice_1: 1.451  loss_ce_2: 0.6243  loss_mask_2: 0.1806  loss_dice_2: 1.369  loss_ce_3: 0.6125  loss_mask_3: 0.1827  loss_dice_3: 1.333  loss_ce_4: 0.5707  loss_mask_4: 0.1798  loss_dice_4: 1.321  loss_ce_5: 0.5415  loss_mask_5: 0.1772  loss_dice_5: 1.306  loss_ce_6: 0.5343  loss_mask_6: 0.1762  loss_dice_6: 1.29  loss_ce_7: 0.5165  loss_mask_7: 0.175  loss_dice_7: 1.304  loss_ce_8: 0.5227  loss_mask_8: 0.1748  loss_dice_8: 1.255    time: 1.0998  last_time: 1.1499  data_time: 0.0808  last_data_time: 0.0749   lr: 0.0001  max_mem: 32672M
[10/08 16:21:07] d2.utils.events INFO:  eta: 0:05:29  iter: 699  total_loss: 19.65  loss_ce: 0.4888  loss_mask: 0.1738  loss_dice: 1.159  loss_ce_0: 0.5664  loss_mask_0: 0.1979  loss_dice_0: 1.354  loss_ce_1: 0.669  loss_mask_1: 0.1806  loss_dice_1: 1.283  loss_ce_2: 0.6029  loss_mask_2: 0.1744  loss_dice_2: 1.246  loss_ce_3: 0.5229  loss_mask_3: 0.176  loss_dice_3: 1.203  loss_ce_4: 0.5147  loss_mask_4: 0.1701  loss_dice_4: 1.189  loss_ce_5: 0.5023  loss_mask_5: 0.1719  loss_dice_5: 1.172  loss_ce_6: 0.4883  loss_mask_6: 0.1705  loss_dice_6: 1.15  loss_ce_7: 0.4797  loss_mask_7: 0.1698  loss_dice_7: 1.173  loss_ce_8: 0.4919  loss_mask_8: 0.1695  loss_dice_8: 1.137    time: 1.1004  last_time: 1.1071  data_time: 0.0843  last_data_time: 0.0789   lr: 0.0001  max_mem: 32672M
[10/08 16:21:30] d2.utils.events INFO:  eta: 0:05:07  iter: 719  total_loss: 19.71  loss_ce: 0.5064  loss_mask: 0.1761  loss_dice: 1.199  loss_ce_0: 0.6019  loss_mask_0: 0.2037  loss_dice_0: 1.431  loss_ce_1: 0.671  loss_mask_1: 0.1903  loss_dice_1: 1.372  loss_ce_2: 0.6149  loss_mask_2: 0.1795  loss_dice_2: 1.299  loss_ce_3: 0.5987  loss_mask_3: 0.1789  loss_dice_3: 1.259  loss_ce_4: 0.5246  loss_mask_4: 0.176  loss_dice_4: 1.237  loss_ce_5: 0.5285  loss_mask_5: 0.1762  loss_dice_5: 1.234  loss_ce_6: 0.511  loss_mask_6: 0.1767  loss_dice_6: 1.203  loss_ce_7: 0.5126  loss_mask_7: 0.1769  loss_dice_7: 1.217  loss_ce_8: 0.5265  loss_mask_8: 0.1772  loss_dice_8: 1.232    time: 1.1009  last_time: 1.1158  data_time: 0.0842  last_data_time: 0.0981   lr: 0.0001  max_mem: 32672M
[10/08 16:21:53] d2.utils.events INFO:  eta: 0:04:46  iter: 739  total_loss: 20.83  loss_ce: 0.513  loss_mask: 0.171  loss_dice: 1.271  loss_ce_0: 0.5725  loss_mask_0: 0.1946  loss_dice_0: 1.537  loss_ce_1: 0.6197  loss_mask_1: 0.1871  loss_dice_1: 1.412  loss_ce_2: 0.5994  loss_mask_2: 0.1762  loss_dice_2: 1.374  loss_ce_3: 0.5509  loss_mask_3: 0.1781  loss_dice_3: 1.297  loss_ce_4: 0.5236  loss_mask_4: 0.1763  loss_dice_4: 1.319  loss_ce_5: 0.5229  loss_mask_5: 0.1735  loss_dice_5: 1.267  loss_ce_6: 0.5208  loss_mask_6: 0.1747  loss_dice_6: 1.262  loss_ce_7: 0.5156  loss_mask_7: 0.1719  loss_dice_7: 1.308  loss_ce_8: 0.4894  loss_mask_8: 0.1696  loss_dice_8: 1.318    time: 1.1021  last_time: 1.1837  data_time: 0.0936  last_data_time: 0.0934   lr: 0.0001  max_mem: 32672M
[10/08 16:22:15] d2.utils.events INFO:  eta: 0:04:24  iter: 759  total_loss: 20.14  loss_ce: 0.4851  loss_mask: 0.1673  loss_dice: 1.218  loss_ce_0: 0.5552  loss_mask_0: 0.186  loss_dice_0: 1.476  loss_ce_1: 0.6565  loss_mask_1: 0.1799  loss_dice_1: 1.394  loss_ce_2: 0.5701  loss_mask_2: 0.1733  loss_dice_2: 1.312  loss_ce_3: 0.5364  loss_mask_3: 0.1695  loss_dice_3: 1.233  loss_ce_4: 0.5359  loss_mask_4: 0.1675  loss_dice_4: 1.247  loss_ce_5: 0.495  loss_mask_5: 0.1682  loss_dice_5: 1.256  loss_ce_6: 0.5041  loss_mask_6: 0.1673  loss_dice_6: 1.222  loss_ce_7: 0.493  loss_mask_7: 0.1694  loss_dice_7: 1.211  loss_ce_8: 0.4657  loss_mask_8: 0.1686  loss_dice_8: 1.223    time: 1.1025  last_time: 1.1550  data_time: 0.0783  last_data_time: 0.0897   lr: 0.0001  max_mem: 32672M
[10/08 16:22:37] d2.utils.events INFO:  eta: 0:04:02  iter: 779  total_loss: 21.3  loss_ce: 0.5182  loss_mask: 0.1675  loss_dice: 1.308  loss_ce_0: 0.5973  loss_mask_0: 0.1934  loss_dice_0: 1.598  loss_ce_1: 0.6738  loss_mask_1: 0.1862  loss_dice_1: 1.492  loss_ce_2: 0.6232  loss_mask_2: 0.1734  loss_dice_2: 1.401  loss_ce_3: 0.5622  loss_mask_3: 0.1753  loss_dice_3: 1.332  loss_ce_4: 0.5492  loss_mask_4: 0.1743  loss_dice_4: 1.344  loss_ce_5: 0.5279  loss_mask_5: 0.1698  loss_dice_5: 1.307  loss_ce_6: 0.4901  loss_mask_6: 0.1713  loss_dice_6: 1.328  loss_ce_7: 0.5076  loss_mask_7: 0.17  loss_dice_7: 1.294  loss_ce_8: 0.4969  loss_mask_8: 0.1693  loss_dice_8: 1.302    time: 1.1026  last_time: 1.0782  data_time: 0.0777  last_data_time: 0.0823   lr: 0.0001  max_mem: 32672M
[10/08 16:23:00] d2.utils.events INFO:  eta: 0:03:40  iter: 799  total_loss: 20.92  loss_ce: 0.5401  loss_mask: 0.1846  loss_dice: 1.306  loss_ce_0: 0.5956  loss_mask_0: 0.202  loss_dice_0: 1.501  loss_ce_1: 0.6886  loss_mask_1: 0.2004  loss_dice_1: 1.429  loss_ce_2: 0.6392  loss_mask_2: 0.1904  loss_dice_2: 1.385  loss_ce_3: 0.5698  loss_mask_3: 0.1848  loss_dice_3: 1.308  loss_ce_4: 0.601  loss_mask_4: 0.1885  loss_dice_4: 1.346  loss_ce_5: 0.5548  loss_mask_5: 0.1869  loss_dice_5: 1.305  loss_ce_6: 0.5429  loss_mask_6: 0.1879  loss_dice_6: 1.283  loss_ce_7: 0.5461  loss_mask_7: 0.1855  loss_dice_7: 1.301  loss_ce_8: 0.5507  loss_mask_8: 0.1855  loss_dice_8: 1.251    time: 1.1031  last_time: 1.1415  data_time: 0.0867  last_data_time: 0.0805   lr: 0.0001  max_mem: 32672M
[10/08 16:23:21] d2.utils.events INFO:  eta: 0:03:18  iter: 819  total_loss: 20.31  loss_ce: 0.5143  loss_mask: 0.1622  loss_dice: 1.21  loss_ce_0: 0.6027  loss_mask_0: 0.197  loss_dice_0: 1.473  loss_ce_1: 0.6406  loss_mask_1: 0.18  loss_dice_1: 1.369  loss_ce_2: 0.5969  loss_mask_2: 0.1683  loss_dice_2: 1.279  loss_ce_3: 0.5598  loss_mask_3: 0.1685  loss_dice_3: 1.277  loss_ce_4: 0.5378  loss_mask_4: 0.1688  loss_dice_4: 1.266  loss_ce_5: 0.5219  loss_mask_5: 0.1672  loss_dice_5: 1.229  loss_ce_6: 0.5231  loss_mask_6: 0.1634  loss_dice_6: 1.227  loss_ce_7: 0.5147  loss_mask_7: 0.1631  loss_dice_7: 1.244  loss_ce_8: 0.5089  loss_mask_8: 0.1642  loss_dice_8: 1.23    time: 1.1026  last_time: 1.0567  data_time: 0.0749  last_data_time: 0.0523   lr: 0.0001  max_mem: 32672M
[10/08 16:23:43] d2.utils.events INFO:  eta: 0:02:56  iter: 839  total_loss: 18.5  loss_ce: 0.4254  loss_mask: 0.1729  loss_dice: 1.146  loss_ce_0: 0.541  loss_mask_0: 0.1975  loss_dice_0: 1.366  loss_ce_1: 0.6096  loss_mask_1: 0.1844  loss_dice_1: 1.316  loss_ce_2: 0.5717  loss_mask_2: 0.1749  loss_dice_2: 1.235  loss_ce_3: 0.5036  loss_mask_3: 0.1741  loss_dice_3: 1.183  loss_ce_4: 0.4755  loss_mask_4: 0.1743  loss_dice_4: 1.208  loss_ce_5: 0.4498  loss_mask_5: 0.174  loss_dice_5: 1.176  loss_ce_6: 0.4166  loss_mask_6: 0.1726  loss_dice_6: 1.165  loss_ce_7: 0.4357  loss_mask_7: 0.172  loss_dice_7: 1.146  loss_ce_8: 0.4786  loss_mask_8: 0.1731  loss_dice_8: 1.183    time: 1.1021  last_time: 1.0521  data_time: 0.0785  last_data_time: 0.0839   lr: 0.0001  max_mem: 32672M
[10/08 16:24:05] d2.utils.events INFO:  eta: 0:02:34  iter: 859  total_loss: 20.82  loss_ce: 0.5068  loss_mask: 0.1701  loss_dice: 1.234  loss_ce_0: 0.6019  loss_mask_0: 0.2107  loss_dice_0: 1.471  loss_ce_1: 0.6555  loss_mask_1: 0.1927  loss_dice_1: 1.403  loss_ce_2: 0.611  loss_mask_2: 0.1769  loss_dice_2: 1.34  loss_ce_3: 0.5551  loss_mask_3: 0.1723  loss_dice_3: 1.304  loss_ce_4: 0.5293  loss_mask_4: 0.1764  loss_dice_4: 1.287  loss_ce_5: 0.5371  loss_mask_5: 0.1757  loss_dice_5: 1.271  loss_ce_6: 0.4899  loss_mask_6: 0.1755  loss_dice_6: 1.224  loss_ce_7: 0.4805  loss_mask_7: 0.175  loss_dice_7: 1.267  loss_ce_8: 0.5172  loss_mask_8: 0.1728  loss_dice_8: 1.266    time: 1.1015  last_time: 1.0653  data_time: 0.0709  last_data_time: 0.0717   lr: 0.0001  max_mem: 32672M
[10/08 16:24:27] d2.utils.events INFO:  eta: 0:02:11  iter: 879  total_loss: 20.42  loss_ce: 0.5156  loss_mask: 0.1616  loss_dice: 1.244  loss_ce_0: 0.5729  loss_mask_0: 0.1903  loss_dice_0: 1.488  loss_ce_1: 0.6796  loss_mask_1: 0.1765  loss_dice_1: 1.365  loss_ce_2: 0.5768  loss_mask_2: 0.1671  loss_dice_2: 1.331  loss_ce_3: 0.5486  loss_mask_3: 0.1656  loss_dice_3: 1.252  loss_ce_4: 0.524  loss_mask_4: 0.1641  loss_dice_4: 1.257  loss_ce_5: 0.5437  loss_mask_5: 0.1602  loss_dice_5: 1.274  loss_ce_6: 0.4934  loss_mask_6: 0.16  loss_dice_6: 1.275  loss_ce_7: 0.5063  loss_mask_7: 0.162  loss_dice_7: 1.249  loss_ce_8: 0.5014  loss_mask_8: 0.1596  loss_dice_8: 1.256    time: 1.1011  last_time: 1.0794  data_time: 0.0752  last_data_time: 0.0599   lr: 0.0001  max_mem: 32672M
[10/08 16:24:48] d2.utils.events INFO:  eta: 0:01:49  iter: 899  total_loss: 20.04  loss_ce: 0.4931  loss_mask: 0.1686  loss_dice: 1.222  loss_ce_0: 0.568  loss_mask_0: 0.2034  loss_dice_0: 1.494  loss_ce_1: 0.6446  loss_mask_1: 0.1858  loss_dice_1: 1.352  loss_ce_2: 0.5891  loss_mask_2: 0.1729  loss_dice_2: 1.305  loss_ce_3: 0.532  loss_mask_3: 0.1727  loss_dice_3: 1.237  loss_ce_4: 0.4997  loss_mask_4: 0.1727  loss_dice_4: 1.253  loss_ce_5: 0.5233  loss_mask_5: 0.1744  loss_dice_5: 1.234  loss_ce_6: 0.5209  loss_mask_6: 0.1721  loss_dice_6: 1.224  loss_ce_7: 0.4533  loss_mask_7: 0.1696  loss_dice_7: 1.218  loss_ce_8: 0.4677  loss_mask_8: 0.1697  loss_dice_8: 1.202    time: 1.1009  last_time: 1.0435  data_time: 0.0771  last_data_time: 0.0538   lr: 0.0001  max_mem: 32672M
[10/08 16:25:10] d2.utils.events INFO:  eta: 0:01:27  iter: 919  total_loss: 19.33  loss_ce: 0.4924  loss_mask: 0.1831  loss_dice: 1.182  loss_ce_0: 0.5917  loss_mask_0: 0.2168  loss_dice_0: 1.4  loss_ce_1: 0.6594  loss_mask_1: 0.2012  loss_dice_1: 1.306  loss_ce_2: 0.5565  loss_mask_2: 0.1877  loss_dice_2: 1.253  loss_ce_3: 0.4925  loss_mask_3: 0.1865  loss_dice_3: 1.209  loss_ce_4: 0.5182  loss_mask_4: 0.1847  loss_dice_4: 1.218  loss_ce_5: 0.4653  loss_mask_5: 0.1861  loss_dice_5: 1.212  loss_ce_6: 0.4661  loss_mask_6: 0.1858  loss_dice_6: 1.193  loss_ce_7: 0.4534  loss_mask_7: 0.1862  loss_dice_7: 1.177  loss_ce_8: 0.5088  loss_mask_8: 0.1833  loss_dice_8: 1.182    time: 1.1003  last_time: 1.0434  data_time: 0.0709  last_data_time: 0.0681   lr: 0.0001  max_mem: 32708M
[10/08 16:25:32] d2.utils.events INFO:  eta: 0:01:05  iter: 939  total_loss: 20.31  loss_ce: 0.4731  loss_mask: 0.1674  loss_dice: 1.23  loss_ce_0: 0.5914  loss_mask_0: 0.2073  loss_dice_0: 1.427  loss_ce_1: 0.6649  loss_mask_1: 0.1885  loss_dice_1: 1.405  loss_ce_2: 0.6065  loss_mask_2: 0.1764  loss_dice_2: 1.348  loss_ce_3: 0.5553  loss_mask_3: 0.1765  loss_dice_3: 1.31  loss_ce_4: 0.5049  loss_mask_4: 0.1731  loss_dice_4: 1.284  loss_ce_5: 0.5242  loss_mask_5: 0.1741  loss_dice_5: 1.236  loss_ce_6: 0.4955  loss_mask_6: 0.1711  loss_dice_6: 1.282  loss_ce_7: 0.5031  loss_mask_7: 0.1692  loss_dice_7: 1.258  loss_ce_8: 0.5131  loss_mask_8: 0.1701  loss_dice_8: 1.263    time: 1.0999  last_time: 1.0914  data_time: 0.0764  last_data_time: 0.0609   lr: 0.0001  max_mem: 32708M
[10/08 16:25:54] d2.utils.events INFO:  eta: 0:00:43  iter: 959  total_loss: 19.66  loss_ce: 0.4833  loss_mask: 0.1872  loss_dice: 1.175  loss_ce_0: 0.5528  loss_mask_0: 0.2198  loss_dice_0: 1.434  loss_ce_1: 0.6084  loss_mask_1: 0.2002  loss_dice_1: 1.318  loss_ce_2: 0.5863  loss_mask_2: 0.1894  loss_dice_2: 1.249  loss_ce_3: 0.4954  loss_mask_3: 0.1857  loss_dice_3: 1.233  loss_ce_4: 0.4936  loss_mask_4: 0.1856  loss_dice_4: 1.209  loss_ce_5: 0.4786  loss_mask_5: 0.1888  loss_dice_5: 1.177  loss_ce_6: 0.5057  loss_mask_6: 0.1865  loss_dice_6: 1.192  loss_ce_7: 0.4994  loss_mask_7: 0.1872  loss_dice_7: 1.193  loss_ce_8: 0.4883  loss_mask_8: 0.1901  loss_dice_8: 1.178    time: 1.1000  last_time: 1.1704  data_time: 0.0818  last_data_time: 0.0750   lr: 0.0001  max_mem: 32708M
[10/08 16:26:16] d2.utils.events INFO:  eta: 0:00:21  iter: 979  total_loss: 19.96  loss_ce: 0.4934  loss_mask: 0.1526  loss_dice: 1.175  loss_ce_0: 0.5811  loss_mask_0: 0.174  loss_dice_0: 1.425  loss_ce_1: 0.6356  loss_mask_1: 0.1616  loss_dice_1: 1.352  loss_ce_2: 0.6111  loss_mask_2: 0.1552  loss_dice_2: 1.257  loss_ce_3: 0.5266  loss_mask_3: 0.1566  loss_dice_3: 1.207  loss_ce_4: 0.5295  loss_mask_4: 0.1558  loss_dice_4: 1.202  loss_ce_5: 0.5104  loss_mask_5: 0.1537  loss_dice_5: 1.197  loss_ce_6: 0.5056  loss_mask_6: 0.1555  loss_dice_6: 1.208  loss_ce_7: 0.5088  loss_mask_7: 0.1541  loss_dice_7: 1.201  loss_ce_8: 0.4872  loss_mask_8: 0.1551  loss_dice_8: 1.223    time: 1.1001  last_time: 1.1240  data_time: 0.0713  last_data_time: 0.0715   lr: 0.0001  max_mem: 32708M
[10/08 16:26:38] fvcore.common.checkpoint INFO: Saving checkpoint to /tsi/hi-paris/GB/segmentation/results/In_vocab/r50_008_1000_19_a_decoder/model_final.pth
[10/08 16:26:41] d2.utils.events INFO:  eta: 0:00:00  iter: 999  total_loss: 19.33  loss_ce: 0.4637  loss_mask: 0.1689  loss_dice: 1.189  loss_ce_0: 0.5406  loss_mask_0: 0.195  loss_dice_0: 1.424  loss_ce_1: 0.5937  loss_mask_1: 0.1796  loss_dice_1: 1.365  loss_ce_2: 0.6111  loss_mask_2: 0.1748  loss_dice_2: 1.283  loss_ce_3: 0.5358  loss_mask_3: 0.1705  loss_dice_3: 1.215  loss_ce_4: 0.5179  loss_mask_4: 0.1688  loss_dice_4: 1.228  loss_ce_5: 0.5317  loss_mask_5: 0.1706  loss_dice_5: 1.199  loss_ce_6: 0.5044  loss_mask_6: 0.1667  loss_dice_6: 1.204  loss_ce_7: 0.502  loss_mask_7: 0.1676  loss_dice_7: 1.196  loss_ce_8: 0.5005  loss_mask_8: 0.1671  loss_dice_8: 1.176    time: 1.1004  last_time: 1.1165  data_time: 0.0790  last_data_time: 0.0702   lr: 0.0001  max_mem: 32708M
[10/08 16:26:41] d2.engine.hooks INFO: Overall training speed: 998 iterations in 0:18:18 (1.1004 s / it)
[10/08 16:26:41] d2.engine.hooks INFO: Total training time: 0:18:23 (0:00:05 on hooks)
[10/08 16:26:41] fcclip.data.datasets.register_cityscapes_panoptic INFO: 3 cities found in 'datasets/cityscapes/leftImg8bit/val'.
[10/08 16:26:41] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(1024, 1024), max_size=2560, sample_style='choice')]
[10/08 16:26:41] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/08 16:26:41] d2.data.common INFO: Serializing 500 elements to byte tensors and concatenating them all ...
[10/08 16:26:41] d2.data.common INFO: Serialized dataset takes 0.74 MiB
[10/08 16:26:41] d2.evaluation.evaluator INFO: Start inference on 500 batches
[10/09 11:42:03] detectron2 INFO: Rank of current process: 0. World size: 2
[10/09 11:42:04] detectron2 INFO: Environment info:
-------------------------------  -------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.8.19 | packaged by conda-forge | (default, Mar 20 2024, 12:47:35) [GCC 12.3.0]
numpy                            1.24.4
detectron2                       0.6 @/home/ids/gbrison/segmentation/segmentation/detectron2/detectron2
Compiler                         GCC 11.4
CUDA compiler                    CUDA 12.5
detectron2 arch flags            8.9
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.0+cu121 @/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0,1                          NVIDIA L40S (arch=8.9)
Driver version                   555.42.06
CUDA_HOME                        /usr/local/cuda
Pillow                           9.4.0
torchvision                      0.19.0+cu121 @/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torchvision
torchvision arch flags           5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.10.0
-------------------------------  -------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX512
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.0, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/09 11:42:04] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco/panoptic-segmentation/fcclip/r50_exp_1000_a_decoder.yaml', dist_url='tcp://127.0.0.1:51163', eval_only=False, machine_rank=0, num_gpus=2, num_machines=1, opts=[], resume=False)
[10/09 11:42:04] detectron2 INFO: Contents of args.config_file=configs/coco/panoptic-segmentation/fcclip/r50_exp_1000_a_decoder.yaml:
_BASE_: ./fcclip_convnext_large_eval_ade20k_r50.yaml

INPUT:
  MIN_SIZE_TEST: 1024
  MAX_SIZE_TEST: 2560

MODEL:
  SEM_SEG_HEAD:
    NUM_CLASSES: 19
  WEIGHTS: /tsi/hi-paris/GB/segmentation/models/fcclip_cocopan_r50.pth
OUTPUT_DIR: /tsi/hi-paris/GB/segmentation/results/In_vocab/r50_008_1000_19_a_decoder
DATASETS:
  TRAIN: ("openvocab_cityscapes_fine_panoptic_train",)
  TEST: ("openvocab_cityscapes_fine_panoptic_val",)
SOLVER:
  IMS_PER_BATCH: 8
  MAX_ITER: 1000
TEST:
  EVAL_PERIOD: 1000


[10/09 11:42:04] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - openvocab_cityscapes_fine_panoptic_val
  TRAIN:
  - openvocab_cityscapes_fine_panoptic_train
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: false
  CROP:
    ENABLED: false
    MINIMUM_INST_AREA: 1
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  DATASET_MAPPER_NAME: coco_panoptic_lsj
  FORMAT: RGB
  IMAGE_SIZE: 1024
  MASK_FORMAT: polygon
  MAX_SCALE: 2.0
  MAX_SIZE_TEST: 2560
  MAX_SIZE_TRAIN: 1333
  MIN_SCALE: 0.1
  MIN_SIZE_TEST: 1024
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: -1
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 0
    NAME: CLIP
  DEVICE: cuda
  FC_CLIP:
    CLIP_MODEL_NAME: RN50
    CLIP_PRETRAINED_WEIGHTS: openai
    EMBED_DIM: 1024
    ENSEMBLE_ON_VALID_MASK: true
    GEOMETRIC_ENSEMBLE_ALPHA: 0.4
    GEOMETRIC_ENSEMBLE_BETA: 0.8
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_FORMER:
    CLASS_WEIGHT: 2.0
    DEC_LAYERS: 10
    DEEP_SUPERVISION: true
    DICE_WEIGHT: 5.0
    DIM_FEEDFORWARD: 2048
    DROPOUT: 0.0
    ENC_LAYERS: 0
    ENFORCE_INPUT_PROJ: false
    HIDDEN_DIM: 256
    IMPORTANCE_SAMPLE_RATIO: 0.75
    MASK_WEIGHT: 5.0
    NHEADS: 8
    NO_OBJECT_WEIGHT: 0.1
    NUM_OBJECT_QUERIES: 250
    OVERSAMPLE_RATIO: 3.0
    PRE_NORM: false
    SIZE_DIVISIBILITY: 32
    TEST:
      INSTANCE_ON: true
      OBJECT_MASK_THRESHOLD: 0.0
      OVERLAP_THRESHOLD: 0.8
      PANOPTIC_ON: true
      SEMANTIC_ON: true
      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: false
    TRAIN_NUM_POINTS: 12544
    TRANSFORMER_DECODER_NAME: MultiScaleMaskedTransformerDecoder
    TRANSFORMER_IN_FEATURE: multi_scale_pixel_decoder
  MASK_ON: false
  META_ARCHITECTURE: FCCLIP
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 122.7709383
  - 116.7460125
  - 104.09373615
  PIXEL_STD:
  - 68.5005327
  - 66.6321579
  - 70.32316305
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 1
    - 1
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: basic
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 256
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES:
    - res3
    - res4
    - res5
    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8
    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    MASK_DIM: 256
    NAME: FCCLIPHead
    NORM: GN
    NUM_CLASSES: 19
    PIXEL_DECODER_NAME: MSDeformAttnPixelDecoder
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    TRANSFORMER_ENC_LAYERS: 6
    USE_DEPTHWISE_SEPARABLE_CONV: false
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    USE_CHECKPOINT: false
    WINDOW_SIZE: 7
  WEIGHTS: /tsi/hi-paris/GB/segmentation/models/fcclip_cocopan_r50.pth
OUTPUT_DIR: /tsi/hi-paris/GB/segmentation/results/In_vocab/r50_008_1000_19_a_decoder
SEED: -1
SOLVER:
  AMP:
    ENABLED: true
  BACKBONE_MULTIPLIER: 0.1
  BASE_LR: 0.0001
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 5000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.01
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 8
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 1000
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 327778
  - 355092
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 1000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[10/09 11:42:04] detectron2 INFO: Full config saved to /tsi/hi-paris/GB/segmentation/results/In_vocab/r50_008_1000_19_a_decoder/config.yaml
[10/09 11:42:04] d2.utils.env INFO: Using a generated random seed 6126473
[10/09 11:42:08] d2.engine.defaults INFO: Model:
FCCLIP(
  (backbone): CLIP(
    (clip_model): CLIP(
      (visual): ModifiedResNet(
        (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
        (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        (layer1): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
            (downsample): Sequential(
              (-1): AvgPool2d(kernel_size=1, stride=1, padding=0)
              (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
        )
        (layer2): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
            (downsample): Sequential(
              (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (3): Bottleneck(
            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
        )
        (layer3): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
            (downsample): Sequential(
              (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
              (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (3): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (4): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (5): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
        )
        (layer4): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
            (downsample): Sequential(
              (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
              (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
        )
        (attnpool): AttentionPool2d(
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (c_proj): Linear(in_features=2048, out_features=1024, bias=True)
        )
      )
      (transformer): Transformer(
        (resblocks): ModuleList(
          (0-11): 12 x ResidualAttentionBlock(
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ls_2): Identity()
          )
        )
      )
      (token_embedding): Embedding(49408, 512)
      (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
  )
  (sem_seg_head): FCCLIPHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0-8): 9 x SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0-8): 9 x CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0-8): 9 x FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(250, 256)
      (query_embed): Embedding(250, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0-2): 3 x Sequential()
      )
      (mask_embed): MLP(
        (layers): ModuleList(
          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (mask_pooling): MaskPooling()
      (_mask_pooling_proj): Sequential(
        (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
      )
      (class_embed): MLP(
        (layers): ModuleList(
          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=1024, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 19
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
  (conv1): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv2): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU()
  (mask_pooling): MaskPooling()
  (decoder_adapter): DecoderAdapter(
    (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
    (conv2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    (norm1): LayerNorm((64, 1, 1), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((256, 1, 1), eps=1e-05, elementwise_affine=True)
    (relu): ReLU()
  )
  (void_embedding): Embedding(1, 1024)
)
[10/09 11:42:08] fcclip.data.dataset_mappers.mask_former_semantic_dataset_mapper INFO: [MaskFormerPanopticDatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(800,), max_size=1333, sample_style='choice'), RandomFlip()]
[10/09 11:42:09] fcclip.data.datasets.register_cityscapes_panoptic INFO: 18 cities found in 'datasets/cityscapes/leftImg8bit/train'.
[10/09 11:42:09] d2.data.build INFO: Using training sampler TrainingSampler
[10/09 11:42:10] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/09 11:42:10] d2.data.common INFO: Serializing 2975 elements to byte tensors and concatenating them all ...
[10/09 11:42:10] d2.data.common INFO: Serialized dataset takes 4.12 MiB
[10/09 11:42:10] d2.data.build INFO: Making batched data loader with batch_size=4
[10/09 11:42:10] d2.solver.build WARNING: SOLVER.STEPS contains values larger than SOLVER.MAX_ITER. These values will be ignored.
[10/09 11:42:10] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /tsi/hi-paris/GB/segmentation/models/fcclip_cocopan_r50.pth ...
[10/09 11:42:10] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /tsi/hi-paris/GB/segmentation/models/fcclip_cocopan_r50.pth ...
[10/09 11:42:13] fvcore.common.checkpoint WARNING: Skip loading parameter 'criterion.empty_weight' to the model due to incompatible shapes: (134,) in the checkpoint but (20,) in the model! You might want to double check if this is expected.
[10/09 11:42:13] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.clip_model.ln_final.{bias, weight}[0m
[34mbackbone.clip_model.token_embedding.weight[0m
[34mbackbone.clip_model.transformer.resblocks.0.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.attnpool.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.attnpool.k_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.attnpool.positional_embedding[0m
[34mbackbone.clip_model.visual.attnpool.q_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.attnpool.v_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.conv1.weight[0m
[34mbackbone.clip_model.visual.conv2.weight[0m
[34mbackbone.clip_model.visual.conv3.weight[0m
[34mbackbone.clip_model.visual.layer1.0.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.0.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.0.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.0.conv1.weight[0m
[34mbackbone.clip_model.visual.layer1.0.conv2.weight[0m
[34mbackbone.clip_model.visual.layer1.0.conv3.weight[0m
[34mbackbone.clip_model.visual.layer1.0.downsample.0.weight[0m
[34mbackbone.clip_model.visual.layer1.0.downsample.1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.1.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.1.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.1.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.1.conv1.weight[0m
[34mbackbone.clip_model.visual.layer1.1.conv2.weight[0m
[34mbackbone.clip_model.visual.layer1.1.conv3.weight[0m
[34mbackbone.clip_model.visual.layer1.2.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.2.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.2.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.2.conv1.weight[0m
[34mbackbone.clip_model.visual.layer1.2.conv2.weight[0m
[34mbackbone.clip_model.visual.layer1.2.conv3.weight[0m
[34mbackbone.clip_model.visual.layer2.0.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.0.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.0.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.0.conv1.weight[0m
[34mbackbone.clip_model.visual.layer2.0.conv2.weight[0m
[34mbackbone.clip_model.visual.layer2.0.conv3.weight[0m
[34mbackbone.clip_model.visual.layer2.0.downsample.0.weight[0m
[34mbackbone.clip_model.visual.layer2.0.downsample.1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.1.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.1.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.1.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.1.conv1.weight[0m
[34mbackbone.clip_model.visual.layer2.1.conv2.weight[0m
[34mbackbone.clip_model.visual.layer2.1.conv3.weight[0m
[34mbackbone.clip_model.visual.layer2.2.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.2.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.2.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.2.conv1.weight[0m
[34mbackbone.clip_model.visual.layer2.2.conv2.weight[0m
[34mbackbone.clip_model.visual.layer2.2.conv3.weight[0m
[34mbackbone.clip_model.visual.layer2.3.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.3.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.3.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.3.conv1.weight[0m
[34mbackbone.clip_model.visual.layer2.3.conv2.weight[0m
[34mbackbone.clip_model.visual.layer2.3.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.0.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.0.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.0.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.0.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.0.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.0.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.0.downsample.0.weight[0m
[34mbackbone.clip_model.visual.layer3.0.downsample.1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.1.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.1.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.1.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.1.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.1.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.1.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.2.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.2.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.2.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.2.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.2.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.2.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.3.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.3.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.3.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.3.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.3.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.3.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.4.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.4.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.4.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.4.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.4.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.4.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.5.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.5.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.5.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.5.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.5.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.5.conv3.weight[0m
[34mbackbone.clip_model.visual.layer4.0.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.0.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.0.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.0.conv1.weight[0m
[34mbackbone.clip_model.visual.layer4.0.conv2.weight[0m
[34mbackbone.clip_model.visual.layer4.0.conv3.weight[0m
[34mbackbone.clip_model.visual.layer4.0.downsample.0.weight[0m
[34mbackbone.clip_model.visual.layer4.0.downsample.1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.1.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.1.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.1.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.1.conv1.weight[0m
[34mbackbone.clip_model.visual.layer4.1.conv2.weight[0m
[34mbackbone.clip_model.visual.layer4.1.conv3.weight[0m
[34mbackbone.clip_model.visual.layer4.2.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.2.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.2.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.2.conv1.weight[0m
[34mbackbone.clip_model.visual.layer4.2.conv2.weight[0m
[34mbackbone.clip_model.visual.layer4.2.conv3.weight[0m
[34mbackbone.clip_model.{logit_scale, positional_embedding, text_projection}[0m
[34mbn1.{bias, running_mean, running_var, weight}[0m
[34mbn2.{bias, running_mean, running_var, weight}[0m
[34mconv1.{bias, weight}[0m
[34mconv2.{bias, weight}[0m
[34mcriterion.empty_weight[0m
[34mdecoder_adapter.conv1.{bias, weight}[0m
[34mdecoder_adapter.conv2.{bias, weight}[0m
[34mdecoder_adapter.norm1.{bias, weight}[0m
[34mdecoder_adapter.norm2.{bias, weight}[0m
[10/09 11:42:13] d2.engine.train_loop INFO: Starting training from iteration 0
[10/09 11:42:22] d2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/home/ids/gbrison/segmentation/segmentation/detectron2/detectron2/engine/train_loop.py", line 155, in train
    self.run_step()
  File "/home/ids/gbrison/segmentation/segmentation/detectron2/detectron2/engine/defaults.py", line 498, in run_step
    self._trainer.run_step()
  File "/home/ids/gbrison/segmentation/segmentation/detectron2/detectron2/engine/train_loop.py", line 494, in run_step
    loss_dict = self.model(data)
  File "/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1632, in forward
    inputs, kwargs = self._pre_forward(*inputs, **kwargs)
  File "/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1523, in _pre_forward
    if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameter indices which did not receive grad for rank 0: 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316
 In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[10/09 11:42:22] d2.engine.hooks INFO: Total training time: 0:00:00 (0:00:00 on hooks)
[10/09 11:42:22] d2.utils.events INFO:  iter: 1  total_loss: 43.03  loss_ce: 2.114  loss_mask: 0.4575  loss_dice: 1.567  loss_ce_0: 2.413  loss_mask_0: 0.4376  loss_dice_0: 1.901  loss_ce_1: 2.347  loss_mask_1: 0.4518  loss_dice_1: 1.743  loss_ce_2: 2.331  loss_mask_2: 0.4328  loss_dice_2: 1.627  loss_ce_3: 2.268  loss_mask_3: 0.437  loss_dice_3: 1.594  loss_ce_4: 2.143  loss_mask_4: 0.4596  loss_dice_4: 1.616  loss_ce_5: 2.24  loss_mask_5: 0.4568  loss_dice_5: 1.533  loss_ce_6: 2.161  loss_mask_6: 0.4501  loss_dice_6: 1.583  loss_ce_7: 2.094  loss_mask_7: 0.4576  loss_dice_7: 1.547  loss_ce_8: 2.159  loss_mask_8: 0.4626  loss_dice_8: 1.551    data_time: 5.5306  last_data_time: 5.5306   lr: 0.0001  max_mem: 15719M
[10/09 11:44:41] detectron2 INFO: Rank of current process: 0. World size: 1
[10/09 11:44:42] detectron2 INFO: Environment info:
-------------------------------  -------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.8.19 | packaged by conda-forge | (default, Mar 20 2024, 12:47:35) [GCC 12.3.0]
numpy                            1.24.4
detectron2                       0.6 @/home/ids/gbrison/segmentation/segmentation/detectron2/detectron2
Compiler                         GCC 11.4
CUDA compiler                    CUDA 12.5
detectron2 arch flags            8.9
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.0+cu121 @/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0,1                          NVIDIA L40S (arch=8.9)
Driver version                   555.42.06
CUDA_HOME                        /usr/local/cuda
Pillow                           9.4.0
torchvision                      0.19.0+cu121 @/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torchvision
torchvision arch flags           5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.10.0
-------------------------------  -------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX512
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.0, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/09 11:44:42] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco/panoptic-segmentation/fcclip/r50_exp_1000_a_decoder.yaml', dist_url='tcp://127.0.0.1:51163', eval_only=False, machine_rank=0, num_gpus=1, num_machines=1, opts=[], resume=False)
[10/09 11:44:42] detectron2 INFO: Contents of args.config_file=configs/coco/panoptic-segmentation/fcclip/r50_exp_1000_a_decoder.yaml:
_BASE_: ./fcclip_convnext_large_eval_ade20k_r50.yaml

INPUT:
  MIN_SIZE_TEST: 1024
  MAX_SIZE_TEST: 2560

MODEL:
  SEM_SEG_HEAD:
    NUM_CLASSES: 19
  WEIGHTS: /tsi/hi-paris/GB/segmentation/models/fcclip_cocopan_r50.pth
OUTPUT_DIR: /tsi/hi-paris/GB/segmentation/results/In_vocab/r50_008_1000_19_a_decoder
DATASETS:
  TRAIN: ("openvocab_cityscapes_fine_panoptic_train",)
  TEST: ("openvocab_cityscapes_fine_panoptic_val",)
SOLVER:
  IMS_PER_BATCH: 8
  MAX_ITER: 1000
TEST:
  EVAL_PERIOD: 1000


[10/09 11:44:42] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - openvocab_cityscapes_fine_panoptic_val
  TRAIN:
  - openvocab_cityscapes_fine_panoptic_train
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: false
  CROP:
    ENABLED: false
    MINIMUM_INST_AREA: 1
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  DATASET_MAPPER_NAME: coco_panoptic_lsj
  FORMAT: RGB
  IMAGE_SIZE: 1024
  MASK_FORMAT: polygon
  MAX_SCALE: 2.0
  MAX_SIZE_TEST: 2560
  MAX_SIZE_TRAIN: 1333
  MIN_SCALE: 0.1
  MIN_SIZE_TEST: 1024
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: -1
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 0
    NAME: CLIP
  DEVICE: cuda
  FC_CLIP:
    CLIP_MODEL_NAME: RN50
    CLIP_PRETRAINED_WEIGHTS: openai
    EMBED_DIM: 1024
    ENSEMBLE_ON_VALID_MASK: true
    GEOMETRIC_ENSEMBLE_ALPHA: 0.4
    GEOMETRIC_ENSEMBLE_BETA: 0.8
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_FORMER:
    CLASS_WEIGHT: 2.0
    DEC_LAYERS: 10
    DEEP_SUPERVISION: true
    DICE_WEIGHT: 5.0
    DIM_FEEDFORWARD: 2048
    DROPOUT: 0.0
    ENC_LAYERS: 0
    ENFORCE_INPUT_PROJ: false
    HIDDEN_DIM: 256
    IMPORTANCE_SAMPLE_RATIO: 0.75
    MASK_WEIGHT: 5.0
    NHEADS: 8
    NO_OBJECT_WEIGHT: 0.1
    NUM_OBJECT_QUERIES: 250
    OVERSAMPLE_RATIO: 3.0
    PRE_NORM: false
    SIZE_DIVISIBILITY: 32
    TEST:
      INSTANCE_ON: true
      OBJECT_MASK_THRESHOLD: 0.0
      OVERLAP_THRESHOLD: 0.8
      PANOPTIC_ON: true
      SEMANTIC_ON: true
      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: false
    TRAIN_NUM_POINTS: 12544
    TRANSFORMER_DECODER_NAME: MultiScaleMaskedTransformerDecoder
    TRANSFORMER_IN_FEATURE: multi_scale_pixel_decoder
  MASK_ON: false
  META_ARCHITECTURE: FCCLIP
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 122.7709383
  - 116.7460125
  - 104.09373615
  PIXEL_STD:
  - 68.5005327
  - 66.6321579
  - 70.32316305
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 1
    - 1
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: basic
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 256
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES:
    - res3
    - res4
    - res5
    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8
    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    MASK_DIM: 256
    NAME: FCCLIPHead
    NORM: GN
    NUM_CLASSES: 19
    PIXEL_DECODER_NAME: MSDeformAttnPixelDecoder
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    TRANSFORMER_ENC_LAYERS: 6
    USE_DEPTHWISE_SEPARABLE_CONV: false
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    USE_CHECKPOINT: false
    WINDOW_SIZE: 7
  WEIGHTS: /tsi/hi-paris/GB/segmentation/models/fcclip_cocopan_r50.pth
OUTPUT_DIR: /tsi/hi-paris/GB/segmentation/results/In_vocab/r50_008_1000_19_a_decoder
SEED: -1
SOLVER:
  AMP:
    ENABLED: true
  BACKBONE_MULTIPLIER: 0.1
  BASE_LR: 0.0001
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 5000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.01
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 8
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 1000
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 327778
  - 355092
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 1000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[10/09 11:44:42] detectron2 INFO: Full config saved to /tsi/hi-paris/GB/segmentation/results/In_vocab/r50_008_1000_19_a_decoder/config.yaml
[10/09 11:44:42] d2.utils.env INFO: Using a generated random seed 43925533
[10/09 11:44:46] d2.engine.defaults INFO: Model:
FCCLIP(
  (backbone): CLIP(
    (clip_model): CLIP(
      (visual): ModifiedResNet(
        (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
        (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        (layer1): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
            (downsample): Sequential(
              (-1): AvgPool2d(kernel_size=1, stride=1, padding=0)
              (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
        )
        (layer2): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
            (downsample): Sequential(
              (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (3): Bottleneck(
            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
        )
        (layer3): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
            (downsample): Sequential(
              (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
              (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (3): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (4): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (5): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
        )
        (layer4): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
            (downsample): Sequential(
              (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
              (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
        )
        (attnpool): AttentionPool2d(
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (c_proj): Linear(in_features=2048, out_features=1024, bias=True)
        )
      )
      (transformer): Transformer(
        (resblocks): ModuleList(
          (0-11): 12 x ResidualAttentionBlock(
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ls_2): Identity()
          )
        )
      )
      (token_embedding): Embedding(49408, 512)
      (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
  )
  (sem_seg_head): FCCLIPHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0-8): 9 x SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0-8): 9 x CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0-8): 9 x FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(250, 256)
      (query_embed): Embedding(250, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0-2): 3 x Sequential()
      )
      (mask_embed): MLP(
        (layers): ModuleList(
          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (mask_pooling): MaskPooling()
      (_mask_pooling_proj): Sequential(
        (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
      )
      (class_embed): MLP(
        (layers): ModuleList(
          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=1024, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 19
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
  (conv1): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv2): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU()
  (mask_pooling): MaskPooling()
  (decoder_adapter): DecoderAdapter(
    (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
    (conv2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    (norm1): LayerNorm((64, 1, 1), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((256, 1, 1), eps=1e-05, elementwise_affine=True)
    (relu): ReLU()
  )
  (void_embedding): Embedding(1, 1024)
)
[10/09 11:44:46] fcclip.data.dataset_mappers.mask_former_semantic_dataset_mapper INFO: [MaskFormerPanopticDatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(800,), max_size=1333, sample_style='choice'), RandomFlip()]
[10/09 11:44:47] fcclip.data.datasets.register_cityscapes_panoptic INFO: 18 cities found in 'datasets/cityscapes/leftImg8bit/train'.
[10/09 11:44:47] d2.data.build INFO: Using training sampler TrainingSampler
[10/09 11:44:47] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/09 11:44:47] d2.data.common INFO: Serializing 2975 elements to byte tensors and concatenating them all ...
[10/09 11:44:47] d2.data.common INFO: Serialized dataset takes 4.12 MiB
[10/09 11:44:47] d2.data.build INFO: Making batched data loader with batch_size=8
[10/09 11:44:47] d2.solver.build WARNING: SOLVER.STEPS contains values larger than SOLVER.MAX_ITER. These values will be ignored.
[10/09 11:44:47] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /tsi/hi-paris/GB/segmentation/models/fcclip_cocopan_r50.pth ...
[10/09 11:44:47] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /tsi/hi-paris/GB/segmentation/models/fcclip_cocopan_r50.pth ...
[10/09 11:44:47] fvcore.common.checkpoint WARNING: Skip loading parameter 'criterion.empty_weight' to the model due to incompatible shapes: (134,) in the checkpoint but (20,) in the model! You might want to double check if this is expected.
[10/09 11:44:47] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.clip_model.ln_final.{bias, weight}[0m
[34mbackbone.clip_model.token_embedding.weight[0m
[34mbackbone.clip_model.transformer.resblocks.0.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.attnpool.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.attnpool.k_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.attnpool.positional_embedding[0m
[34mbackbone.clip_model.visual.attnpool.q_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.attnpool.v_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.conv1.weight[0m
[34mbackbone.clip_model.visual.conv2.weight[0m
[34mbackbone.clip_model.visual.conv3.weight[0m
[34mbackbone.clip_model.visual.layer1.0.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.0.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.0.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.0.conv1.weight[0m
[34mbackbone.clip_model.visual.layer1.0.conv2.weight[0m
[34mbackbone.clip_model.visual.layer1.0.conv3.weight[0m
[34mbackbone.clip_model.visual.layer1.0.downsample.0.weight[0m
[34mbackbone.clip_model.visual.layer1.0.downsample.1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.1.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.1.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.1.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.1.conv1.weight[0m
[34mbackbone.clip_model.visual.layer1.1.conv2.weight[0m
[34mbackbone.clip_model.visual.layer1.1.conv3.weight[0m
[34mbackbone.clip_model.visual.layer1.2.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.2.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.2.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.2.conv1.weight[0m
[34mbackbone.clip_model.visual.layer1.2.conv2.weight[0m
[34mbackbone.clip_model.visual.layer1.2.conv3.weight[0m
[34mbackbone.clip_model.visual.layer2.0.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.0.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.0.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.0.conv1.weight[0m
[34mbackbone.clip_model.visual.layer2.0.conv2.weight[0m
[34mbackbone.clip_model.visual.layer2.0.conv3.weight[0m
[34mbackbone.clip_model.visual.layer2.0.downsample.0.weight[0m
[34mbackbone.clip_model.visual.layer2.0.downsample.1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.1.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.1.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.1.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.1.conv1.weight[0m
[34mbackbone.clip_model.visual.layer2.1.conv2.weight[0m
[34mbackbone.clip_model.visual.layer2.1.conv3.weight[0m
[34mbackbone.clip_model.visual.layer2.2.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.2.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.2.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.2.conv1.weight[0m
[34mbackbone.clip_model.visual.layer2.2.conv2.weight[0m
[34mbackbone.clip_model.visual.layer2.2.conv3.weight[0m
[34mbackbone.clip_model.visual.layer2.3.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.3.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.3.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.3.conv1.weight[0m
[34mbackbone.clip_model.visual.layer2.3.conv2.weight[0m
[34mbackbone.clip_model.visual.layer2.3.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.0.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.0.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.0.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.0.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.0.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.0.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.0.downsample.0.weight[0m
[34mbackbone.clip_model.visual.layer3.0.downsample.1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.1.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.1.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.1.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.1.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.1.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.1.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.2.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.2.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.2.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.2.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.2.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.2.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.3.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.3.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.3.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.3.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.3.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.3.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.4.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.4.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.4.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.4.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.4.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.4.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.5.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.5.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.5.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.5.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.5.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.5.conv3.weight[0m
[34mbackbone.clip_model.visual.layer4.0.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.0.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.0.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.0.conv1.weight[0m
[34mbackbone.clip_model.visual.layer4.0.conv2.weight[0m
[34mbackbone.clip_model.visual.layer4.0.conv3.weight[0m
[34mbackbone.clip_model.visual.layer4.0.downsample.0.weight[0m
[34mbackbone.clip_model.visual.layer4.0.downsample.1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.1.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.1.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.1.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.1.conv1.weight[0m
[34mbackbone.clip_model.visual.layer4.1.conv2.weight[0m
[34mbackbone.clip_model.visual.layer4.1.conv3.weight[0m
[34mbackbone.clip_model.visual.layer4.2.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.2.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.2.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.2.conv1.weight[0m
[34mbackbone.clip_model.visual.layer4.2.conv2.weight[0m
[34mbackbone.clip_model.visual.layer4.2.conv3.weight[0m
[34mbackbone.clip_model.{logit_scale, positional_embedding, text_projection}[0m
[34mbn1.{bias, running_mean, running_var, weight}[0m
[34mbn2.{bias, running_mean, running_var, weight}[0m
[34mconv1.{bias, weight}[0m
[34mconv2.{bias, weight}[0m
[34mcriterion.empty_weight[0m
[34mdecoder_adapter.conv1.{bias, weight}[0m
[34mdecoder_adapter.conv2.{bias, weight}[0m
[34mdecoder_adapter.norm1.{bias, weight}[0m
[34mdecoder_adapter.norm2.{bias, weight}[0m
[10/09 11:44:47] d2.engine.train_loop INFO: Starting training from iteration 0
[10/09 11:45:12] d2.utils.events INFO:  eta: 0:18:07  iter: 19  total_loss: 33.43  loss_ce: 1.292  loss_mask: 0.2764  loss_dice: 1.536  loss_ce_0: 1.502  loss_mask_0: 0.3063  loss_dice_0: 1.879  loss_ce_1: 1.546  loss_mask_1: 0.3066  loss_dice_1: 1.703  loss_ce_2: 1.464  loss_mask_2: 0.2943  loss_dice_2: 1.649  loss_ce_3: 1.387  loss_mask_3: 0.2809  loss_dice_3: 1.613  loss_ce_4: 1.359  loss_mask_4: 0.2873  loss_dice_4: 1.57  loss_ce_5: 1.293  loss_mask_5: 0.2801  loss_dice_5: 1.578  loss_ce_6: 1.337  loss_mask_6: 0.2792  loss_dice_6: 1.537  loss_ce_7: 1.344  loss_mask_7: 0.2777  loss_dice_7: 1.526  loss_ce_8: 1.309  loss_mask_8: 0.2748  loss_dice_8: 1.538    time: 1.1132  last_time: 1.1563  data_time: 0.1250  last_data_time: 0.0702   lr: 0.0001  max_mem: 32515M
[10/09 11:45:34] d2.utils.events INFO:  eta: 0:17:40  iter: 39  total_loss: 25.51  loss_ce: 0.776  loss_mask: 0.2299  loss_dice: 1.45  loss_ce_0: 0.8173  loss_mask_0: 0.2709  loss_dice_0: 1.805  loss_ce_1: 0.959  loss_mask_1: 0.2429  loss_dice_1: 1.611  loss_ce_2: 0.8787  loss_mask_2: 0.2402  loss_dice_2: 1.553  loss_ce_3: 0.8036  loss_mask_3: 0.228  loss_dice_3: 1.459  loss_ce_4: 0.807  loss_mask_4: 0.2309  loss_dice_4: 1.468  loss_ce_5: 0.8046  loss_mask_5: 0.2313  loss_dice_5: 1.458  loss_ce_6: 0.7759  loss_mask_6: 0.2344  loss_dice_6: 1.461  loss_ce_7: 0.7513  loss_mask_7: 0.233  loss_dice_7: 1.474  loss_ce_8: 0.8051  loss_mask_8: 0.2347  loss_dice_8: 1.451    time: 1.1076  last_time: 1.0840  data_time: 0.0743  last_data_time: 0.0801   lr: 0.0001  max_mem: 32515M
[10/09 11:45:56] d2.utils.events INFO:  eta: 0:17:16  iter: 59  total_loss: 24.54  loss_ce: 0.7608  loss_mask: 0.2231  loss_dice: 1.381  loss_ce_0: 0.8148  loss_mask_0: 0.2483  loss_dice_0: 1.679  loss_ce_1: 0.9111  loss_mask_1: 0.2351  loss_dice_1: 1.584  loss_ce_2: 0.8369  loss_mask_2: 0.2278  loss_dice_2: 1.471  loss_ce_3: 0.8069  loss_mask_3: 0.2254  loss_dice_3: 1.426  loss_ce_4: 0.8124  loss_mask_4: 0.2222  loss_dice_4: 1.423  loss_ce_5: 0.7812  loss_mask_5: 0.2244  loss_dice_5: 1.386  loss_ce_6: 0.7316  loss_mask_6: 0.2217  loss_dice_6: 1.35  loss_ce_7: 0.7268  loss_mask_7: 0.2241  loss_dice_7: 1.394  loss_ce_8: 0.7347  loss_mask_8: 0.2237  loss_dice_8: 1.371    time: 1.1033  last_time: 1.2102  data_time: 0.0731  last_data_time: 0.0649   lr: 0.0001  max_mem: 32515M
[10/09 11:46:18] d2.utils.events INFO:  eta: 0:16:50  iter: 79  total_loss: 22.94  loss_ce: 0.6569  loss_mask: 0.2181  loss_dice: 1.304  loss_ce_0: 0.7354  loss_mask_0: 0.2454  loss_dice_0: 1.576  loss_ce_1: 0.8451  loss_mask_1: 0.2284  loss_dice_1: 1.498  loss_ce_2: 0.7469  loss_mask_2: 0.2179  loss_dice_2: 1.406  loss_ce_3: 0.6979  loss_mask_3: 0.2159  loss_dice_3: 1.336  loss_ce_4: 0.6886  loss_mask_4: 0.2155  loss_dice_4: 1.346  loss_ce_5: 0.6547  loss_mask_5: 0.2149  loss_dice_5: 1.333  loss_ce_6: 0.6902  loss_mask_6: 0.2147  loss_dice_6: 1.316  loss_ce_7: 0.6737  loss_mask_7: 0.2191  loss_dice_7: 1.307  loss_ce_8: 0.6461  loss_mask_8: 0.2182  loss_dice_8: 1.285    time: 1.1004  last_time: 1.1182  data_time: 0.0708  last_data_time: 0.0780   lr: 0.0001  max_mem: 32515M
[10/09 11:46:40] d2.utils.events INFO:  eta: 0:16:24  iter: 99  total_loss: 22.49  loss_ce: 0.6196  loss_mask: 0.1972  loss_dice: 1.321  loss_ce_0: 0.74  loss_mask_0: 0.2306  loss_dice_0: 1.572  loss_ce_1: 0.8206  loss_mask_1: 0.2136  loss_dice_1: 1.44  loss_ce_2: 0.7318  loss_mask_2: 0.2058  loss_dice_2: 1.392  loss_ce_3: 0.6585  loss_mask_3: 0.2009  loss_dice_3: 1.325  loss_ce_4: 0.6435  loss_mask_4: 0.1987  loss_dice_4: 1.335  loss_ce_5: 0.6326  loss_mask_5: 0.1995  loss_dice_5: 1.328  loss_ce_6: 0.6152  loss_mask_6: 0.1973  loss_dice_6: 1.291  loss_ce_7: 0.6197  loss_mask_7: 0.1983  loss_dice_7: 1.305  loss_ce_8: 0.6171  loss_mask_8: 0.1993  loss_dice_8: 1.298    time: 1.0951  last_time: 1.0633  data_time: 0.0690  last_data_time: 0.0554   lr: 0.0001  max_mem: 32515M
[10/09 11:47:02] d2.utils.events INFO:  eta: 0:16:03  iter: 119  total_loss: 22.58  loss_ce: 0.5624  loss_mask: 0.1809  loss_dice: 1.339  loss_ce_0: 0.658  loss_mask_0: 0.217  loss_dice_0: 1.649  loss_ce_1: 0.7648  loss_mask_1: 0.2008  loss_dice_1: 1.509  loss_ce_2: 0.694  loss_mask_2: 0.1869  loss_dice_2: 1.459  loss_ce_3: 0.6213  loss_mask_3: 0.1885  loss_dice_3: 1.396  loss_ce_4: 0.5799  loss_mask_4: 0.1877  loss_dice_4: 1.406  loss_ce_5: 0.5828  loss_mask_5: 0.1844  loss_dice_5: 1.408  loss_ce_6: 0.5694  loss_mask_6: 0.1839  loss_dice_6: 1.368  loss_ce_7: 0.5704  loss_mask_7: 0.1831  loss_dice_7: 1.366  loss_ce_8: 0.5417  loss_mask_8: 0.179  loss_dice_8: 1.349    time: 1.0957  last_time: 1.0895  data_time: 0.0779  last_data_time: 0.0935   lr: 0.0001  max_mem: 32515M
[10/09 11:47:23] d2.utils.events INFO:  eta: 0:15:37  iter: 139  total_loss: 21.8  loss_ce: 0.5688  loss_mask: 0.2008  loss_dice: 1.314  loss_ce_0: 0.6633  loss_mask_0: 0.2424  loss_dice_0: 1.572  loss_ce_1: 0.7418  loss_mask_1: 0.213  loss_dice_1: 1.445  loss_ce_2: 0.6767  loss_mask_2: 0.2115  loss_dice_2: 1.42  loss_ce_3: 0.6335  loss_mask_3: 0.2046  loss_dice_3: 1.362  loss_ce_4: 0.6167  loss_mask_4: 0.2043  loss_dice_4: 1.294  loss_ce_5: 0.593  loss_mask_5: 0.2044  loss_dice_5: 1.336  loss_ce_6: 0.5837  loss_mask_6: 0.2051  loss_dice_6: 1.319  loss_ce_7: 0.5917  loss_mask_7: 0.2061  loss_dice_7: 1.333  loss_ce_8: 0.6148  loss_mask_8: 0.2007  loss_dice_8: 1.314    time: 1.0915  last_time: 1.0777  data_time: 0.0715  last_data_time: 0.0677   lr: 0.0001  max_mem: 32515M
[10/09 11:47:45] d2.utils.events INFO:  eta: 0:15:15  iter: 159  total_loss: 22.79  loss_ce: 0.5987  loss_mask: 0.1949  loss_dice: 1.315  loss_ce_0: 0.6447  loss_mask_0: 0.2221  loss_dice_0: 1.656  loss_ce_1: 0.7618  loss_mask_1: 0.2012  loss_dice_1: 1.5  loss_ce_2: 0.6877  loss_mask_2: 0.1968  loss_dice_2: 1.448  loss_ce_3: 0.6487  loss_mask_3: 0.1952  loss_dice_3: 1.383  loss_ce_4: 0.6041  loss_mask_4: 0.1945  loss_dice_4: 1.408  loss_ce_5: 0.6162  loss_mask_5: 0.1949  loss_dice_5: 1.402  loss_ce_6: 0.6069  loss_mask_6: 0.194  loss_dice_6: 1.373  loss_ce_7: 0.6159  loss_mask_7: 0.1959  loss_dice_7: 1.37  loss_ce_8: 0.6192  loss_mask_8: 0.1943  loss_dice_8: 1.329    time: 1.0923  last_time: 1.0653  data_time: 0.0764  last_data_time: 0.0711   lr: 0.0001  max_mem: 32515M
[10/09 11:48:08] d2.utils.events INFO:  eta: 0:14:56  iter: 179  total_loss: 22.92  loss_ce: 0.6349  loss_mask: 0.1853  loss_dice: 1.358  loss_ce_0: 0.6853  loss_mask_0: 0.2077  loss_dice_0: 1.595  loss_ce_1: 0.7617  loss_mask_1: 0.2045  loss_dice_1: 1.489  loss_ce_2: 0.7058  loss_mask_2: 0.1994  loss_dice_2: 1.439  loss_ce_3: 0.6462  loss_mask_3: 0.1873  loss_dice_3: 1.431  loss_ce_4: 0.6473  loss_mask_4: 0.1849  loss_dice_4: 1.372  loss_ce_5: 0.6316  loss_mask_5: 0.1834  loss_dice_5: 1.416  loss_ce_6: 0.6153  loss_mask_6: 0.1805  loss_dice_6: 1.364  loss_ce_7: 0.6049  loss_mask_7: 0.1839  loss_dice_7: 1.393  loss_ce_8: 0.6245  loss_mask_8: 0.1829  loss_dice_8: 1.39    time: 1.0943  last_time: 1.1420  data_time: 0.0795  last_data_time: 0.0805   lr: 0.0001  max_mem: 32515M
[10/09 11:48:29] d2.utils.events INFO:  eta: 0:14:32  iter: 199  total_loss: 21.24  loss_ce: 0.5579  loss_mask: 0.1883  loss_dice: 1.243  loss_ce_0: 0.6233  loss_mask_0: 0.2239  loss_dice_0: 1.502  loss_ce_1: 0.7196  loss_mask_1: 0.2079  loss_dice_1: 1.371  loss_ce_2: 0.6445  loss_mask_2: 0.1953  loss_dice_2: 1.324  loss_ce_3: 0.578  loss_mask_3: 0.1887  loss_dice_3: 1.276  loss_ce_4: 0.5682  loss_mask_4: 0.193  loss_dice_4: 1.299  loss_ce_5: 0.5602  loss_mask_5: 0.1886  loss_dice_5: 1.266  loss_ce_6: 0.5307  loss_mask_6: 0.1905  loss_dice_6: 1.237  loss_ce_7: 0.5428  loss_mask_7: 0.1897  loss_dice_7: 1.258  loss_ce_8: 0.5799  loss_mask_8: 0.189  loss_dice_8: 1.222    time: 1.0914  last_time: 1.0366  data_time: 0.0715  last_data_time: 0.0591   lr: 0.0001  max_mem: 32515M
[10/09 11:48:51] d2.utils.events INFO:  eta: 0:14:11  iter: 219  total_loss: 22.77  loss_ce: 0.5799  loss_mask: 0.1776  loss_dice: 1.381  loss_ce_0: 0.6716  loss_mask_0: 0.2079  loss_dice_0: 1.588  loss_ce_1: 0.7723  loss_mask_1: 0.1917  loss_dice_1: 1.535  loss_ce_2: 0.6719  loss_mask_2: 0.181  loss_dice_2: 1.448  loss_ce_3: 0.6054  loss_mask_3: 0.18  loss_dice_3: 1.422  loss_ce_4: 0.6491  loss_mask_4: 0.176  loss_dice_4: 1.389  loss_ce_5: 0.595  loss_mask_5: 0.1748  loss_dice_5: 1.422  loss_ce_6: 0.5698  loss_mask_6: 0.175  loss_dice_6: 1.359  loss_ce_7: 0.5793  loss_mask_7: 0.1762  loss_dice_7: 1.408  loss_ce_8: 0.5907  loss_mask_8: 0.1759  loss_dice_8: 1.389    time: 1.0932  last_time: 1.1946  data_time: 0.0786  last_data_time: 0.0774   lr: 0.0001  max_mem: 32515M
[10/09 11:49:13] d2.utils.events INFO:  eta: 0:13:50  iter: 239  total_loss: 21.61  loss_ce: 0.5954  loss_mask: 0.1861  loss_dice: 1.244  loss_ce_0: 0.6654  loss_mask_0: 0.2137  loss_dice_0: 1.501  loss_ce_1: 0.7561  loss_mask_1: 0.2054  loss_dice_1: 1.41  loss_ce_2: 0.6707  loss_mask_2: 0.191  loss_dice_2: 1.31  loss_ce_3: 0.6454  loss_mask_3: 0.1881  loss_dice_3: 1.303  loss_ce_4: 0.6301  loss_mask_4: 0.1872  loss_dice_4: 1.279  loss_ce_5: 0.6116  loss_mask_5: 0.1877  loss_dice_5: 1.262  loss_ce_6: 0.6152  loss_mask_6: 0.1862  loss_dice_6: 1.245  loss_ce_7: 0.5997  loss_mask_7: 0.1866  loss_dice_7: 1.24  loss_ce_8: 0.5928  loss_mask_8: 0.1868  loss_dice_8: 1.261    time: 1.0937  last_time: 1.0425  data_time: 0.0776  last_data_time: 0.0703   lr: 0.0001  max_mem: 32515M
[10/09 11:49:35] d2.utils.events INFO:  eta: 0:13:28  iter: 259  total_loss: 21.31  loss_ce: 0.509  loss_mask: 0.1873  loss_dice: 1.245  loss_ce_0: 0.6133  loss_mask_0: 0.2134  loss_dice_0: 1.505  loss_ce_1: 0.6694  loss_mask_1: 0.2014  loss_dice_1: 1.413  loss_ce_2: 0.6263  loss_mask_2: 0.1868  loss_dice_2: 1.405  loss_ce_3: 0.5586  loss_mask_3: 0.1893  loss_dice_3: 1.333  loss_ce_4: 0.5104  loss_mask_4: 0.1863  loss_dice_4: 1.31  loss_ce_5: 0.5279  loss_mask_5: 0.1881  loss_dice_5: 1.239  loss_ce_6: 0.5176  loss_mask_6: 0.1875  loss_dice_6: 1.272  loss_ce_7: 0.5067  loss_mask_7: 0.1859  loss_dice_7: 1.325  loss_ce_8: 0.5272  loss_mask_8: 0.1874  loss_dice_8: 1.27    time: 1.0940  last_time: 1.0620  data_time: 0.0756  last_data_time: 0.0809   lr: 0.0001  max_mem: 32515M
[10/09 11:49:57] d2.utils.events INFO:  eta: 0:13:05  iter: 279  total_loss: 20.36  loss_ce: 0.4999  loss_mask: 0.1937  loss_dice: 1.246  loss_ce_0: 0.5896  loss_mask_0: 0.2157  loss_dice_0: 1.499  loss_ce_1: 0.6523  loss_mask_1: 0.2073  loss_dice_1: 1.384  loss_ce_2: 0.6251  loss_mask_2: 0.1982  loss_dice_2: 1.298  loss_ce_3: 0.5695  loss_mask_3: 0.1978  loss_dice_3: 1.264  loss_ce_4: 0.5318  loss_mask_4: 0.1941  loss_dice_4: 1.261  loss_ce_5: 0.525  loss_mask_5: 0.1992  loss_dice_5: 1.246  loss_ce_6: 0.5131  loss_mask_6: 0.1961  loss_dice_6: 1.222  loss_ce_7: 0.5124  loss_mask_7: 0.1985  loss_dice_7: 1.226  loss_ce_8: 0.5411  loss_mask_8: 0.1945  loss_dice_8: 1.211    time: 1.0929  last_time: 1.1450  data_time: 0.0753  last_data_time: 0.0818   lr: 0.0001  max_mem: 32515M
[10/09 11:50:18] d2.utils.events INFO:  eta: 0:12:42  iter: 299  total_loss: 21.2  loss_ce: 0.5289  loss_mask: 0.1878  loss_dice: 1.28  loss_ce_0: 0.5922  loss_mask_0: 0.2197  loss_dice_0: 1.488  loss_ce_1: 0.6842  loss_mask_1: 0.2024  loss_dice_1: 1.426  loss_ce_2: 0.6289  loss_mask_2: 0.1964  loss_dice_2: 1.346  loss_ce_3: 0.579  loss_mask_3: 0.1944  loss_dice_3: 1.313  loss_ce_4: 0.5681  loss_mask_4: 0.1929  loss_dice_4: 1.306  loss_ce_5: 0.5286  loss_mask_5: 0.1915  loss_dice_5: 1.271  loss_ce_6: 0.5154  loss_mask_6: 0.1882  loss_dice_6: 1.285  loss_ce_7: 0.5196  loss_mask_7: 0.1882  loss_dice_7: 1.252  loss_ce_8: 0.5248  loss_mask_8: 0.1885  loss_dice_8: 1.26    time: 1.0919  last_time: 1.0808  data_time: 0.0731  last_data_time: 0.0773   lr: 0.0001  max_mem: 32515M
[10/09 11:50:40] d2.utils.events INFO:  eta: 0:12:20  iter: 319  total_loss: 21.12  loss_ce: 0.5328  loss_mask: 0.1831  loss_dice: 1.295  loss_ce_0: 0.6036  loss_mask_0: 0.2246  loss_dice_0: 1.555  loss_ce_1: 0.672  loss_mask_1: 0.2017  loss_dice_1: 1.44  loss_ce_2: 0.6233  loss_mask_2: 0.1855  loss_dice_2: 1.379  loss_ce_3: 0.5567  loss_mask_3: 0.1852  loss_dice_3: 1.355  loss_ce_4: 0.5555  loss_mask_4: 0.1877  loss_dice_4: 1.365  loss_ce_5: 0.5586  loss_mask_5: 0.1841  loss_dice_5: 1.297  loss_ce_6: 0.5435  loss_mask_6: 0.1852  loss_dice_6: 1.309  loss_ce_7: 0.5283  loss_mask_7: 0.1863  loss_dice_7: 1.323  loss_ce_8: 0.5342  loss_mask_8: 0.185  loss_dice_8: 1.258    time: 1.0918  last_time: 1.0363  data_time: 0.0747  last_data_time: 0.0714   lr: 0.0001  max_mem: 32515M
[10/09 11:51:02] d2.utils.events INFO:  eta: 0:11:57  iter: 339  total_loss: 21.06  loss_ce: 0.5661  loss_mask: 0.1901  loss_dice: 1.306  loss_ce_0: 0.6052  loss_mask_0: 0.221  loss_dice_0: 1.567  loss_ce_1: 0.7061  loss_mask_1: 0.2085  loss_dice_1: 1.468  loss_ce_2: 0.644  loss_mask_2: 0.2016  loss_dice_2: 1.419  loss_ce_3: 0.5841  loss_mask_3: 0.1982  loss_dice_3: 1.314  loss_ce_4: 0.6106  loss_mask_4: 0.193  loss_dice_4: 1.324  loss_ce_5: 0.5787  loss_mask_5: 0.1935  loss_dice_5: 1.333  loss_ce_6: 0.5654  loss_mask_6: 0.1907  loss_dice_6: 1.301  loss_ce_7: 0.573  loss_mask_7: 0.1917  loss_dice_7: 1.337  loss_ce_8: 0.5507  loss_mask_8: 0.1922  loss_dice_8: 1.278    time: 1.0911  last_time: 1.0564  data_time: 0.0742  last_data_time: 0.0665   lr: 0.0001  max_mem: 32515M
[10/09 11:51:23] d2.utils.events INFO:  eta: 0:11:34  iter: 359  total_loss: 21.2  loss_ce: 0.5374  loss_mask: 0.2051  loss_dice: 1.302  loss_ce_0: 0.5978  loss_mask_0: 0.226  loss_dice_0: 1.469  loss_ce_1: 0.7109  loss_mask_1: 0.213  loss_dice_1: 1.387  loss_ce_2: 0.6283  loss_mask_2: 0.2052  loss_dice_2: 1.331  loss_ce_3: 0.61  loss_mask_3: 0.2073  loss_dice_3: 1.262  loss_ce_4: 0.5623  loss_mask_4: 0.2041  loss_dice_4: 1.281  loss_ce_5: 0.5837  loss_mask_5: 0.2037  loss_dice_5: 1.307  loss_ce_6: 0.5694  loss_mask_6: 0.202  loss_dice_6: 1.264  loss_ce_7: 0.552  loss_mask_7: 0.2048  loss_dice_7: 1.279  loss_ce_8: 0.5316  loss_mask_8: 0.2048  loss_dice_8: 1.262    time: 1.0891  last_time: 1.0040  data_time: 0.0653  last_data_time: 0.0542   lr: 0.0001  max_mem: 32515M
[10/09 11:51:44] d2.utils.events INFO:  eta: 0:11:11  iter: 379  total_loss: 20.95  loss_ce: 0.5631  loss_mask: 0.1816  loss_dice: 1.255  loss_ce_0: 0.6579  loss_mask_0: 0.2139  loss_dice_0: 1.473  loss_ce_1: 0.7118  loss_mask_1: 0.1982  loss_dice_1: 1.352  loss_ce_2: 0.6503  loss_mask_2: 0.1861  loss_dice_2: 1.323  loss_ce_3: 0.6309  loss_mask_3: 0.189  loss_dice_3: 1.253  loss_ce_4: 0.5827  loss_mask_4: 0.1879  loss_dice_4: 1.258  loss_ce_5: 0.5793  loss_mask_5: 0.1859  loss_dice_5: 1.256  loss_ce_6: 0.5622  loss_mask_6: 0.1861  loss_dice_6: 1.253  loss_ce_7: 0.5599  loss_mask_7: 0.1857  loss_dice_7: 1.25  loss_ce_8: 0.5802  loss_mask_8: 0.1844  loss_dice_8: 1.232    time: 1.0874  last_time: 1.0234  data_time: 0.0669  last_data_time: 0.0576   lr: 0.0001  max_mem: 32515M
[10/09 11:52:06] d2.utils.events INFO:  eta: 0:10:48  iter: 399  total_loss: 21.49  loss_ce: 0.546  loss_mask: 0.1764  loss_dice: 1.347  loss_ce_0: 0.6685  loss_mask_0: 0.2025  loss_dice_0: 1.5  loss_ce_1: 0.7156  loss_mask_1: 0.1855  loss_dice_1: 1.408  loss_ce_2: 0.623  loss_mask_2: 0.1853  loss_dice_2: 1.388  loss_ce_3: 0.583  loss_mask_3: 0.1787  loss_dice_3: 1.333  loss_ce_4: 0.5684  loss_mask_4: 0.1819  loss_dice_4: 1.331  loss_ce_5: 0.5559  loss_mask_5: 0.1798  loss_dice_5: 1.292  loss_ce_6: 0.547  loss_mask_6: 0.1773  loss_dice_6: 1.292  loss_ce_7: 0.5314  loss_mask_7: 0.1791  loss_dice_7: 1.314  loss_ce_8: 0.5288  loss_mask_8: 0.176  loss_dice_8: 1.338    time: 1.0862  last_time: 1.0919  data_time: 0.0708  last_data_time: 0.0805   lr: 0.0001  max_mem: 32795M
[10/09 11:52:27] d2.utils.events INFO:  eta: 0:10:26  iter: 419  total_loss: 21.2  loss_ce: 0.4976  loss_mask: 0.169  loss_dice: 1.293  loss_ce_0: 0.612  loss_mask_0: 0.1998  loss_dice_0: 1.59  loss_ce_1: 0.6682  loss_mask_1: 0.1825  loss_dice_1: 1.487  loss_ce_2: 0.6157  loss_mask_2: 0.1734  loss_dice_2: 1.382  loss_ce_3: 0.5528  loss_mask_3: 0.1718  loss_dice_3: 1.324  loss_ce_4: 0.5331  loss_mask_4: 0.1694  loss_dice_4: 1.325  loss_ce_5: 0.5284  loss_mask_5: 0.1701  loss_dice_5: 1.316  loss_ce_6: 0.499  loss_mask_6: 0.1705  loss_dice_6: 1.306  loss_ce_7: 0.5145  loss_mask_7: 0.1718  loss_dice_7: 1.307  loss_ce_8: 0.5279  loss_mask_8: 0.17  loss_dice_8: 1.265    time: 1.0856  last_time: 1.0830  data_time: 0.0722  last_data_time: 0.0593   lr: 0.0001  max_mem: 32795M
[10/09 11:52:49] d2.utils.events INFO:  eta: 0:10:04  iter: 439  total_loss: 20.28  loss_ce: 0.4791  loss_mask: 0.1811  loss_dice: 1.212  loss_ce_0: 0.5686  loss_mask_0: 0.1988  loss_dice_0: 1.496  loss_ce_1: 0.65  loss_mask_1: 0.1917  loss_dice_1: 1.366  loss_ce_2: 0.6003  loss_mask_2: 0.1809  loss_dice_2: 1.311  loss_ce_3: 0.5217  loss_mask_3: 0.1799  loss_dice_3: 1.264  loss_ce_4: 0.5216  loss_mask_4: 0.178  loss_dice_4: 1.257  loss_ce_5: 0.497  loss_mask_5: 0.1795  loss_dice_5: 1.248  loss_ce_6: 0.4899  loss_mask_6: 0.1783  loss_dice_6: 1.222  loss_ce_7: 0.4959  loss_mask_7: 0.1805  loss_dice_7: 1.251  loss_ce_8: 0.4693  loss_mask_8: 0.1775  loss_dice_8: 1.225    time: 1.0845  last_time: 1.0577  data_time: 0.0709  last_data_time: 0.0704   lr: 0.0001  max_mem: 32795M
[10/09 11:53:10] d2.utils.events INFO:  eta: 0:09:42  iter: 459  total_loss: 20.46  loss_ce: 0.5058  loss_mask: 0.1718  loss_dice: 1.246  loss_ce_0: 0.6204  loss_mask_0: 0.201  loss_dice_0: 1.468  loss_ce_1: 0.7013  loss_mask_1: 0.1886  loss_dice_1: 1.368  loss_ce_2: 0.6281  loss_mask_2: 0.1775  loss_dice_2: 1.349  loss_ce_3: 0.5587  loss_mask_3: 0.1739  loss_dice_3: 1.269  loss_ce_4: 0.5748  loss_mask_4: 0.1727  loss_dice_4: 1.247  loss_ce_5: 0.5617  loss_mask_5: 0.1704  loss_dice_5: 1.245  loss_ce_6: 0.5091  loss_mask_6: 0.17  loss_dice_6: 1.226  loss_ce_7: 0.5365  loss_mask_7: 0.1714  loss_dice_7: 1.267  loss_ce_8: 0.4898  loss_mask_8: 0.1725  loss_dice_8: 1.216    time: 1.0837  last_time: 1.1388  data_time: 0.0699  last_data_time: 0.0776   lr: 0.0001  max_mem: 32795M
[10/09 11:53:31] d2.utils.events INFO:  eta: 0:09:20  iter: 479  total_loss: 21.1  loss_ce: 0.4884  loss_mask: 0.1782  loss_dice: 1.266  loss_ce_0: 0.5949  loss_mask_0: 0.2047  loss_dice_0: 1.486  loss_ce_1: 0.6463  loss_mask_1: 0.1959  loss_dice_1: 1.386  loss_ce_2: 0.5943  loss_mask_2: 0.1886  loss_dice_2: 1.359  loss_ce_3: 0.5697  loss_mask_3: 0.1812  loss_dice_3: 1.289  loss_ce_4: 0.562  loss_mask_4: 0.1782  loss_dice_4: 1.289  loss_ce_5: 0.5304  loss_mask_5: 0.1787  loss_dice_5: 1.275  loss_ce_6: 0.505  loss_mask_6: 0.1781  loss_dice_6: 1.242  loss_ce_7: 0.4955  loss_mask_7: 0.1779  loss_dice_7: 1.239  loss_ce_8: 0.5076  loss_mask_8: 0.1758  loss_dice_8: 1.262    time: 1.0824  last_time: 1.0169  data_time: 0.0694  last_data_time: 0.0631   lr: 0.0001  max_mem: 32795M
[10/09 11:53:52] d2.utils.events INFO:  eta: 0:08:57  iter: 499  total_loss: 21.02  loss_ce: 0.5309  loss_mask: 0.1704  loss_dice: 1.319  loss_ce_0: 0.58  loss_mask_0: 0.198  loss_dice_0: 1.607  loss_ce_1: 0.6866  loss_mask_1: 0.1867  loss_dice_1: 1.455  loss_ce_2: 0.6166  loss_mask_2: 0.1749  loss_dice_2: 1.384  loss_ce_3: 0.5841  loss_mask_3: 0.1736  loss_dice_3: 1.336  loss_ce_4: 0.5394  loss_mask_4: 0.1729  loss_dice_4: 1.328  loss_ce_5: 0.5454  loss_mask_5: 0.1748  loss_dice_5: 1.348  loss_ce_6: 0.5398  loss_mask_6: 0.1725  loss_dice_6: 1.306  loss_ce_7: 0.5199  loss_mask_7: 0.1716  loss_dice_7: 1.296  loss_ce_8: 0.5436  loss_mask_8: 0.1718  loss_dice_8: 1.292    time: 1.0818  last_time: 1.0685  data_time: 0.0710  last_data_time: 0.0658   lr: 0.0001  max_mem: 32795M
[10/09 11:54:14] d2.utils.events INFO:  eta: 0:08:35  iter: 519  total_loss: 20.57  loss_ce: 0.5056  loss_mask: 0.1745  loss_dice: 1.288  loss_ce_0: 0.5329  loss_mask_0: 0.2069  loss_dice_0: 1.582  loss_ce_1: 0.6612  loss_mask_1: 0.1919  loss_dice_1: 1.418  loss_ce_2: 0.6087  loss_mask_2: 0.1836  loss_dice_2: 1.377  loss_ce_3: 0.5862  loss_mask_3: 0.1801  loss_dice_3: 1.301  loss_ce_4: 0.551  loss_mask_4: 0.1821  loss_dice_4: 1.334  loss_ce_5: 0.5496  loss_mask_5: 0.1777  loss_dice_5: 1.317  loss_ce_6: 0.5249  loss_mask_6: 0.1751  loss_dice_6: 1.285  loss_ce_7: 0.4955  loss_mask_7: 0.1762  loss_dice_7: 1.321  loss_ce_8: 0.5115  loss_mask_8: 0.1769  loss_dice_8: 1.269    time: 1.0811  last_time: 1.1241  data_time: 0.0697  last_data_time: 0.0778   lr: 0.0001  max_mem: 32795M
[10/09 11:54:35] d2.utils.events INFO:  eta: 0:08:14  iter: 539  total_loss: 19.53  loss_ce: 0.4874  loss_mask: 0.1879  loss_dice: 1.256  loss_ce_0: 0.5653  loss_mask_0: 0.2179  loss_dice_0: 1.434  loss_ce_1: 0.645  loss_mask_1: 0.1996  loss_dice_1: 1.347  loss_ce_2: 0.5877  loss_mask_2: 0.1914  loss_dice_2: 1.269  loss_ce_3: 0.5462  loss_mask_3: 0.19  loss_dice_3: 1.211  loss_ce_4: 0.5302  loss_mask_4: 0.1935  loss_dice_4: 1.253  loss_ce_5: 0.5063  loss_mask_5: 0.1908  loss_dice_5: 1.26  loss_ce_6: 0.4551  loss_mask_6: 0.1888  loss_dice_6: 1.241  loss_ce_7: 0.4722  loss_mask_7: 0.1876  loss_dice_7: 1.21  loss_ce_8: 0.4622  loss_mask_8: 0.1871  loss_dice_8: 1.233    time: 1.0802  last_time: 1.0527  data_time: 0.0673  last_data_time: 0.0527   lr: 0.0001  max_mem: 32795M
[10/09 11:54:56] d2.utils.events INFO:  eta: 0:07:52  iter: 559  total_loss: 20.93  loss_ce: 0.4736  loss_mask: 0.171  loss_dice: 1.287  loss_ce_0: 0.6037  loss_mask_0: 0.2069  loss_dice_0: 1.484  loss_ce_1: 0.6458  loss_mask_1: 0.1817  loss_dice_1: 1.383  loss_ce_2: 0.5929  loss_mask_2: 0.1679  loss_dice_2: 1.343  loss_ce_3: 0.5709  loss_mask_3: 0.1733  loss_dice_3: 1.275  loss_ce_4: 0.5081  loss_mask_4: 0.1745  loss_dice_4: 1.322  loss_ce_5: 0.5092  loss_mask_5: 0.1703  loss_dice_5: 1.301  loss_ce_6: 0.4924  loss_mask_6: 0.1707  loss_dice_6: 1.286  loss_ce_7: 0.5186  loss_mask_7: 0.1707  loss_dice_7: 1.306  loss_ce_8: 0.5062  loss_mask_8: 0.1724  loss_dice_8: 1.276    time: 1.0798  last_time: 1.1084  data_time: 0.0687  last_data_time: 0.0634   lr: 0.0001  max_mem: 32795M
[10/09 11:55:18] d2.utils.events INFO:  eta: 0:07:31  iter: 579  total_loss: 20.69  loss_ce: 0.5256  loss_mask: 0.1796  loss_dice: 1.243  loss_ce_0: 0.61  loss_mask_0: 0.217  loss_dice_0: 1.522  loss_ce_1: 0.6963  loss_mask_1: 0.1961  loss_dice_1: 1.409  loss_ce_2: 0.5951  loss_mask_2: 0.1882  loss_dice_2: 1.374  loss_ce_3: 0.5816  loss_mask_3: 0.1859  loss_dice_3: 1.274  loss_ce_4: 0.5495  loss_mask_4: 0.1824  loss_dice_4: 1.26  loss_ce_5: 0.5413  loss_mask_5: 0.1809  loss_dice_5: 1.251  loss_ce_6: 0.5326  loss_mask_6: 0.1786  loss_dice_6: 1.249  loss_ce_7: 0.5258  loss_mask_7: 0.1797  loss_dice_7: 1.274  loss_ce_8: 0.5446  loss_mask_8: 0.1807  loss_dice_8: 1.278    time: 1.0805  last_time: 1.0970  data_time: 0.0800  last_data_time: 0.0985   lr: 0.0001  max_mem: 32795M
[10/09 11:55:41] d2.utils.events INFO:  eta: 0:07:09  iter: 599  total_loss: 21.05  loss_ce: 0.5374  loss_mask: 0.1646  loss_dice: 1.27  loss_ce_0: 0.6107  loss_mask_0: 0.1925  loss_dice_0: 1.496  loss_ce_1: 0.685  loss_mask_1: 0.1812  loss_dice_1: 1.415  loss_ce_2: 0.6537  loss_mask_2: 0.1693  loss_dice_2: 1.362  loss_ce_3: 0.5633  loss_mask_3: 0.1665  loss_dice_3: 1.307  loss_ce_4: 0.535  loss_mask_4: 0.1642  loss_dice_4: 1.337  loss_ce_5: 0.5284  loss_mask_5: 0.1617  loss_dice_5: 1.288  loss_ce_6: 0.5192  loss_mask_6: 0.1642  loss_dice_6: 1.274  loss_ce_7: 0.5521  loss_mask_7: 0.1635  loss_dice_7: 1.265  loss_ce_8: 0.5237  loss_mask_8: 0.1624  loss_dice_8: 1.248    time: 1.0819  last_time: 1.1272  data_time: 0.0760  last_data_time: 0.0615   lr: 0.0001  max_mem: 32795M
[10/09 11:56:03] d2.utils.events INFO:  eta: 0:06:48  iter: 619  total_loss: 20.41  loss_ce: 0.4925  loss_mask: 0.1793  loss_dice: 1.306  loss_ce_0: 0.5634  loss_mask_0: 0.2039  loss_dice_0: 1.486  loss_ce_1: 0.6336  loss_mask_1: 0.1962  loss_dice_1: 1.374  loss_ce_2: 0.617  loss_mask_2: 0.1827  loss_dice_2: 1.313  loss_ce_3: 0.5413  loss_mask_3: 0.1825  loss_dice_3: 1.293  loss_ce_4: 0.5024  loss_mask_4: 0.1798  loss_dice_4: 1.325  loss_ce_5: 0.5142  loss_mask_5: 0.1795  loss_dice_5: 1.321  loss_ce_6: 0.4882  loss_mask_6: 0.18  loss_dice_6: 1.315  loss_ce_7: 0.514  loss_mask_7: 0.1779  loss_dice_7: 1.251  loss_ce_8: 0.4795  loss_mask_8: 0.1784  loss_dice_8: 1.287    time: 1.0822  last_time: 1.1485  data_time: 0.0655  last_data_time: 0.0569   lr: 0.0001  max_mem: 32795M
[10/09 11:56:25] d2.utils.events INFO:  eta: 0:06:27  iter: 639  total_loss: 20.32  loss_ce: 0.5032  loss_mask: 0.1873  loss_dice: 1.208  loss_ce_0: 0.5654  loss_mask_0: 0.2303  loss_dice_0: 1.46  loss_ce_1: 0.6145  loss_mask_1: 0.2065  loss_dice_1: 1.368  loss_ce_2: 0.5641  loss_mask_2: 0.1931  loss_dice_2: 1.3  loss_ce_3: 0.533  loss_mask_3: 0.1883  loss_dice_3: 1.256  loss_ce_4: 0.5156  loss_mask_4: 0.1878  loss_dice_4: 1.212  loss_ce_5: 0.5014  loss_mask_5: 0.1861  loss_dice_5: 1.26  loss_ce_6: 0.483  loss_mask_6: 0.1838  loss_dice_6: 1.19  loss_ce_7: 0.476  loss_mask_7: 0.1855  loss_dice_7: 1.207  loss_ce_8: 0.4695  loss_mask_8: 0.1842  loss_dice_8: 1.179    time: 1.0827  last_time: 1.1489  data_time: 0.0681  last_data_time: 0.0757   lr: 0.0001  max_mem: 32795M
[10/09 11:56:47] d2.utils.events INFO:  eta: 0:06:06  iter: 659  total_loss: 20.48  loss_ce: 0.5208  loss_mask: 0.1923  loss_dice: 1.279  loss_ce_0: 0.6194  loss_mask_0: 0.2163  loss_dice_0: 1.464  loss_ce_1: 0.7013  loss_mask_1: 0.1991  loss_dice_1: 1.399  loss_ce_2: 0.6199  loss_mask_2: 0.1962  loss_dice_2: 1.299  loss_ce_3: 0.5429  loss_mask_3: 0.1922  loss_dice_3: 1.313  loss_ce_4: 0.5283  loss_mask_4: 0.1904  loss_dice_4: 1.268  loss_ce_5: 0.549  loss_mask_5: 0.1953  loss_dice_5: 1.236  loss_ce_6: 0.5231  loss_mask_6: 0.1951  loss_dice_6: 1.243  loss_ce_7: 0.4935  loss_mask_7: 0.1962  loss_dice_7: 1.262  loss_ce_8: 0.5409  loss_mask_8: 0.1949  loss_dice_8: 1.216    time: 1.0827  last_time: 1.0688  data_time: 0.0639  last_data_time: 0.0487   lr: 0.0001  max_mem: 32795M
[10/09 11:57:08] d2.utils.events INFO:  eta: 0:05:44  iter: 679  total_loss: 20.36  loss_ce: 0.503  loss_mask: 0.1755  loss_dice: 1.246  loss_ce_0: 0.5883  loss_mask_0: 0.2017  loss_dice_0: 1.523  loss_ce_1: 0.6631  loss_mask_1: 0.1895  loss_dice_1: 1.417  loss_ce_2: 0.5985  loss_mask_2: 0.1775  loss_dice_2: 1.341  loss_ce_3: 0.5518  loss_mask_3: 0.1746  loss_dice_3: 1.321  loss_ce_4: 0.5515  loss_mask_4: 0.1802  loss_dice_4: 1.313  loss_ce_5: 0.5527  loss_mask_5: 0.1773  loss_dice_5: 1.282  loss_ce_6: 0.5327  loss_mask_6: 0.1727  loss_dice_6: 1.289  loss_ce_7: 0.4967  loss_mask_7: 0.1753  loss_dice_7: 1.284  loss_ce_8: 0.5189  loss_mask_8: 0.1752  loss_dice_8: 1.261    time: 1.0819  last_time: 1.0964  data_time: 0.0676  last_data_time: 0.0746   lr: 0.0001  max_mem: 32795M
[10/09 11:57:29] d2.utils.events INFO:  eta: 0:05:22  iter: 699  total_loss: 20.38  loss_ce: 0.5113  loss_mask: 0.176  loss_dice: 1.26  loss_ce_0: 0.5744  loss_mask_0: 0.1939  loss_dice_0: 1.544  loss_ce_1: 0.64  loss_mask_1: 0.1874  loss_dice_1: 1.423  loss_ce_2: 0.6189  loss_mask_2: 0.1752  loss_dice_2: 1.382  loss_ce_3: 0.5503  loss_mask_3: 0.1781  loss_dice_3: 1.327  loss_ce_4: 0.5464  loss_mask_4: 0.1757  loss_dice_4: 1.321  loss_ce_5: 0.558  loss_mask_5: 0.1722  loss_dice_5: 1.297  loss_ce_6: 0.5323  loss_mask_6: 0.177  loss_dice_6: 1.268  loss_ce_7: 0.5094  loss_mask_7: 0.1767  loss_dice_7: 1.28  loss_ce_8: 0.5355  loss_mask_8: 0.1725  loss_dice_8: 1.249    time: 1.0815  last_time: 1.0244  data_time: 0.0698  last_data_time: 0.0537   lr: 0.0001  max_mem: 32795M
[10/09 11:57:50] d2.utils.events INFO:  eta: 0:05:00  iter: 719  total_loss: 20.22  loss_ce: 0.4966  loss_mask: 0.1884  loss_dice: 1.226  loss_ce_0: 0.5872  loss_mask_0: 0.222  loss_dice_0: 1.491  loss_ce_1: 0.6675  loss_mask_1: 0.2029  loss_dice_1: 1.374  loss_ce_2: 0.591  loss_mask_2: 0.1966  loss_dice_2: 1.296  loss_ce_3: 0.5553  loss_mask_3: 0.1972  loss_dice_3: 1.265  loss_ce_4: 0.5053  loss_mask_4: 0.1911  loss_dice_4: 1.236  loss_ce_5: 0.5125  loss_mask_5: 0.1898  loss_dice_5: 1.237  loss_ce_6: 0.4971  loss_mask_6: 0.1909  loss_dice_6: 1.218  loss_ce_7: 0.5324  loss_mask_7: 0.1882  loss_dice_7: 1.222  loss_ce_8: 0.5075  loss_mask_8: 0.1842  loss_dice_8: 1.221    time: 1.0809  last_time: 1.0337  data_time: 0.0657  last_data_time: 0.0630   lr: 0.0001  max_mem: 32795M
[10/09 11:58:12] d2.utils.events INFO:  eta: 0:04:39  iter: 739  total_loss: 21.14  loss_ce: 0.5368  loss_mask: 0.1673  loss_dice: 1.276  loss_ce_0: 0.6335  loss_mask_0: 0.1945  loss_dice_0: 1.477  loss_ce_1: 0.698  loss_mask_1: 0.1823  loss_dice_1: 1.412  loss_ce_2: 0.6661  loss_mask_2: 0.1734  loss_dice_2: 1.359  loss_ce_3: 0.5967  loss_mask_3: 0.1664  loss_dice_3: 1.298  loss_ce_4: 0.5542  loss_mask_4: 0.1698  loss_dice_4: 1.27  loss_ce_5: 0.5525  loss_mask_5: 0.1694  loss_dice_5: 1.287  loss_ce_6: 0.5404  loss_mask_6: 0.1653  loss_dice_6: 1.253  loss_ce_7: 0.5577  loss_mask_7: 0.1691  loss_dice_7: 1.285  loss_ce_8: 0.5204  loss_mask_8: 0.1665  loss_dice_8: 1.265    time: 1.0805  last_time: 1.0217  data_time: 0.0708  last_data_time: 0.0607   lr: 0.0001  max_mem: 32795M
[10/09 11:58:33] d2.utils.events INFO:  eta: 0:04:17  iter: 759  total_loss: 20.6  loss_ce: 0.5554  loss_mask: 0.1738  loss_dice: 1.215  loss_ce_0: 0.5888  loss_mask_0: 0.1935  loss_dice_0: 1.427  loss_ce_1: 0.6707  loss_mask_1: 0.1874  loss_dice_1: 1.365  loss_ce_2: 0.6231  loss_mask_2: 0.1792  loss_dice_2: 1.311  loss_ce_3: 0.5664  loss_mask_3: 0.1808  loss_dice_3: 1.239  loss_ce_4: 0.5191  loss_mask_4: 0.1739  loss_dice_4: 1.246  loss_ce_5: 0.5394  loss_mask_5: 0.1753  loss_dice_5: 1.262  loss_ce_6: 0.5589  loss_mask_6: 0.1766  loss_dice_6: 1.239  loss_ce_7: 0.5311  loss_mask_7: 0.1749  loss_dice_7: 1.242  loss_ce_8: 0.5179  loss_mask_8: 0.1726  loss_dice_8: 1.244    time: 1.0799  last_time: 1.1284  data_time: 0.0676  last_data_time: 0.0854   lr: 0.0001  max_mem: 32795M
[10/09 11:58:55] d2.utils.events INFO:  eta: 0:03:56  iter: 779  total_loss: 19.59  loss_ce: 0.4907  loss_mask: 0.1703  loss_dice: 1.158  loss_ce_0: 0.5922  loss_mask_0: 0.1888  loss_dice_0: 1.432  loss_ce_1: 0.6139  loss_mask_1: 0.1771  loss_dice_1: 1.356  loss_ce_2: 0.5983  loss_mask_2: 0.1705  loss_dice_2: 1.274  loss_ce_3: 0.5293  loss_mask_3: 0.1721  loss_dice_3: 1.234  loss_ce_4: 0.5087  loss_mask_4: 0.171  loss_dice_4: 1.204  loss_ce_5: 0.491  loss_mask_5: 0.1714  loss_dice_5: 1.195  loss_ce_6: 0.5111  loss_mask_6: 0.1703  loss_dice_6: 1.194  loss_ce_7: 0.4656  loss_mask_7: 0.1689  loss_dice_7: 1.212  loss_ce_8: 0.494  loss_mask_8: 0.1688  loss_dice_8: 1.155    time: 1.0801  last_time: 1.0219  data_time: 0.0726  last_data_time: 0.0626   lr: 0.0001  max_mem: 32795M
[10/09 11:59:16] d2.utils.events INFO:  eta: 0:03:34  iter: 799  total_loss: 21.44  loss_ce: 0.4917  loss_mask: 0.1797  loss_dice: 1.295  loss_ce_0: 0.6359  loss_mask_0: 0.207  loss_dice_0: 1.539  loss_ce_1: 0.6822  loss_mask_1: 0.1906  loss_dice_1: 1.449  loss_ce_2: 0.6203  loss_mask_2: 0.1856  loss_dice_2: 1.414  loss_ce_3: 0.5404  loss_mask_3: 0.1837  loss_dice_3: 1.354  loss_ce_4: 0.5126  loss_mask_4: 0.1803  loss_dice_4: 1.33  loss_ce_5: 0.5118  loss_mask_5: 0.181  loss_dice_5: 1.329  loss_ce_6: 0.5117  loss_mask_6: 0.1805  loss_dice_6: 1.289  loss_ce_7: 0.5081  loss_mask_7: 0.1805  loss_dice_7: 1.319  loss_ce_8: 0.5025  loss_mask_8: 0.1812  loss_dice_8: 1.297    time: 1.0798  last_time: 1.0829  data_time: 0.0686  last_data_time: 0.0715   lr: 0.0001  max_mem: 32795M
[10/09 11:59:38] d2.utils.events INFO:  eta: 0:03:13  iter: 819  total_loss: 20.45  loss_ce: 0.5064  loss_mask: 0.1726  loss_dice: 1.239  loss_ce_0: 0.5817  loss_mask_0: 0.1971  loss_dice_0: 1.465  loss_ce_1: 0.6378  loss_mask_1: 0.1829  loss_dice_1: 1.409  loss_ce_2: 0.6155  loss_mask_2: 0.1797  loss_dice_2: 1.321  loss_ce_3: 0.5288  loss_mask_3: 0.1762  loss_dice_3: 1.262  loss_ce_4: 0.5551  loss_mask_4: 0.1723  loss_dice_4: 1.302  loss_ce_5: 0.5036  loss_mask_5: 0.1744  loss_dice_5: 1.267  loss_ce_6: 0.511  loss_mask_6: 0.1718  loss_dice_6: 1.255  loss_ce_7: 0.519  loss_mask_7: 0.1713  loss_dice_7: 1.256  loss_ce_8: 0.5112  loss_mask_8: 0.172  loss_dice_8: 1.249    time: 1.0795  last_time: 1.0508  data_time: 0.0715  last_data_time: 0.0646   lr: 0.0001  max_mem: 32795M
[10/09 11:59:59] d2.utils.events INFO:  eta: 0:02:51  iter: 839  total_loss: 19.59  loss_ce: 0.4411  loss_mask: 0.1599  loss_dice: 1.189  loss_ce_0: 0.5625  loss_mask_0: 0.192  loss_dice_0: 1.41  loss_ce_1: 0.6007  loss_mask_1: 0.1736  loss_dice_1: 1.316  loss_ce_2: 0.5481  loss_mask_2: 0.1689  loss_dice_2: 1.275  loss_ce_3: 0.5366  loss_mask_3: 0.1609  loss_dice_3: 1.201  loss_ce_4: 0.499  loss_mask_4: 0.1575  loss_dice_4: 1.217  loss_ce_5: 0.4717  loss_mask_5: 0.1599  loss_dice_5: 1.192  loss_ce_6: 0.4628  loss_mask_6: 0.1604  loss_dice_6: 1.173  loss_ce_7: 0.4548  loss_mask_7: 0.159  loss_dice_7: 1.181  loss_ce_8: 0.479  loss_mask_8: 0.1581  loss_dice_8: 1.205    time: 1.0795  last_time: 1.0955  data_time: 0.0733  last_data_time: 0.0932   lr: 0.0001  max_mem: 32795M
[10/09 12:00:21] d2.utils.events INFO:  eta: 0:02:30  iter: 859  total_loss: 20.4  loss_ce: 0.4889  loss_mask: 0.1765  loss_dice: 1.287  loss_ce_0: 0.5582  loss_mask_0: 0.1971  loss_dice_0: 1.462  loss_ce_1: 0.6254  loss_mask_1: 0.1821  loss_dice_1: 1.405  loss_ce_2: 0.6086  loss_mask_2: 0.1762  loss_dice_2: 1.326  loss_ce_3: 0.533  loss_mask_3: 0.1771  loss_dice_3: 1.276  loss_ce_4: 0.4884  loss_mask_4: 0.1783  loss_dice_4: 1.237  loss_ce_5: 0.4938  loss_mask_5: 0.1744  loss_dice_5: 1.23  loss_ce_6: 0.485  loss_mask_6: 0.1727  loss_dice_6: 1.227  loss_ce_7: 0.4716  loss_mask_7: 0.1721  loss_dice_7: 1.258  loss_ce_8: 0.4675  loss_mask_8: 0.1773  loss_dice_8: 1.276    time: 1.0792  last_time: 1.0696  data_time: 0.0700  last_data_time: 0.0872   lr: 0.0001  max_mem: 32795M
[10/09 12:00:42] d2.utils.events INFO:  eta: 0:02:08  iter: 879  total_loss: 21.36  loss_ce: 0.5189  loss_mask: 0.1714  loss_dice: 1.307  loss_ce_0: 0.6004  loss_mask_0: 0.2032  loss_dice_0: 1.493  loss_ce_1: 0.6702  loss_mask_1: 0.183  loss_dice_1: 1.438  loss_ce_2: 0.6259  loss_mask_2: 0.1725  loss_dice_2: 1.364  loss_ce_3: 0.5729  loss_mask_3: 0.1744  loss_dice_3: 1.37  loss_ce_4: 0.5554  loss_mask_4: 0.1708  loss_dice_4: 1.321  loss_ce_5: 0.5505  loss_mask_5: 0.1686  loss_dice_5: 1.294  loss_ce_6: 0.5396  loss_mask_6: 0.1708  loss_dice_6: 1.324  loss_ce_7: 0.5513  loss_mask_7: 0.1693  loss_dice_7: 1.269  loss_ce_8: 0.5078  loss_mask_8: 0.1694  loss_dice_8: 1.303    time: 1.0791  last_time: 1.0557  data_time: 0.0713  last_data_time: 0.0542   lr: 0.0001  max_mem: 32795M
[10/09 12:01:04] d2.utils.events INFO:  eta: 0:01:47  iter: 899  total_loss: 20.44  loss_ce: 0.458  loss_mask: 0.1756  loss_dice: 1.266  loss_ce_0: 0.6118  loss_mask_0: 0.1994  loss_dice_0: 1.495  loss_ce_1: 0.6367  loss_mask_1: 0.1885  loss_dice_1: 1.404  loss_ce_2: 0.5855  loss_mask_2: 0.1781  loss_dice_2: 1.355  loss_ce_3: 0.4995  loss_mask_3: 0.1795  loss_dice_3: 1.281  loss_ce_4: 0.4944  loss_mask_4: 0.1784  loss_dice_4: 1.28  loss_ce_5: 0.5072  loss_mask_5: 0.1778  loss_dice_5: 1.279  loss_ce_6: 0.4949  loss_mask_6: 0.1778  loss_dice_6: 1.242  loss_ce_7: 0.4828  loss_mask_7: 0.1754  loss_dice_7: 1.235  loss_ce_8: 0.4784  loss_mask_8: 0.1725  loss_dice_8: 1.262    time: 1.0791  last_time: 1.0729  data_time: 0.0728  last_data_time: 0.0562   lr: 0.0001  max_mem: 32795M
[10/09 12:01:25] d2.utils.events INFO:  eta: 0:01:25  iter: 919  total_loss: 19.74  loss_ce: 0.5059  loss_mask: 0.1683  loss_dice: 1.2  loss_ce_0: 0.5272  loss_mask_0: 0.1951  loss_dice_0: 1.39  loss_ce_1: 0.6418  loss_mask_1: 0.1885  loss_dice_1: 1.315  loss_ce_2: 0.5511  loss_mask_2: 0.1715  loss_dice_2: 1.274  loss_ce_3: 0.5219  loss_mask_3: 0.1714  loss_dice_3: 1.247  loss_ce_4: 0.5176  loss_mask_4: 0.1677  loss_dice_4: 1.199  loss_ce_5: 0.4973  loss_mask_5: 0.169  loss_dice_5: 1.191  loss_ce_6: 0.4867  loss_mask_6: 0.167  loss_dice_6: 1.229  loss_ce_7: 0.5073  loss_mask_7: 0.169  loss_dice_7: 1.211  loss_ce_8: 0.4854  loss_mask_8: 0.1693  loss_dice_8: 1.207    time: 1.0788  last_time: 1.0315  data_time: 0.0694  last_data_time: 0.0659   lr: 0.0001  max_mem: 32795M
[10/09 12:01:46] d2.utils.events INFO:  eta: 0:01:04  iter: 939  total_loss: 20.03  loss_ce: 0.4772  loss_mask: 0.1591  loss_dice: 1.272  loss_ce_0: 0.5863  loss_mask_0: 0.2002  loss_dice_0: 1.483  loss_ce_1: 0.6517  loss_mask_1: 0.1737  loss_dice_1: 1.38  loss_ce_2: 0.5545  loss_mask_2: 0.1654  loss_dice_2: 1.346  loss_ce_3: 0.5189  loss_mask_3: 0.1646  loss_dice_3: 1.313  loss_ce_4: 0.4826  loss_mask_4: 0.1633  loss_dice_4: 1.291  loss_ce_5: 0.4892  loss_mask_5: 0.1629  loss_dice_5: 1.265  loss_ce_6: 0.4588  loss_mask_6: 0.1616  loss_dice_6: 1.241  loss_ce_7: 0.4752  loss_mask_7: 0.1611  loss_dice_7: 1.262  loss_ce_8: 0.4915  loss_mask_8: 0.1595  loss_dice_8: 1.234    time: 1.0783  last_time: 1.0254  data_time: 0.0672  last_data_time: 0.0660   lr: 0.0001  max_mem: 32795M
[10/09 12:02:08] d2.utils.events INFO:  eta: 0:00:42  iter: 959  total_loss: 20.23  loss_ce: 0.5487  loss_mask: 0.1819  loss_dice: 1.241  loss_ce_0: 0.5984  loss_mask_0: 0.2118  loss_dice_0: 1.45  loss_ce_1: 0.6921  loss_mask_1: 0.1927  loss_dice_1: 1.377  loss_ce_2: 0.6302  loss_mask_2: 0.1844  loss_dice_2: 1.316  loss_ce_3: 0.5708  loss_mask_3: 0.1816  loss_dice_3: 1.27  loss_ce_4: 0.5555  loss_mask_4: 0.1845  loss_dice_4: 1.258  loss_ce_5: 0.5317  loss_mask_5: 0.1795  loss_dice_5: 1.245  loss_ce_6: 0.5051  loss_mask_6: 0.1838  loss_dice_6: 1.225  loss_ce_7: 0.5304  loss_mask_7: 0.1838  loss_dice_7: 1.251  loss_ce_8: 0.5347  loss_mask_8: 0.1828  loss_dice_8: 1.221    time: 1.0785  last_time: 1.0680  data_time: 0.0760  last_data_time: 0.0947   lr: 0.0001  max_mem: 32795M
[10/09 12:02:31] d2.utils.events INFO:  eta: 0:00:21  iter: 979  total_loss: 20.41  loss_ce: 0.5091  loss_mask: 0.16  loss_dice: 1.253  loss_ce_0: 0.6112  loss_mask_0: 0.1891  loss_dice_0: 1.493  loss_ce_1: 0.7029  loss_mask_1: 0.1755  loss_dice_1: 1.395  loss_ce_2: 0.6283  loss_mask_2: 0.1681  loss_dice_2: 1.347  loss_ce_3: 0.5921  loss_mask_3: 0.1667  loss_dice_3: 1.288  loss_ce_4: 0.5387  loss_mask_4: 0.1654  loss_dice_4: 1.308  loss_ce_5: 0.5393  loss_mask_5: 0.1645  loss_dice_5: 1.301  loss_ce_6: 0.528  loss_mask_6: 0.1607  loss_dice_6: 1.258  loss_ce_7: 0.5229  loss_mask_7: 0.1648  loss_dice_7: 1.278  loss_ce_8: 0.5205  loss_mask_8: 0.1629  loss_dice_8: 1.252    time: 1.0798  last_time: 1.0583  data_time: 0.0833  last_data_time: 0.0692   lr: 0.0001  max_mem: 32795M
[10/09 12:02:53] fvcore.common.checkpoint INFO: Saving checkpoint to /tsi/hi-paris/GB/segmentation/results/In_vocab/r50_008_1000_19_a_decoder/model_final.pth
[10/09 12:02:56] d2.utils.events INFO:  eta: 0:00:00  iter: 999  total_loss: 18.9  loss_ce: 0.4559  loss_mask: 0.1714  loss_dice: 1.167  loss_ce_0: 0.4989  loss_mask_0: 0.1945  loss_dice_0: 1.372  loss_ce_1: 0.585  loss_mask_1: 0.1812  loss_dice_1: 1.293  loss_ce_2: 0.5444  loss_mask_2: 0.174  loss_dice_2: 1.239  loss_ce_3: 0.5229  loss_mask_3: 0.1733  loss_dice_3: 1.191  loss_ce_4: 0.4954  loss_mask_4: 0.1747  loss_dice_4: 1.188  loss_ce_5: 0.4621  loss_mask_5: 0.1715  loss_dice_5: 1.197  loss_ce_6: 0.4536  loss_mask_6: 0.1723  loss_dice_6: 1.173  loss_ce_7: 0.4719  loss_mask_7: 0.1722  loss_dice_7: 1.158  loss_ce_8: 0.4681  loss_mask_8: 0.1734  loss_dice_8: 1.154    time: 1.0796  last_time: 1.0836  data_time: 0.0698  last_data_time: 0.0605   lr: 0.0001  max_mem: 32795M
[10/09 12:02:56] d2.engine.hooks INFO: Overall training speed: 998 iterations in 0:17:57 (1.0796 s / it)
[10/09 12:02:56] d2.engine.hooks INFO: Total training time: 0:18:03 (0:00:06 on hooks)
[10/09 12:02:56] fcclip.data.datasets.register_cityscapes_panoptic INFO: 3 cities found in 'datasets/cityscapes/leftImg8bit/val'.
[10/09 12:02:56] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(1024, 1024), max_size=2560, sample_style='choice')]
[10/09 12:02:56] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/09 12:02:56] d2.data.common INFO: Serializing 500 elements to byte tensors and concatenating them all ...
[10/09 12:02:56] d2.data.common INFO: Serialized dataset takes 0.74 MiB
[10/09 12:02:56] d2.evaluation.evaluator INFO: Start inference on 500 batches
[10/09 12:07:39] detectron2 INFO: Rank of current process: 0. World size: 1
[10/09 12:07:40] detectron2 INFO: Environment info:
-------------------------------  -------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.8.19 | packaged by conda-forge | (default, Mar 20 2024, 12:47:35) [GCC 12.3.0]
numpy                            1.24.4
detectron2                       0.6 @/home/ids/gbrison/segmentation/segmentation/detectron2/detectron2
Compiler                         GCC 11.4
CUDA compiler                    CUDA 12.5
detectron2 arch flags            8.9
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.0+cu121 @/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0,1                          NVIDIA L40S (arch=8.9)
Driver version                   555.42.06
CUDA_HOME                        /usr/local/cuda
Pillow                           9.4.0
torchvision                      0.19.0+cu121 @/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torchvision
torchvision arch flags           5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.10.0
-------------------------------  -------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX512
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.0, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/09 12:07:40] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco/panoptic-segmentation/fcclip/r50_exp_1000_a_decoder.yaml', dist_url='tcp://127.0.0.1:51163', eval_only=False, machine_rank=0, num_gpus=1, num_machines=1, opts=[], resume=False)
[10/09 12:07:40] detectron2 INFO: Contents of args.config_file=configs/coco/panoptic-segmentation/fcclip/r50_exp_1000_a_decoder.yaml:
_BASE_: ./fcclip_convnext_large_eval_ade20k_r50.yaml

INPUT:
  MIN_SIZE_TEST: 1024
  MAX_SIZE_TEST: 2560

MODEL:
  SEM_SEG_HEAD:
    NUM_CLASSES: 19
  WEIGHTS: /tsi/hi-paris/GB/segmentation/models/fcclip_cocopan_r50.pth
OUTPUT_DIR: /tsi/hi-paris/GB/segmentation/results/In_vocab/r50_008_1000_19_a_decoder
DATASETS:
  TRAIN: ("openvocab_cityscapes_fine_panoptic_train",)
  TEST: ("openvocab_cityscapes_fine_panoptic_val",)
SOLVER:
  IMS_PER_BATCH: 8
  MAX_ITER: 1000
TEST:
  EVAL_PERIOD: 1000


[10/09 12:07:40] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - openvocab_cityscapes_fine_panoptic_val
  TRAIN:
  - openvocab_cityscapes_fine_panoptic_train
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: false
  CROP:
    ENABLED: false
    MINIMUM_INST_AREA: 1
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  DATASET_MAPPER_NAME: coco_panoptic_lsj
  FORMAT: RGB
  IMAGE_SIZE: 1024
  MASK_FORMAT: polygon
  MAX_SCALE: 2.0
  MAX_SIZE_TEST: 2560
  MAX_SIZE_TRAIN: 1333
  MIN_SCALE: 0.1
  MIN_SIZE_TEST: 1024
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: -1
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 0
    NAME: CLIP
  DEVICE: cuda
  FC_CLIP:
    CLIP_MODEL_NAME: RN50
    CLIP_PRETRAINED_WEIGHTS: openai
    EMBED_DIM: 1024
    ENSEMBLE_ON_VALID_MASK: true
    GEOMETRIC_ENSEMBLE_ALPHA: 0.4
    GEOMETRIC_ENSEMBLE_BETA: 0.8
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_FORMER:
    CLASS_WEIGHT: 2.0
    DEC_LAYERS: 10
    DEEP_SUPERVISION: true
    DICE_WEIGHT: 5.0
    DIM_FEEDFORWARD: 2048
    DROPOUT: 0.0
    ENC_LAYERS: 0
    ENFORCE_INPUT_PROJ: false
    HIDDEN_DIM: 256
    IMPORTANCE_SAMPLE_RATIO: 0.75
    MASK_WEIGHT: 5.0
    NHEADS: 8
    NO_OBJECT_WEIGHT: 0.1
    NUM_OBJECT_QUERIES: 250
    OVERSAMPLE_RATIO: 3.0
    PRE_NORM: false
    SIZE_DIVISIBILITY: 32
    TEST:
      INSTANCE_ON: true
      OBJECT_MASK_THRESHOLD: 0.0
      OVERLAP_THRESHOLD: 0.8
      PANOPTIC_ON: true
      SEMANTIC_ON: true
      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: false
    TRAIN_NUM_POINTS: 12544
    TRANSFORMER_DECODER_NAME: MultiScaleMaskedTransformerDecoder
    TRANSFORMER_IN_FEATURE: multi_scale_pixel_decoder
  MASK_ON: false
  META_ARCHITECTURE: FCCLIP
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 122.7709383
  - 116.7460125
  - 104.09373615
  PIXEL_STD:
  - 68.5005327
  - 66.6321579
  - 70.32316305
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 1
    - 1
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: basic
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 256
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES:
    - res3
    - res4
    - res5
    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8
    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    MASK_DIM: 256
    NAME: FCCLIPHead
    NORM: GN
    NUM_CLASSES: 19
    PIXEL_DECODER_NAME: MSDeformAttnPixelDecoder
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    TRANSFORMER_ENC_LAYERS: 6
    USE_DEPTHWISE_SEPARABLE_CONV: false
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    USE_CHECKPOINT: false
    WINDOW_SIZE: 7
  WEIGHTS: /tsi/hi-paris/GB/segmentation/models/fcclip_cocopan_r50.pth
OUTPUT_DIR: /tsi/hi-paris/GB/segmentation/results/In_vocab/r50_008_1000_19_a_decoder
SEED: -1
SOLVER:
  AMP:
    ENABLED: true
  BACKBONE_MULTIPLIER: 0.1
  BASE_LR: 0.0001
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 5000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.01
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 8
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 1000
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 327778
  - 355092
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 1000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[10/09 12:07:40] detectron2 INFO: Full config saved to /tsi/hi-paris/GB/segmentation/results/In_vocab/r50_008_1000_19_a_decoder/config.yaml
[10/09 12:07:40] d2.utils.env INFO: Using a generated random seed 41687128
[10/09 12:07:44] d2.engine.defaults INFO: Model:
FCCLIP(
  (backbone): CLIP(
    (clip_model): CLIP(
      (visual): ModifiedResNet(
        (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
        (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        (layer1): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
            (downsample): Sequential(
              (-1): AvgPool2d(kernel_size=1, stride=1, padding=0)
              (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
        )
        (layer2): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
            (downsample): Sequential(
              (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (3): Bottleneck(
            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
        )
        (layer3): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
            (downsample): Sequential(
              (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
              (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (3): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (4): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (5): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
        )
        (layer4): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
            (downsample): Sequential(
              (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
              (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
        )
        (attnpool): AttentionPool2d(
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (c_proj): Linear(in_features=2048, out_features=1024, bias=True)
        )
      )
      (transformer): Transformer(
        (resblocks): ModuleList(
          (0-11): 12 x ResidualAttentionBlock(
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ls_2): Identity()
          )
        )
      )
      (token_embedding): Embedding(49408, 512)
      (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
  )
  (sem_seg_head): FCCLIPHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0-8): 9 x SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0-8): 9 x CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0-8): 9 x FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(250, 256)
      (query_embed): Embedding(250, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0-2): 3 x Sequential()
      )
      (mask_embed): MLP(
        (layers): ModuleList(
          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (mask_pooling): MaskPooling()
      (_mask_pooling_proj): Sequential(
        (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
      )
      (class_embed): MLP(
        (layers): ModuleList(
          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=1024, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 19
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
  (conv1): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv2): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU()
  (mask_pooling): MaskPooling()
  (decoder_adapter): DecoderAdapter(
    (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
    (conv2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    (norm1): LayerNorm((64, 1, 1), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((256, 1, 1), eps=1e-05, elementwise_affine=True)
    (relu): ReLU()
  )
  (void_embedding): Embedding(1, 1024)
)
[10/09 12:07:44] fcclip.data.dataset_mappers.mask_former_semantic_dataset_mapper INFO: [MaskFormerPanopticDatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(800,), max_size=1333, sample_style='choice'), RandomFlip()]
[10/09 12:07:45] fcclip.data.datasets.register_cityscapes_panoptic INFO: 18 cities found in 'datasets/cityscapes/leftImg8bit/train'.
[10/09 12:07:45] d2.data.build INFO: Using training sampler TrainingSampler
[10/09 12:07:45] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/09 12:07:45] d2.data.common INFO: Serializing 2975 elements to byte tensors and concatenating them all ...
[10/09 12:07:45] d2.data.common INFO: Serialized dataset takes 4.12 MiB
[10/09 12:07:45] d2.data.build INFO: Making batched data loader with batch_size=8
[10/09 12:07:45] d2.solver.build WARNING: SOLVER.STEPS contains values larger than SOLVER.MAX_ITER. These values will be ignored.
[10/09 12:07:45] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /tsi/hi-paris/GB/segmentation/models/fcclip_cocopan_r50.pth ...
[10/09 12:07:45] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /tsi/hi-paris/GB/segmentation/models/fcclip_cocopan_r50.pth ...
[10/09 12:07:45] fvcore.common.checkpoint WARNING: Skip loading parameter 'criterion.empty_weight' to the model due to incompatible shapes: (134,) in the checkpoint but (20,) in the model! You might want to double check if this is expected.
[10/09 12:07:45] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.clip_model.ln_final.{bias, weight}[0m
[34mbackbone.clip_model.token_embedding.weight[0m
[34mbackbone.clip_model.transformer.resblocks.0.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.attnpool.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.attnpool.k_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.attnpool.positional_embedding[0m
[34mbackbone.clip_model.visual.attnpool.q_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.attnpool.v_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.conv1.weight[0m
[34mbackbone.clip_model.visual.conv2.weight[0m
[34mbackbone.clip_model.visual.conv3.weight[0m
[34mbackbone.clip_model.visual.layer1.0.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.0.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.0.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.0.conv1.weight[0m
[34mbackbone.clip_model.visual.layer1.0.conv2.weight[0m
[34mbackbone.clip_model.visual.layer1.0.conv3.weight[0m
[34mbackbone.clip_model.visual.layer1.0.downsample.0.weight[0m
[34mbackbone.clip_model.visual.layer1.0.downsample.1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.1.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.1.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.1.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.1.conv1.weight[0m
[34mbackbone.clip_model.visual.layer1.1.conv2.weight[0m
[34mbackbone.clip_model.visual.layer1.1.conv3.weight[0m
[34mbackbone.clip_model.visual.layer1.2.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.2.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.2.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.2.conv1.weight[0m
[34mbackbone.clip_model.visual.layer1.2.conv2.weight[0m
[34mbackbone.clip_model.visual.layer1.2.conv3.weight[0m
[34mbackbone.clip_model.visual.layer2.0.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.0.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.0.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.0.conv1.weight[0m
[34mbackbone.clip_model.visual.layer2.0.conv2.weight[0m
[34mbackbone.clip_model.visual.layer2.0.conv3.weight[0m
[34mbackbone.clip_model.visual.layer2.0.downsample.0.weight[0m
[34mbackbone.clip_model.visual.layer2.0.downsample.1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.1.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.1.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.1.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.1.conv1.weight[0m
[34mbackbone.clip_model.visual.layer2.1.conv2.weight[0m
[34mbackbone.clip_model.visual.layer2.1.conv3.weight[0m
[34mbackbone.clip_model.visual.layer2.2.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.2.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.2.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.2.conv1.weight[0m
[34mbackbone.clip_model.visual.layer2.2.conv2.weight[0m
[34mbackbone.clip_model.visual.layer2.2.conv3.weight[0m
[34mbackbone.clip_model.visual.layer2.3.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.3.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.3.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.3.conv1.weight[0m
[34mbackbone.clip_model.visual.layer2.3.conv2.weight[0m
[34mbackbone.clip_model.visual.layer2.3.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.0.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.0.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.0.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.0.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.0.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.0.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.0.downsample.0.weight[0m
[34mbackbone.clip_model.visual.layer3.0.downsample.1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.1.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.1.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.1.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.1.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.1.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.1.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.2.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.2.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.2.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.2.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.2.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.2.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.3.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.3.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.3.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.3.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.3.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.3.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.4.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.4.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.4.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.4.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.4.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.4.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.5.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.5.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.5.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.5.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.5.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.5.conv3.weight[0m
[34mbackbone.clip_model.visual.layer4.0.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.0.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.0.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.0.conv1.weight[0m
[34mbackbone.clip_model.visual.layer4.0.conv2.weight[0m
[34mbackbone.clip_model.visual.layer4.0.conv3.weight[0m
[34mbackbone.clip_model.visual.layer4.0.downsample.0.weight[0m
[34mbackbone.clip_model.visual.layer4.0.downsample.1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.1.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.1.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.1.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.1.conv1.weight[0m
[34mbackbone.clip_model.visual.layer4.1.conv2.weight[0m
[34mbackbone.clip_model.visual.layer4.1.conv3.weight[0m
[34mbackbone.clip_model.visual.layer4.2.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.2.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.2.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.2.conv1.weight[0m
[34mbackbone.clip_model.visual.layer4.2.conv2.weight[0m
[34mbackbone.clip_model.visual.layer4.2.conv3.weight[0m
[34mbackbone.clip_model.{logit_scale, positional_embedding, text_projection}[0m
[34mbn1.{bias, running_mean, running_var, weight}[0m
[34mbn2.{bias, running_mean, running_var, weight}[0m
[34mconv1.{bias, weight}[0m
[34mconv2.{bias, weight}[0m
[34mcriterion.empty_weight[0m
[34mdecoder_adapter.conv1.{bias, weight}[0m
[34mdecoder_adapter.conv2.{bias, weight}[0m
[34mdecoder_adapter.norm1.{bias, weight}[0m
[34mdecoder_adapter.norm2.{bias, weight}[0m
[10/09 12:07:45] d2.engine.train_loop INFO: Starting training from iteration 0
[10/09 12:08:10] d2.utils.events INFO:  eta: 0:17:50  iter: 19  total_loss: 30.72  loss_ce: 1.117  loss_mask: 0.2666  loss_dice: 1.532  loss_ce_0: 1.34  loss_mask_0: 0.3107  loss_dice_0: 1.878  loss_ce_1: 1.322  loss_mask_1: 0.2881  loss_dice_1: 1.688  loss_ce_2: 1.245  loss_mask_2: 0.2812  loss_dice_2: 1.613  loss_ce_3: 1.219  loss_mask_3: 0.2688  loss_dice_3: 1.546  loss_ce_4: 1.151  loss_mask_4: 0.2708  loss_dice_4: 1.586  loss_ce_5: 1.122  loss_mask_5: 0.2713  loss_dice_5: 1.545  loss_ce_6: 1.097  loss_mask_6: 0.2676  loss_dice_6: 1.514  loss_ce_7: 1.129  loss_mask_7: 0.2668  loss_dice_7: 1.567  loss_ce_8: 1.147  loss_mask_8: 0.2658  loss_dice_8: 1.514    time: 1.1023  last_time: 0.9975  data_time: 0.1129  last_data_time: 0.0639   lr: 0.0001  max_mem: 31770M
[10/09 12:08:32] d2.utils.events INFO:  eta: 0:17:38  iter: 39  total_loss: 26.06  loss_ce: 0.7997  loss_mask: 0.2134  loss_dice: 1.482  loss_ce_0: 0.9169  loss_mask_0: 0.25  loss_dice_0: 1.747  loss_ce_1: 0.9724  loss_mask_1: 0.2418  loss_dice_1: 1.63  loss_ce_2: 0.9006  loss_mask_2: 0.2239  loss_dice_2: 1.559  loss_ce_3: 0.8587  loss_mask_3: 0.2203  loss_dice_3: 1.474  loss_ce_4: 0.8264  loss_mask_4: 0.2157  loss_dice_4: 1.504  loss_ce_5: 0.8422  loss_mask_5: 0.2183  loss_dice_5: 1.516  loss_ce_6: 0.7906  loss_mask_6: 0.2152  loss_dice_6: 1.48  loss_ce_7: 0.8097  loss_mask_7: 0.2181  loss_dice_7: 1.484  loss_ce_8: 0.8166  loss_mask_8: 0.2201  loss_dice_8: 1.502    time: 1.1068  last_time: 1.1640  data_time: 0.0769  last_data_time: 0.0946   lr: 0.0001  max_mem: 31770M
[10/09 12:08:55] d2.utils.events INFO:  eta: 0:17:32  iter: 59  total_loss: 24.12  loss_ce: 0.6533  loss_mask: 0.2136  loss_dice: 1.437  loss_ce_0: 0.7311  loss_mask_0: 0.253  loss_dice_0: 1.736  loss_ce_1: 0.8687  loss_mask_1: 0.2368  loss_dice_1: 1.59  loss_ce_2: 0.8024  loss_mask_2: 0.2204  loss_dice_2: 1.503  loss_ce_3: 0.7691  loss_mask_3: 0.2139  loss_dice_3: 1.457  loss_ce_4: 0.7443  loss_mask_4: 0.2156  loss_dice_4: 1.439  loss_ce_5: 0.7132  loss_mask_5: 0.2176  loss_dice_5: 1.421  loss_ce_6: 0.7094  loss_mask_6: 0.2159  loss_dice_6: 1.377  loss_ce_7: 0.6831  loss_mask_7: 0.2146  loss_dice_7: 1.406  loss_ce_8: 0.6612  loss_mask_8: 0.2167  loss_dice_8: 1.417    time: 1.1136  last_time: 0.9999  data_time: 0.0803  last_data_time: 0.0693   lr: 0.0001  max_mem: 32315M
[10/09 12:09:17] d2.utils.events INFO:  eta: 0:17:02  iter: 79  total_loss: 23.35  loss_ce: 0.5942  loss_mask: 0.2156  loss_dice: 1.346  loss_ce_0: 0.6127  loss_mask_0: 0.2421  loss_dice_0: 1.605  loss_ce_1: 0.8  loss_mask_1: 0.2221  loss_dice_1: 1.496  loss_ce_2: 0.7234  loss_mask_2: 0.2071  loss_dice_2: 1.445  loss_ce_3: 0.6965  loss_mask_3: 0.2083  loss_dice_3: 1.409  loss_ce_4: 0.6135  loss_mask_4: 0.2108  loss_dice_4: 1.4  loss_ce_5: 0.653  loss_mask_5: 0.209  loss_dice_5: 1.38  loss_ce_6: 0.6002  loss_mask_6: 0.2134  loss_dice_6: 1.411  loss_ce_7: 0.6087  loss_mask_7: 0.2127  loss_dice_7: 1.345  loss_ce_8: 0.5975  loss_mask_8: 0.2164  loss_dice_8: 1.325    time: 1.1064  last_time: 1.0970  data_time: 0.0746  last_data_time: 0.0704   lr: 0.0001  max_mem: 32315M
[10/09 12:09:39] d2.utils.events INFO:  eta: 0:16:39  iter: 99  total_loss: 22.47  loss_ce: 0.5905  loss_mask: 0.2088  loss_dice: 1.261  loss_ce_0: 0.6646  loss_mask_0: 0.2415  loss_dice_0: 1.626  loss_ce_1: 0.8043  loss_mask_1: 0.225  loss_dice_1: 1.474  loss_ce_2: 0.7615  loss_mask_2: 0.2187  loss_dice_2: 1.396  loss_ce_3: 0.6812  loss_mask_3: 0.2101  loss_dice_3: 1.335  loss_ce_4: 0.6376  loss_mask_4: 0.2117  loss_dice_4: 1.294  loss_ce_5: 0.6242  loss_mask_5: 0.2115  loss_dice_5: 1.306  loss_ce_6: 0.6077  loss_mask_6: 0.2104  loss_dice_6: 1.322  loss_ce_7: 0.6072  loss_mask_7: 0.21  loss_dice_7: 1.314  loss_ce_8: 0.6145  loss_mask_8: 0.2098  loss_dice_8: 1.305    time: 1.1053  last_time: 1.0847  data_time: 0.0761  last_data_time: 0.0700   lr: 0.0001  max_mem: 32315M
[10/09 12:10:01] d2.utils.events INFO:  eta: 0:16:19  iter: 119  total_loss: 23.36  loss_ce: 0.6271  loss_mask: 0.1864  loss_dice: 1.415  loss_ce_0: 0.672  loss_mask_0: 0.2159  loss_dice_0: 1.771  loss_ce_1: 0.8379  loss_mask_1: 0.2028  loss_dice_1: 1.624  loss_ce_2: 0.7391  loss_mask_2: 0.1933  loss_dice_2: 1.511  loss_ce_3: 0.7009  loss_mask_3: 0.1883  loss_dice_3: 1.478  loss_ce_4: 0.675  loss_mask_4: 0.1908  loss_dice_4: 1.455  loss_ce_5: 0.6528  loss_mask_5: 0.1913  loss_dice_5: 1.455  loss_ce_6: 0.6488  loss_mask_6: 0.1886  loss_dice_6: 1.406  loss_ce_7: 0.6307  loss_mask_7: 0.1894  loss_dice_7: 1.405  loss_ce_8: 0.6381  loss_mask_8: 0.1867  loss_dice_8: 1.398    time: 1.1099  last_time: 1.1050  data_time: 0.0787  last_data_time: 0.0896   lr: 0.0001  max_mem: 32784M
[10/09 12:10:24] d2.utils.events INFO:  eta: 0:15:56  iter: 139  total_loss: 22.98  loss_ce: 0.6417  loss_mask: 0.2062  loss_dice: 1.358  loss_ce_0: 0.6569  loss_mask_0: 0.236  loss_dice_0: 1.621  loss_ce_1: 0.8086  loss_mask_1: 0.2069  loss_dice_1: 1.525  loss_ce_2: 0.7354  loss_mask_2: 0.202  loss_dice_2: 1.447  loss_ce_3: 0.6884  loss_mask_3: 0.1973  loss_dice_3: 1.367  loss_ce_4: 0.6559  loss_mask_4: 0.2032  loss_dice_4: 1.392  loss_ce_5: 0.6553  loss_mask_5: 0.1974  loss_dice_5: 1.385  loss_ce_6: 0.6551  loss_mask_6: 0.2074  loss_dice_6: 1.322  loss_ce_7: 0.6733  loss_mask_7: 0.2051  loss_dice_7: 1.345  loss_ce_8: 0.6645  loss_mask_8: 0.2048  loss_dice_8: 1.364    time: 1.1092  last_time: 1.1326  data_time: 0.0754  last_data_time: 0.0726   lr: 0.0001  max_mem: 32784M
[10/09 12:10:45] d2.utils.events INFO:  eta: 0:15:32  iter: 159  total_loss: 22.33  loss_ce: 0.5894  loss_mask: 0.1878  loss_dice: 1.351  loss_ce_0: 0.6607  loss_mask_0: 0.2174  loss_dice_0: 1.609  loss_ce_1: 0.7283  loss_mask_1: 0.2005  loss_dice_1: 1.532  loss_ce_2: 0.6969  loss_mask_2: 0.1907  loss_dice_2: 1.444  loss_ce_3: 0.6647  loss_mask_3: 0.1931  loss_dice_3: 1.392  loss_ce_4: 0.5934  loss_mask_4: 0.1913  loss_dice_4: 1.39  loss_ce_5: 0.5908  loss_mask_5: 0.1893  loss_dice_5: 1.376  loss_ce_6: 0.6019  loss_mask_6: 0.1883  loss_dice_6: 1.326  loss_ce_7: 0.6104  loss_mask_7: 0.1882  loss_dice_7: 1.359  loss_ce_8: 0.5983  loss_mask_8: 0.1878  loss_dice_8: 1.349    time: 1.1065  last_time: 1.0688  data_time: 0.0746  last_data_time: 0.0802   lr: 0.0001  max_mem: 32784M
[10/09 12:11:07] d2.utils.events INFO:  eta: 0:15:05  iter: 179  total_loss: 21.42  loss_ce: 0.5386  loss_mask: 0.1914  loss_dice: 1.26  loss_ce_0: 0.644  loss_mask_0: 0.223  loss_dice_0: 1.538  loss_ce_1: 0.71  loss_mask_1: 0.2033  loss_dice_1: 1.403  loss_ce_2: 0.6509  loss_mask_2: 0.196  loss_dice_2: 1.382  loss_ce_3: 0.6172  loss_mask_3: 0.1964  loss_dice_3: 1.326  loss_ce_4: 0.6168  loss_mask_4: 0.1969  loss_dice_4: 1.294  loss_ce_5: 0.5916  loss_mask_5: 0.1932  loss_dice_5: 1.284  loss_ce_6: 0.5626  loss_mask_6: 0.1929  loss_dice_6: 1.271  loss_ce_7: 0.5908  loss_mask_7: 0.1944  loss_dice_7: 1.241  loss_ce_8: 0.5808  loss_mask_8: 0.1928  loss_dice_8: 1.255    time: 1.1039  last_time: 1.1005  data_time: 0.0688  last_data_time: 0.0681   lr: 0.0001  max_mem: 32784M
[10/09 12:11:29] d2.utils.events INFO:  eta: 0:14:44  iter: 199  total_loss: 22.79  loss_ce: 0.5779  loss_mask: 0.1932  loss_dice: 1.346  loss_ce_0: 0.5896  loss_mask_0: 0.215  loss_dice_0: 1.622  loss_ce_1: 0.6928  loss_mask_1: 0.2085  loss_dice_1: 1.515  loss_ce_2: 0.6639  loss_mask_2: 0.1966  loss_dice_2: 1.436  loss_ce_3: 0.6181  loss_mask_3: 0.1959  loss_dice_3: 1.377  loss_ce_4: 0.6017  loss_mask_4: 0.1946  loss_dice_4: 1.373  loss_ce_5: 0.5802  loss_mask_5: 0.1894  loss_dice_5: 1.384  loss_ce_6: 0.5671  loss_mask_6: 0.1967  loss_dice_6: 1.36  loss_ce_7: 0.5889  loss_mask_7: 0.1956  loss_dice_7: 1.316  loss_ce_8: 0.6039  loss_mask_8: 0.1913  loss_dice_8: 1.337    time: 1.1050  last_time: 1.1608  data_time: 0.0751  last_data_time: 0.0857   lr: 0.0001  max_mem: 32784M
[10/09 12:11:52] d2.utils.events INFO:  eta: 0:14:22  iter: 219  total_loss: 22.07  loss_ce: 0.5479  loss_mask: 0.2063  loss_dice: 1.265  loss_ce_0: 0.6386  loss_mask_0: 0.221  loss_dice_0: 1.522  loss_ce_1: 0.7202  loss_mask_1: 0.2078  loss_dice_1: 1.413  loss_ce_2: 0.6753  loss_mask_2: 0.208  loss_dice_2: 1.413  loss_ce_3: 0.6187  loss_mask_3: 0.2095  loss_dice_3: 1.305  loss_ce_4: 0.5688  loss_mask_4: 0.2069  loss_dice_4: 1.34  loss_ce_5: 0.5904  loss_mask_5: 0.2083  loss_dice_5: 1.302  loss_ce_6: 0.6031  loss_mask_6: 0.2112  loss_dice_6: 1.28  loss_ce_7: 0.5858  loss_mask_7: 0.2066  loss_dice_7: 1.283  loss_ce_8: 0.6028  loss_mask_8: 0.2071  loss_dice_8: 1.269    time: 1.1057  last_time: 1.0815  data_time: 0.0765  last_data_time: 0.0761   lr: 0.0001  max_mem: 32784M
[10/09 12:12:14] d2.utils.events INFO:  eta: 0:14:02  iter: 239  total_loss: 22.01  loss_ce: 0.5665  loss_mask: 0.1896  loss_dice: 1.322  loss_ce_0: 0.6365  loss_mask_0: 0.2273  loss_dice_0: 1.538  loss_ce_1: 0.7068  loss_mask_1: 0.2077  loss_dice_1: 1.478  loss_ce_2: 0.6255  loss_mask_2: 0.2003  loss_dice_2: 1.377  loss_ce_3: 0.5876  loss_mask_3: 0.1957  loss_dice_3: 1.343  loss_ce_4: 0.5742  loss_mask_4: 0.1972  loss_dice_4: 1.359  loss_ce_5: 0.5779  loss_mask_5: 0.1903  loss_dice_5: 1.332  loss_ce_6: 0.5642  loss_mask_6: 0.1917  loss_dice_6: 1.289  loss_ce_7: 0.5555  loss_mask_7: 0.1902  loss_dice_7: 1.291  loss_ce_8: 0.5528  loss_mask_8: 0.1892  loss_dice_8: 1.304    time: 1.1064  last_time: 1.1154  data_time: 0.0777  last_data_time: 0.0852   lr: 0.0001  max_mem: 32784M
[10/09 12:12:36] d2.utils.events INFO:  eta: 0:13:41  iter: 259  total_loss: 22.29  loss_ce: 0.5514  loss_mask: 0.1954  loss_dice: 1.291  loss_ce_0: 0.6888  loss_mask_0: 0.2261  loss_dice_0: 1.555  loss_ce_1: 0.7435  loss_mask_1: 0.2116  loss_dice_1: 1.453  loss_ce_2: 0.7086  loss_mask_2: 0.2082  loss_dice_2: 1.404  loss_ce_3: 0.6217  loss_mask_3: 0.2022  loss_dice_3: 1.318  loss_ce_4: 0.6084  loss_mask_4: 0.1982  loss_dice_4: 1.326  loss_ce_5: 0.624  loss_mask_5: 0.1965  loss_dice_5: 1.336  loss_ce_6: 0.6137  loss_mask_6: 0.1963  loss_dice_6: 1.281  loss_ce_7: 0.5646  loss_mask_7: 0.1952  loss_dice_7: 1.32  loss_ce_8: 0.6085  loss_mask_8: 0.1947  loss_dice_8: 1.324    time: 1.1069  last_time: 1.0287  data_time: 0.0828  last_data_time: 0.0607   lr: 0.0001  max_mem: 32784M
[10/09 12:12:59] d2.utils.events INFO:  eta: 0:13:18  iter: 279  total_loss: 20.62  loss_ce: 0.5206  loss_mask: 0.1853  loss_dice: 1.234  loss_ce_0: 0.6095  loss_mask_0: 0.2124  loss_dice_0: 1.483  loss_ce_1: 0.6908  loss_mask_1: 0.2067  loss_dice_1: 1.368  loss_ce_2: 0.6484  loss_mask_2: 0.1981  loss_dice_2: 1.292  loss_ce_3: 0.5979  loss_mask_3: 0.1894  loss_dice_3: 1.252  loss_ce_4: 0.5615  loss_mask_4: 0.1893  loss_dice_4: 1.251  loss_ce_5: 0.5656  loss_mask_5: 0.1838  loss_dice_5: 1.266  loss_ce_6: 0.5321  loss_mask_6: 0.1854  loss_dice_6: 1.239  loss_ce_7: 0.5254  loss_mask_7: 0.1853  loss_dice_7: 1.224  loss_ce_8: 0.5616  loss_mask_8: 0.1861  loss_dice_8: 1.218    time: 1.1073  last_time: 1.2328  data_time: 0.0789  last_data_time: 0.0875   lr: 0.0001  max_mem: 32784M
[10/09 12:13:21] d2.utils.events INFO:  eta: 0:12:57  iter: 299  total_loss: 22.23  loss_ce: 0.606  loss_mask: 0.1924  loss_dice: 1.285  loss_ce_0: 0.5732  loss_mask_0: 0.2225  loss_dice_0: 1.596  loss_ce_1: 0.7013  loss_mask_1: 0.2056  loss_dice_1: 1.411  loss_ce_2: 0.6764  loss_mask_2: 0.1975  loss_dice_2: 1.388  loss_ce_3: 0.6268  loss_mask_3: 0.1947  loss_dice_3: 1.362  loss_ce_4: 0.6022  loss_mask_4: 0.1924  loss_dice_4: 1.351  loss_ce_5: 0.5911  loss_mask_5: 0.1908  loss_dice_5: 1.344  loss_ce_6: 0.5784  loss_mask_6: 0.1929  loss_dice_6: 1.312  loss_ce_7: 0.5767  loss_mask_7: 0.1916  loss_dice_7: 1.315  loss_ce_8: 0.5812  loss_mask_8: 0.1921  loss_dice_8: 1.308    time: 1.1077  last_time: 1.1143  data_time: 0.0749  last_data_time: 0.0858   lr: 0.0001  max_mem: 32784M
[10/09 12:13:44] d2.utils.events INFO:  eta: 0:12:35  iter: 319  total_loss: 22.6  loss_ce: 0.5986  loss_mask: 0.1759  loss_dice: 1.342  loss_ce_0: 0.6693  loss_mask_0: 0.1985  loss_dice_0: 1.584  loss_ce_1: 0.7305  loss_mask_1: 0.1887  loss_dice_1: 1.515  loss_ce_2: 0.6347  loss_mask_2: 0.1796  loss_dice_2: 1.463  loss_ce_3: 0.6292  loss_mask_3: 0.1767  loss_dice_3: 1.393  loss_ce_4: 0.603  loss_mask_4: 0.175  loss_dice_4: 1.387  loss_ce_5: 0.6171  loss_mask_5: 0.1756  loss_dice_5: 1.373  loss_ce_6: 0.582  loss_mask_6: 0.1764  loss_dice_6: 1.371  loss_ce_7: 0.5989  loss_mask_7: 0.178  loss_dice_7: 1.341  loss_ce_8: 0.56  loss_mask_8: 0.1762  loss_dice_8: 1.374    time: 1.1089  last_time: 1.0328  data_time: 0.0796  last_data_time: 0.0742   lr: 0.0001  max_mem: 32784M
[10/09 12:14:06] d2.utils.events INFO:  eta: 0:12:13  iter: 339  total_loss: 20.63  loss_ce: 0.5241  loss_mask: 0.2007  loss_dice: 1.222  loss_ce_0: 0.5979  loss_mask_0: 0.2326  loss_dice_0: 1.513  loss_ce_1: 0.7063  loss_mask_1: 0.2095  loss_dice_1: 1.36  loss_ce_2: 0.6512  loss_mask_2: 0.2001  loss_dice_2: 1.277  loss_ce_3: 0.581  loss_mask_3: 0.203  loss_dice_3: 1.249  loss_ce_4: 0.5599  loss_mask_4: 0.2059  loss_dice_4: 1.249  loss_ce_5: 0.5677  loss_mask_5: 0.2017  loss_dice_5: 1.232  loss_ce_6: 0.5882  loss_mask_6: 0.2012  loss_dice_6: 1.226  loss_ce_7: 0.561  loss_mask_7: 0.1998  loss_dice_7: 1.216  loss_ce_8: 0.5565  loss_mask_8: 0.1996  loss_dice_8: 1.223    time: 1.1082  last_time: 1.0884  data_time: 0.0776  last_data_time: 0.0603   lr: 0.0001  max_mem: 32784M
[10/09 12:14:28] d2.utils.events INFO:  eta: 0:11:51  iter: 359  total_loss: 21.81  loss_ce: 0.5988  loss_mask: 0.1802  loss_dice: 1.301  loss_ce_0: 0.6558  loss_mask_0: 0.2133  loss_dice_0: 1.529  loss_ce_1: 0.7638  loss_mask_1: 0.1932  loss_dice_1: 1.452  loss_ce_2: 0.6861  loss_mask_2: 0.1835  loss_dice_2: 1.353  loss_ce_3: 0.6863  loss_mask_3: 0.1848  loss_dice_3: 1.356  loss_ce_4: 0.6311  loss_mask_4: 0.1818  loss_dice_4: 1.349  loss_ce_5: 0.6022  loss_mask_5: 0.1814  loss_dice_5: 1.291  loss_ce_6: 0.6126  loss_mask_6: 0.1818  loss_dice_6: 1.288  loss_ce_7: 0.6266  loss_mask_7: 0.1822  loss_dice_7: 1.266  loss_ce_8: 0.5699  loss_mask_8: 0.1802  loss_dice_8: 1.304    time: 1.1092  last_time: 1.0930  data_time: 0.0839  last_data_time: 0.0814   lr: 0.0001  max_mem: 32784M
[10/09 12:14:50] d2.utils.events INFO:  eta: 0:11:29  iter: 379  total_loss: 20.28  loss_ce: 0.5517  loss_mask: 0.2017  loss_dice: 1.183  loss_ce_0: 0.6339  loss_mask_0: 0.2333  loss_dice_0: 1.414  loss_ce_1: 0.6684  loss_mask_1: 0.2102  loss_dice_1: 1.337  loss_ce_2: 0.6132  loss_mask_2: 0.2072  loss_dice_2: 1.274  loss_ce_3: 0.5774  loss_mask_3: 0.2044  loss_dice_3: 1.234  loss_ce_4: 0.5541  loss_mask_4: 0.2065  loss_dice_4: 1.187  loss_ce_5: 0.5512  loss_mask_5: 0.2014  loss_dice_5: 1.196  loss_ce_6: 0.5092  loss_mask_6: 0.2061  loss_dice_6: 1.195  loss_ce_7: 0.5245  loss_mask_7: 0.2038  loss_dice_7: 1.215  loss_ce_8: 0.5296  loss_mask_8: 0.2027  loss_dice_8: 1.222    time: 1.1088  last_time: 1.0850  data_time: 0.0741  last_data_time: 0.0796   lr: 0.0001  max_mem: 32784M
[10/09 12:15:13] d2.utils.events INFO:  eta: 0:11:06  iter: 399  total_loss: 20.31  loss_ce: 0.4896  loss_mask: 0.1709  loss_dice: 1.272  loss_ce_0: 0.6001  loss_mask_0: 0.1895  loss_dice_0: 1.487  loss_ce_1: 0.6844  loss_mask_1: 0.1815  loss_dice_1: 1.418  loss_ce_2: 0.5925  loss_mask_2: 0.1701  loss_dice_2: 1.336  loss_ce_3: 0.5668  loss_mask_3: 0.1748  loss_dice_3: 1.288  loss_ce_4: 0.5475  loss_mask_4: 0.1723  loss_dice_4: 1.276  loss_ce_5: 0.5005  loss_mask_5: 0.1699  loss_dice_5: 1.274  loss_ce_6: 0.5231  loss_mask_6: 0.1713  loss_dice_6: 1.26  loss_ce_7: 0.5305  loss_mask_7: 0.1706  loss_dice_7: 1.234  loss_ce_8: 0.5185  loss_mask_8: 0.1682  loss_dice_8: 1.243    time: 1.1097  last_time: 1.1491  data_time: 0.0827  last_data_time: 0.0887   lr: 0.0001  max_mem: 32784M
[10/09 12:15:35] d2.utils.events INFO:  eta: 0:10:44  iter: 419  total_loss: 21.26  loss_ce: 0.5367  loss_mask: 0.1804  loss_dice: 1.272  loss_ce_0: 0.6205  loss_mask_0: 0.2002  loss_dice_0: 1.507  loss_ce_1: 0.6902  loss_mask_1: 0.1922  loss_dice_1: 1.401  loss_ce_2: 0.6199  loss_mask_2: 0.1818  loss_dice_2: 1.362  loss_ce_3: 0.553  loss_mask_3: 0.1811  loss_dice_3: 1.302  loss_ce_4: 0.5817  loss_mask_4: 0.1803  loss_dice_4: 1.315  loss_ce_5: 0.585  loss_mask_5: 0.1812  loss_dice_5: 1.285  loss_ce_6: 0.5487  loss_mask_6: 0.1801  loss_dice_6: 1.302  loss_ce_7: 0.544  loss_mask_7: 0.1804  loss_dice_7: 1.256  loss_ce_8: 0.5441  loss_mask_8: 0.1798  loss_dice_8: 1.251    time: 1.1101  last_time: 1.0821  data_time: 0.0820  last_data_time: 0.0705   lr: 0.0001  max_mem: 32784M
[10/09 12:15:58] d2.utils.events INFO:  eta: 0:10:22  iter: 439  total_loss: 21.85  loss_ce: 0.5406  loss_mask: 0.1723  loss_dice: 1.312  loss_ce_0: 0.6186  loss_mask_0: 0.1983  loss_dice_0: 1.589  loss_ce_1: 0.7121  loss_mask_1: 0.1828  loss_dice_1: 1.507  loss_ce_2: 0.6265  loss_mask_2: 0.1722  loss_dice_2: 1.416  loss_ce_3: 0.6056  loss_mask_3: 0.1738  loss_dice_3: 1.374  loss_ce_4: 0.5803  loss_mask_4: 0.1714  loss_dice_4: 1.355  loss_ce_5: 0.5485  loss_mask_5: 0.1725  loss_dice_5: 1.368  loss_ce_6: 0.5513  loss_mask_6: 0.1726  loss_dice_6: 1.325  loss_ce_7: 0.5803  loss_mask_7: 0.1715  loss_dice_7: 1.305  loss_ce_8: 0.5574  loss_mask_8: 0.1711  loss_dice_8: 1.334    time: 1.1111  last_time: 1.1834  data_time: 0.0832  last_data_time: 0.0773   lr: 0.0001  max_mem: 32784M
[10/09 12:16:20] d2.utils.events INFO:  eta: 0:10:00  iter: 459  total_loss: 20.89  loss_ce: 0.5217  loss_mask: 0.1743  loss_dice: 1.297  loss_ce_0: 0.5803  loss_mask_0: 0.2042  loss_dice_0: 1.507  loss_ce_1: 0.6945  loss_mask_1: 0.1991  loss_dice_1: 1.458  loss_ce_2: 0.5759  loss_mask_2: 0.1816  loss_dice_2: 1.377  loss_ce_3: 0.5523  loss_mask_3: 0.1771  loss_dice_3: 1.316  loss_ce_4: 0.5378  loss_mask_4: 0.1765  loss_dice_4: 1.31  loss_ce_5: 0.5136  loss_mask_5: 0.1773  loss_dice_5: 1.319  loss_ce_6: 0.5326  loss_mask_6: 0.176  loss_dice_6: 1.288  loss_ce_7: 0.5139  loss_mask_7: 0.1778  loss_dice_7: 1.284  loss_ce_8: 0.5291  loss_mask_8: 0.1762  loss_dice_8: 1.297    time: 1.1107  last_time: 1.1875  data_time: 0.0751  last_data_time: 0.0804   lr: 0.0001  max_mem: 32784M
[10/09 12:16:42] d2.utils.events INFO:  eta: 0:09:38  iter: 479  total_loss: 20.53  loss_ce: 0.5088  loss_mask: 0.1814  loss_dice: 1.222  loss_ce_0: 0.5713  loss_mask_0: 0.2044  loss_dice_0: 1.468  loss_ce_1: 0.6794  loss_mask_1: 0.1972  loss_dice_1: 1.374  loss_ce_2: 0.5876  loss_mask_2: 0.1851  loss_dice_2: 1.319  loss_ce_3: 0.5467  loss_mask_3: 0.1848  loss_dice_3: 1.259  loss_ce_4: 0.5305  loss_mask_4: 0.1827  loss_dice_4: 1.264  loss_ce_5: 0.4984  loss_mask_5: 0.1824  loss_dice_5: 1.27  loss_ce_6: 0.525  loss_mask_6: 0.1824  loss_dice_6: 1.245  loss_ce_7: 0.5054  loss_mask_7: 0.1837  loss_dice_7: 1.247  loss_ce_8: 0.5048  loss_mask_8: 0.1813  loss_dice_8: 1.211    time: 1.1110  last_time: 1.1322  data_time: 0.0756  last_data_time: 0.0779   lr: 0.0001  max_mem: 32784M
[10/09 12:17:05] d2.utils.events INFO:  eta: 0:09:16  iter: 499  total_loss: 21.72  loss_ce: 0.5361  loss_mask: 0.1653  loss_dice: 1.35  loss_ce_0: 0.5988  loss_mask_0: 0.1958  loss_dice_0: 1.557  loss_ce_1: 0.7013  loss_mask_1: 0.1833  loss_dice_1: 1.471  loss_ce_2: 0.6113  loss_mask_2: 0.1764  loss_dice_2: 1.381  loss_ce_3: 0.5762  loss_mask_3: 0.1687  loss_dice_3: 1.374  loss_ce_4: 0.5568  loss_mask_4: 0.1676  loss_dice_4: 1.33  loss_ce_5: 0.5774  loss_mask_5: 0.1654  loss_dice_5: 1.35  loss_ce_6: 0.5406  loss_mask_6: 0.1662  loss_dice_6: 1.337  loss_ce_7: 0.5701  loss_mask_7: 0.1659  loss_dice_7: 1.308  loss_ce_8: 0.5644  loss_mask_8: 0.1645  loss_dice_8: 1.305    time: 1.1112  last_time: 1.0302  data_time: 0.0783  last_data_time: 0.0663   lr: 0.0001  max_mem: 32784M
[10/09 12:17:27] d2.utils.events INFO:  eta: 0:08:54  iter: 519  total_loss: 20.72  loss_ce: 0.5269  loss_mask: 0.1758  loss_dice: 1.222  loss_ce_0: 0.5607  loss_mask_0: 0.2178  loss_dice_0: 1.516  loss_ce_1: 0.7397  loss_mask_1: 0.1971  loss_dice_1: 1.391  loss_ce_2: 0.6715  loss_mask_2: 0.1912  loss_dice_2: 1.332  loss_ce_3: 0.5886  loss_mask_3: 0.1768  loss_dice_3: 1.241  loss_ce_4: 0.5728  loss_mask_4: 0.1741  loss_dice_4: 1.242  loss_ce_5: 0.5578  loss_mask_5: 0.1742  loss_dice_5: 1.261  loss_ce_6: 0.5454  loss_mask_6: 0.1765  loss_dice_6: 1.226  loss_ce_7: 0.5397  loss_mask_7: 0.1793  loss_dice_7: 1.241  loss_ce_8: 0.5259  loss_mask_8: 0.1752  loss_dice_8: 1.247    time: 1.1116  last_time: 1.1279  data_time: 0.0761  last_data_time: 0.0824   lr: 0.0001  max_mem: 32784M
[10/09 12:17:50] d2.utils.events INFO:  eta: 0:08:32  iter: 539  total_loss: 20.82  loss_ce: 0.5285  loss_mask: 0.1687  loss_dice: 1.244  loss_ce_0: 0.5989  loss_mask_0: 0.2056  loss_dice_0: 1.509  loss_ce_1: 0.7352  loss_mask_1: 0.1872  loss_dice_1: 1.424  loss_ce_2: 0.6221  loss_mask_2: 0.1752  loss_dice_2: 1.34  loss_ce_3: 0.5747  loss_mask_3: 0.1749  loss_dice_3: 1.278  loss_ce_4: 0.5367  loss_mask_4: 0.1732  loss_dice_4: 1.292  loss_ce_5: 0.5274  loss_mask_5: 0.169  loss_dice_5: 1.264  loss_ce_6: 0.513  loss_mask_6: 0.174  loss_dice_6: 1.228  loss_ce_7: 0.5556  loss_mask_7: 0.1668  loss_dice_7: 1.241  loss_ce_8: 0.5336  loss_mask_8: 0.1668  loss_dice_8: 1.251    time: 1.1118  last_time: 1.1207  data_time: 0.0775  last_data_time: 0.0685   lr: 0.0001  max_mem: 32784M
[10/09 12:18:12] d2.utils.events INFO:  eta: 0:08:10  iter: 559  total_loss: 20.77  loss_ce: 0.4821  loss_mask: 0.1824  loss_dice: 1.279  loss_ce_0: 0.5786  loss_mask_0: 0.2113  loss_dice_0: 1.546  loss_ce_1: 0.6499  loss_mask_1: 0.2017  loss_dice_1: 1.429  loss_ce_2: 0.6204  loss_mask_2: 0.186  loss_dice_2: 1.328  loss_ce_3: 0.5471  loss_mask_3: 0.1855  loss_dice_3: 1.303  loss_ce_4: 0.5247  loss_mask_4: 0.1851  loss_dice_4: 1.3  loss_ce_5: 0.5094  loss_mask_5: 0.1855  loss_dice_5: 1.31  loss_ce_6: 0.5319  loss_mask_6: 0.1819  loss_dice_6: 1.298  loss_ce_7: 0.5222  loss_mask_7: 0.1831  loss_dice_7: 1.293  loss_ce_8: 0.5095  loss_mask_8: 0.1814  loss_dice_8: 1.258    time: 1.1118  last_time: 1.1638  data_time: 0.0751  last_data_time: 0.0843   lr: 0.0001  max_mem: 32784M
[10/09 12:18:34] d2.utils.events INFO:  eta: 0:07:48  iter: 579  total_loss: 20.16  loss_ce: 0.5078  loss_mask: 0.1759  loss_dice: 1.219  loss_ce_0: 0.5906  loss_mask_0: 0.199  loss_dice_0: 1.528  loss_ce_1: 0.6401  loss_mask_1: 0.1802  loss_dice_1: 1.37  loss_ce_2: 0.5841  loss_mask_2: 0.1762  loss_dice_2: 1.323  loss_ce_3: 0.5568  loss_mask_3: 0.1755  loss_dice_3: 1.262  loss_ce_4: 0.5474  loss_mask_4: 0.1739  loss_dice_4: 1.257  loss_ce_5: 0.5109  loss_mask_5: 0.1777  loss_dice_5: 1.259  loss_ce_6: 0.5331  loss_mask_6: 0.1747  loss_dice_6: 1.248  loss_ce_7: 0.547  loss_mask_7: 0.1755  loss_dice_7: 1.248  loss_ce_8: 0.501  loss_mask_8: 0.1737  loss_dice_8: 1.222    time: 1.1116  last_time: 1.1550  data_time: 0.0782  last_data_time: 0.0922   lr: 0.0001  max_mem: 32784M
[10/09 12:18:56] d2.utils.events INFO:  eta: 0:07:25  iter: 599  total_loss: 21.04  loss_ce: 0.5272  loss_mask: 0.1653  loss_dice: 1.256  loss_ce_0: 0.5872  loss_mask_0: 0.1896  loss_dice_0: 1.534  loss_ce_1: 0.675  loss_mask_1: 0.1723  loss_dice_1: 1.435  loss_ce_2: 0.6318  loss_mask_2: 0.1663  loss_dice_2: 1.358  loss_ce_3: 0.5813  loss_mask_3: 0.1653  loss_dice_3: 1.343  loss_ce_4: 0.5798  loss_mask_4: 0.1664  loss_dice_4: 1.3  loss_ce_5: 0.5484  loss_mask_5: 0.1669  loss_dice_5: 1.343  loss_ce_6: 0.5081  loss_mask_6: 0.1646  loss_dice_6: 1.288  loss_ce_7: 0.5013  loss_mask_7: 0.1668  loss_dice_7: 1.271  loss_ce_8: 0.5051  loss_mask_8: 0.1669  loss_dice_8: 1.276    time: 1.1114  last_time: 1.1105  data_time: 0.0793  last_data_time: 0.0706   lr: 0.0001  max_mem: 32784M
[10/09 12:19:19] d2.utils.events INFO:  eta: 0:07:03  iter: 619  total_loss: 20.33  loss_ce: 0.4932  loss_mask: 0.1796  loss_dice: 1.225  loss_ce_0: 0.5595  loss_mask_0: 0.2153  loss_dice_0: 1.541  loss_ce_1: 0.6566  loss_mask_1: 0.1944  loss_dice_1: 1.401  loss_ce_2: 0.593  loss_mask_2: 0.1897  loss_dice_2: 1.356  loss_ce_3: 0.5563  loss_mask_3: 0.1885  loss_dice_3: 1.289  loss_ce_4: 0.5318  loss_mask_4: 0.1813  loss_dice_4: 1.242  loss_ce_5: 0.5266  loss_mask_5: 0.1846  loss_dice_5: 1.257  loss_ce_6: 0.5283  loss_mask_6: 0.179  loss_dice_6: 1.26  loss_ce_7: 0.5311  loss_mask_7: 0.1773  loss_dice_7: 1.268  loss_ce_8: 0.5186  loss_mask_8: 0.18  loss_dice_8: 1.225    time: 1.1121  last_time: 1.2253  data_time: 0.0792  last_data_time: 0.0902   lr: 0.0001  max_mem: 32784M
[10/09 12:19:41] d2.utils.events INFO:  eta: 0:06:41  iter: 639  total_loss: 20.24  loss_ce: 0.4782  loss_mask: 0.1932  loss_dice: 1.236  loss_ce_0: 0.5616  loss_mask_0: 0.2236  loss_dice_0: 1.497  loss_ce_1: 0.6279  loss_mask_1: 0.2062  loss_dice_1: 1.364  loss_ce_2: 0.5901  loss_mask_2: 0.203  loss_dice_2: 1.325  loss_ce_3: 0.5376  loss_mask_3: 0.1994  loss_dice_3: 1.266  loss_ce_4: 0.5136  loss_mask_4: 0.196  loss_dice_4: 1.263  loss_ce_5: 0.4932  loss_mask_5: 0.1954  loss_dice_5: 1.297  loss_ce_6: 0.4495  loss_mask_6: 0.193  loss_dice_6: 1.251  loss_ce_7: 0.4935  loss_mask_7: 0.1912  loss_dice_7: 1.236  loss_ce_8: 0.4792  loss_mask_8: 0.1935  loss_dice_8: 1.233    time: 1.1119  last_time: 1.1068  data_time: 0.0773  last_data_time: 0.0852   lr: 0.0001  max_mem: 32784M
[10/09 12:20:03] d2.utils.events INFO:  eta: 0:06:18  iter: 659  total_loss: 22.19  loss_ce: 0.5516  loss_mask: 0.1653  loss_dice: 1.329  loss_ce_0: 0.614  loss_mask_0: 0.1896  loss_dice_0: 1.633  loss_ce_1: 0.6836  loss_mask_1: 0.1776  loss_dice_1: 1.511  loss_ce_2: 0.6516  loss_mask_2: 0.1658  loss_dice_2: 1.427  loss_ce_3: 0.5707  loss_mask_3: 0.1633  loss_dice_3: 1.414  loss_ce_4: 0.5577  loss_mask_4: 0.1638  loss_dice_4: 1.343  loss_ce_5: 0.5541  loss_mask_5: 0.1625  loss_dice_5: 1.345  loss_ce_6: 0.5191  loss_mask_6: 0.1649  loss_dice_6: 1.369  loss_ce_7: 0.5328  loss_mask_7: 0.1642  loss_dice_7: 1.353  loss_ce_8: 0.5516  loss_mask_8: 0.1609  loss_dice_8: 1.375    time: 1.1115  last_time: 1.1131  data_time: 0.0729  last_data_time: 0.0729   lr: 0.0001  max_mem: 32784M
[10/09 12:20:25] d2.utils.events INFO:  eta: 0:05:56  iter: 679  total_loss: 20.1  loss_ce: 0.4925  loss_mask: 0.1796  loss_dice: 1.248  loss_ce_0: 0.5999  loss_mask_0: 0.2023  loss_dice_0: 1.462  loss_ce_1: 0.6592  loss_mask_1: 0.1837  loss_dice_1: 1.397  loss_ce_2: 0.5824  loss_mask_2: 0.1792  loss_dice_2: 1.331  loss_ce_3: 0.5534  loss_mask_3: 0.1807  loss_dice_3: 1.283  loss_ce_4: 0.523  loss_mask_4: 0.1807  loss_dice_4: 1.275  loss_ce_5: 0.525  loss_mask_5: 0.18  loss_dice_5: 1.266  loss_ce_6: 0.5188  loss_mask_6: 0.1793  loss_dice_6: 1.247  loss_ce_7: 0.5068  loss_mask_7: 0.1782  loss_dice_7: 1.242  loss_ce_8: 0.5204  loss_mask_8: 0.1797  loss_dice_8: 1.248    time: 1.1107  last_time: 1.0527  data_time: 0.0725  last_data_time: 0.0725   lr: 0.0001  max_mem: 32784M
[10/09 12:20:47] d2.utils.events INFO:  eta: 0:05:33  iter: 699  total_loss: 20.8  loss_ce: 0.5191  loss_mask: 0.1702  loss_dice: 1.283  loss_ce_0: 0.6572  loss_mask_0: 0.1991  loss_dice_0: 1.485  loss_ce_1: 0.6888  loss_mask_1: 0.1785  loss_dice_1: 1.42  loss_ce_2: 0.6164  loss_mask_2: 0.1734  loss_dice_2: 1.325  loss_ce_3: 0.5916  loss_mask_3: 0.1708  loss_dice_3: 1.289  loss_ce_4: 0.5464  loss_mask_4: 0.1748  loss_dice_4: 1.32  loss_ce_5: 0.553  loss_mask_5: 0.1739  loss_dice_5: 1.336  loss_ce_6: 0.5293  loss_mask_6: 0.1702  loss_dice_6: 1.308  loss_ce_7: 0.5017  loss_mask_7: 0.1742  loss_dice_7: 1.279  loss_ce_8: 0.525  loss_mask_8: 0.1717  loss_dice_8: 1.315    time: 1.1110  last_time: 1.1286  data_time: 0.0834  last_data_time: 0.0817   lr: 0.0001  max_mem: 32784M
[10/09 12:21:10] d2.utils.events INFO:  eta: 0:05:11  iter: 719  total_loss: 20.67  loss_ce: 0.503  loss_mask: 0.1834  loss_dice: 1.277  loss_ce_0: 0.6382  loss_mask_0: 0.2111  loss_dice_0: 1.496  loss_ce_1: 0.6814  loss_mask_1: 0.1993  loss_dice_1: 1.387  loss_ce_2: 0.5919  loss_mask_2: 0.1909  loss_dice_2: 1.345  loss_ce_3: 0.5665  loss_mask_3: 0.1854  loss_dice_3: 1.27  loss_ce_4: 0.5545  loss_mask_4: 0.1862  loss_dice_4: 1.295  loss_ce_5: 0.5493  loss_mask_5: 0.1796  loss_dice_5: 1.242  loss_ce_6: 0.5268  loss_mask_6: 0.1819  loss_dice_6: 1.249  loss_ce_7: 0.509  loss_mask_7: 0.181  loss_dice_7: 1.235  loss_ce_8: 0.5095  loss_mask_8: 0.1808  loss_dice_8: 1.25    time: 1.1106  last_time: 1.0572  data_time: 0.0806  last_data_time: 0.0676   lr: 0.0001  max_mem: 32784M
[10/09 12:21:31] d2.utils.events INFO:  eta: 0:04:49  iter: 739  total_loss: 20.34  loss_ce: 0.4738  loss_mask: 0.1827  loss_dice: 1.243  loss_ce_0: 0.5692  loss_mask_0: 0.2033  loss_dice_0: 1.494  loss_ce_1: 0.5924  loss_mask_1: 0.192  loss_dice_1: 1.363  loss_ce_2: 0.5842  loss_mask_2: 0.1876  loss_dice_2: 1.277  loss_ce_3: 0.5241  loss_mask_3: 0.1846  loss_dice_3: 1.242  loss_ce_4: 0.5118  loss_mask_4: 0.181  loss_dice_4: 1.24  loss_ce_5: 0.4842  loss_mask_5: 0.1808  loss_dice_5: 1.285  loss_ce_6: 0.4658  loss_mask_6: 0.1805  loss_dice_6: 1.238  loss_ce_7: 0.4847  loss_mask_7: 0.1805  loss_dice_7: 1.269  loss_ce_8: 0.4762  loss_mask_8: 0.1805  loss_dice_8: 1.249    time: 1.1097  last_time: 1.1595  data_time: 0.0719  last_data_time: 0.0891   lr: 0.0001  max_mem: 32784M
[10/09 12:21:53] d2.utils.events INFO:  eta: 0:04:26  iter: 759  total_loss: 20.37  loss_ce: 0.4819  loss_mask: 0.1823  loss_dice: 1.236  loss_ce_0: 0.591  loss_mask_0: 0.203  loss_dice_0: 1.484  loss_ce_1: 0.6522  loss_mask_1: 0.1975  loss_dice_1: 1.361  loss_ce_2: 0.6231  loss_mask_2: 0.183  loss_dice_2: 1.285  loss_ce_3: 0.5742  loss_mask_3: 0.1847  loss_dice_3: 1.248  loss_ce_4: 0.5094  loss_mask_4: 0.1894  loss_dice_4: 1.273  loss_ce_5: 0.5108  loss_mask_5: 0.1862  loss_dice_5: 1.272  loss_ce_6: 0.5256  loss_mask_6: 0.1859  loss_dice_6: 1.19  loss_ce_7: 0.4978  loss_mask_7: 0.185  loss_dice_7: 1.261  loss_ce_8: 0.497  loss_mask_8: 0.1816  loss_dice_8: 1.215    time: 1.1093  last_time: 1.1227  data_time: 0.0730  last_data_time: 0.0786   lr: 0.0001  max_mem: 32784M
[10/09 12:22:15] d2.utils.events INFO:  eta: 0:04:04  iter: 779  total_loss: 20.21  loss_ce: 0.4784  loss_mask: 0.1782  loss_dice: 1.206  loss_ce_0: 0.5893  loss_mask_0: 0.2097  loss_dice_0: 1.499  loss_ce_1: 0.6652  loss_mask_1: 0.1893  loss_dice_1: 1.408  loss_ce_2: 0.5814  loss_mask_2: 0.1835  loss_dice_2: 1.331  loss_ce_3: 0.5316  loss_mask_3: 0.1811  loss_dice_3: 1.263  loss_ce_4: 0.4956  loss_mask_4: 0.1817  loss_dice_4: 1.269  loss_ce_5: 0.5123  loss_mask_5: 0.1789  loss_dice_5: 1.247  loss_ce_6: 0.5161  loss_mask_6: 0.1796  loss_dice_6: 1.24  loss_ce_7: 0.5055  loss_mask_7: 0.1804  loss_dice_7: 1.252  loss_ce_8: 0.5064  loss_mask_8: 0.178  loss_dice_8: 1.252    time: 1.1093  last_time: 1.0843  data_time: 0.0781  last_data_time: 0.0703   lr: 0.0001  max_mem: 32784M
[10/09 12:22:38] d2.utils.events INFO:  eta: 0:03:42  iter: 799  total_loss: 20.65  loss_ce: 0.483  loss_mask: 0.1641  loss_dice: 1.291  loss_ce_0: 0.5904  loss_mask_0: 0.197  loss_dice_0: 1.558  loss_ce_1: 0.6775  loss_mask_1: 0.1875  loss_dice_1: 1.457  loss_ce_2: 0.6035  loss_mask_2: 0.1738  loss_dice_2: 1.382  loss_ce_3: 0.5182  loss_mask_3: 0.1708  loss_dice_3: 1.341  loss_ce_4: 0.5304  loss_mask_4: 0.1685  loss_dice_4: 1.331  loss_ce_5: 0.4963  loss_mask_5: 0.1668  loss_dice_5: 1.315  loss_ce_6: 0.5017  loss_mask_6: 0.164  loss_dice_6: 1.324  loss_ce_7: 0.5026  loss_mask_7: 0.1633  loss_dice_7: 1.282  loss_ce_8: 0.4805  loss_mask_8: 0.1675  loss_dice_8: 1.281    time: 1.1094  last_time: 1.0565  data_time: 0.0744  last_data_time: 0.0736   lr: 0.0001  max_mem: 32784M
[10/09 12:23:00] d2.utils.events INFO:  eta: 0:03:20  iter: 819  total_loss: 19.95  loss_ce: 0.4486  loss_mask: 0.1845  loss_dice: 1.229  loss_ce_0: 0.5533  loss_mask_0: 0.2173  loss_dice_0: 1.41  loss_ce_1: 0.6252  loss_mask_1: 0.2001  loss_dice_1: 1.323  loss_ce_2: 0.5585  loss_mask_2: 0.1948  loss_dice_2: 1.307  loss_ce_3: 0.5134  loss_mask_3: 0.1917  loss_dice_3: 1.26  loss_ce_4: 0.4859  loss_mask_4: 0.1857  loss_dice_4: 1.259  loss_ce_5: 0.4799  loss_mask_5: 0.1867  loss_dice_5: 1.276  loss_ce_6: 0.447  loss_mask_6: 0.1854  loss_dice_6: 1.255  loss_ce_7: 0.4531  loss_mask_7: 0.1846  loss_dice_7: 1.23  loss_ce_8: 0.4827  loss_mask_8: 0.1842  loss_dice_8: 1.228    time: 1.1094  last_time: 1.0928  data_time: 0.0735  last_data_time: 0.0751   lr: 0.0001  max_mem: 32784M
[10/09 12:23:22] d2.utils.events INFO:  eta: 0:02:57  iter: 839  total_loss: 19.75  loss_ce: 0.492  loss_mask: 0.1764  loss_dice: 1.191  loss_ce_0: 0.6062  loss_mask_0: 0.2005  loss_dice_0: 1.454  loss_ce_1: 0.648  loss_mask_1: 0.1858  loss_dice_1: 1.365  loss_ce_2: 0.5991  loss_mask_2: 0.1782  loss_dice_2: 1.324  loss_ce_3: 0.5193  loss_mask_3: 0.1754  loss_dice_3: 1.255  loss_ce_4: 0.5295  loss_mask_4: 0.1744  loss_dice_4: 1.248  loss_ce_5: 0.5091  loss_mask_5: 0.1744  loss_dice_5: 1.247  loss_ce_6: 0.4974  loss_mask_6: 0.1754  loss_dice_6: 1.244  loss_ce_7: 0.5236  loss_mask_7: 0.1766  loss_dice_7: 1.197  loss_ce_8: 0.4958  loss_mask_8: 0.1733  loss_dice_8: 1.203    time: 1.1088  last_time: 1.1443  data_time: 0.0729  last_data_time: 0.0843   lr: 0.0001  max_mem: 32784M
[10/09 12:23:44] d2.utils.events INFO:  eta: 0:02:35  iter: 859  total_loss: 20.2  loss_ce: 0.4871  loss_mask: 0.1743  loss_dice: 1.244  loss_ce_0: 0.5328  loss_mask_0: 0.1953  loss_dice_0: 1.475  loss_ce_1: 0.6622  loss_mask_1: 0.1934  loss_dice_1: 1.391  loss_ce_2: 0.5667  loss_mask_2: 0.1804  loss_dice_2: 1.371  loss_ce_3: 0.5222  loss_mask_3: 0.176  loss_dice_3: 1.304  loss_ce_4: 0.5044  loss_mask_4: 0.1768  loss_dice_4: 1.306  loss_ce_5: 0.5192  loss_mask_5: 0.1767  loss_dice_5: 1.31  loss_ce_6: 0.4813  loss_mask_6: 0.1749  loss_dice_6: 1.272  loss_ce_7: 0.4993  loss_mask_7: 0.1767  loss_dice_7: 1.274  loss_ce_8: 0.5246  loss_mask_8: 0.1733  loss_dice_8: 1.257    time: 1.1084  last_time: 1.1632  data_time: 0.0721  last_data_time: 0.0870   lr: 0.0001  max_mem: 32784M
[10/09 12:24:06] d2.utils.events INFO:  eta: 0:02:13  iter: 879  total_loss: 20.95  loss_ce: 0.5248  loss_mask: 0.1713  loss_dice: 1.258  loss_ce_0: 0.6448  loss_mask_0: 0.198  loss_dice_0: 1.475  loss_ce_1: 0.6895  loss_mask_1: 0.1804  loss_dice_1: 1.406  loss_ce_2: 0.5884  loss_mask_2: 0.1764  loss_dice_2: 1.354  loss_ce_3: 0.5538  loss_mask_3: 0.1767  loss_dice_3: 1.308  loss_ce_4: 0.511  loss_mask_4: 0.1756  loss_dice_4: 1.319  loss_ce_5: 0.5492  loss_mask_5: 0.1709  loss_dice_5: 1.306  loss_ce_6: 0.5281  loss_mask_6: 0.1729  loss_dice_6: 1.244  loss_ce_7: 0.5389  loss_mask_7: 0.1729  loss_dice_7: 1.267  loss_ce_8: 0.5387  loss_mask_8: 0.1714  loss_dice_8: 1.235    time: 1.1084  last_time: 1.1181  data_time: 0.0782  last_data_time: 0.0757   lr: 0.0001  max_mem: 32784M
[10/09 12:24:28] d2.utils.events INFO:  eta: 0:01:51  iter: 899  total_loss: 19.96  loss_ce: 0.5102  loss_mask: 0.1617  loss_dice: 1.204  loss_ce_0: 0.6049  loss_mask_0: 0.192  loss_dice_0: 1.422  loss_ce_1: 0.6947  loss_mask_1: 0.1737  loss_dice_1: 1.338  loss_ce_2: 0.5854  loss_mask_2: 0.1626  loss_dice_2: 1.304  loss_ce_3: 0.5398  loss_mask_3: 0.166  loss_dice_3: 1.269  loss_ce_4: 0.5176  loss_mask_4: 0.1621  loss_dice_4: 1.233  loss_ce_5: 0.5104  loss_mask_5: 0.1617  loss_dice_5: 1.243  loss_ce_6: 0.522  loss_mask_6: 0.163  loss_dice_6: 1.226  loss_ce_7: 0.523  loss_mask_7: 0.1618  loss_dice_7: 1.198  loss_ce_8: 0.5222  loss_mask_8: 0.1622  loss_dice_8: 1.186    time: 1.1079  last_time: 1.0528  data_time: 0.0750  last_data_time: 0.0676   lr: 0.0001  max_mem: 32784M
[10/09 12:24:50] d2.utils.events INFO:  eta: 0:01:28  iter: 919  total_loss: 20.65  loss_ce: 0.5024  loss_mask: 0.1681  loss_dice: 1.252  loss_ce_0: 0.5874  loss_mask_0: 0.1924  loss_dice_0: 1.489  loss_ce_1: 0.6348  loss_mask_1: 0.182  loss_dice_1: 1.411  loss_ce_2: 0.5861  loss_mask_2: 0.1761  loss_dice_2: 1.357  loss_ce_3: 0.5444  loss_mask_3: 0.18  loss_dice_3: 1.301  loss_ce_4: 0.5024  loss_mask_4: 0.174  loss_dice_4: 1.301  loss_ce_5: 0.541  loss_mask_5: 0.17  loss_dice_5: 1.299  loss_ce_6: 0.5052  loss_mask_6: 0.1736  loss_dice_6: 1.291  loss_ce_7: 0.4964  loss_mask_7: 0.1705  loss_dice_7: 1.268  loss_ce_8: 0.5202  loss_mask_8: 0.1682  loss_dice_8: 1.27    time: 1.1078  last_time: 1.1521  data_time: 0.0698  last_data_time: 0.0820   lr: 0.0001  max_mem: 32784M
[10/09 12:25:12] d2.utils.events INFO:  eta: 0:01:06  iter: 939  total_loss: 19.98  loss_ce: 0.4988  loss_mask: 0.1644  loss_dice: 1.223  loss_ce_0: 0.5579  loss_mask_0: 0.1889  loss_dice_0: 1.447  loss_ce_1: 0.6326  loss_mask_1: 0.1777  loss_dice_1: 1.371  loss_ce_2: 0.5442  loss_mask_2: 0.1772  loss_dice_2: 1.28  loss_ce_3: 0.4926  loss_mask_3: 0.1728  loss_dice_3: 1.257  loss_ce_4: 0.4866  loss_mask_4: 0.1725  loss_dice_4: 1.283  loss_ce_5: 0.4692  loss_mask_5: 0.1678  loss_dice_5: 1.247  loss_ce_6: 0.4778  loss_mask_6: 0.166  loss_dice_6: 1.207  loss_ce_7: 0.4881  loss_mask_7: 0.1661  loss_dice_7: 1.203  loss_ce_8: 0.4776  loss_mask_8: 0.1651  loss_dice_8: 1.246    time: 1.1080  last_time: 1.1729  data_time: 0.0769  last_data_time: 0.0696   lr: 0.0001  max_mem: 32784M
[10/09 12:25:34] d2.utils.events INFO:  eta: 0:00:44  iter: 959  total_loss: 20.26  loss_ce: 0.4869  loss_mask: 0.1814  loss_dice: 1.267  loss_ce_0: 0.5625  loss_mask_0: 0.216  loss_dice_0: 1.452  loss_ce_1: 0.6616  loss_mask_1: 0.1998  loss_dice_1: 1.366  loss_ce_2: 0.563  loss_mask_2: 0.1997  loss_dice_2: 1.342  loss_ce_3: 0.5406  loss_mask_3: 0.1877  loss_dice_3: 1.31  loss_ce_4: 0.5635  loss_mask_4: 0.1865  loss_dice_4: 1.287  loss_ce_5: 0.504  loss_mask_5: 0.1866  loss_dice_5: 1.279  loss_ce_6: 0.5074  loss_mask_6: 0.1836  loss_dice_6: 1.26  loss_ce_7: 0.511  loss_mask_7: 0.1825  loss_dice_7: 1.244  loss_ce_8: 0.5004  loss_mask_8: 0.1826  loss_dice_8: 1.231    time: 1.1079  last_time: 1.0606  data_time: 0.0755  last_data_time: 0.0603   lr: 0.0001  max_mem: 32784M
[10/09 12:25:56] d2.utils.events INFO:  eta: 0:00:22  iter: 979  total_loss: 19.72  loss_ce: 0.4639  loss_mask: 0.1678  loss_dice: 1.236  loss_ce_0: 0.598  loss_mask_0: 0.1944  loss_dice_0: 1.435  loss_ce_1: 0.6497  loss_mask_1: 0.1849  loss_dice_1: 1.378  loss_ce_2: 0.5842  loss_mask_2: 0.1746  loss_dice_2: 1.305  loss_ce_3: 0.5184  loss_mask_3: 0.1719  loss_dice_3: 1.274  loss_ce_4: 0.4876  loss_mask_4: 0.1649  loss_dice_4: 1.263  loss_ce_5: 0.4678  loss_mask_5: 0.1709  loss_dice_5: 1.245  loss_ce_6: 0.4943  loss_mask_6: 0.1704  loss_dice_6: 1.243  loss_ce_7: 0.4739  loss_mask_7: 0.1681  loss_dice_7: 1.246  loss_ce_8: 0.4718  loss_mask_8: 0.1655  loss_dice_8: 1.25    time: 1.1075  last_time: 1.0040  data_time: 0.0732  last_data_time: 0.0615   lr: 0.0001  max_mem: 32784M
[10/09 12:26:18] fvcore.common.checkpoint INFO: Saving checkpoint to /tsi/hi-paris/GB/segmentation/results/In_vocab/r50_008_1000_19_a_decoder/model_final.pth
[10/09 12:26:21] d2.utils.events INFO:  eta: 0:00:00  iter: 999  total_loss: 19.69  loss_ce: 0.4883  loss_mask: 0.1691  loss_dice: 1.169  loss_ce_0: 0.5671  loss_mask_0: 0.198  loss_dice_0: 1.431  loss_ce_1: 0.658  loss_mask_1: 0.181  loss_dice_1: 1.332  loss_ce_2: 0.5663  loss_mask_2: 0.1716  loss_dice_2: 1.265  loss_ce_3: 0.5499  loss_mask_3: 0.1717  loss_dice_3: 1.227  loss_ce_4: 0.5233  loss_mask_4: 0.1721  loss_dice_4: 1.245  loss_ce_5: 0.4822  loss_mask_5: 0.1728  loss_dice_5: 1.222  loss_ce_6: 0.5111  loss_mask_6: 0.1707  loss_dice_6: 1.209  loss_ce_7: 0.5021  loss_mask_7: 0.1738  loss_dice_7: 1.178  loss_ce_8: 0.4912  loss_mask_8: 0.1699  loss_dice_8: 1.183    time: 1.1074  last_time: 1.0860  data_time: 0.0782  last_data_time: 0.0833   lr: 0.0001  max_mem: 32784M
[10/09 12:26:21] d2.engine.hooks INFO: Overall training speed: 998 iterations in 0:18:25 (1.1074 s / it)
[10/09 12:26:21] d2.engine.hooks INFO: Total training time: 0:18:30 (0:00:05 on hooks)
[10/09 12:26:21] fcclip.data.datasets.register_cityscapes_panoptic INFO: 3 cities found in 'datasets/cityscapes/leftImg8bit/val'.
[10/09 12:26:21] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(1024, 1024), max_size=2560, sample_style='choice')]
[10/09 12:26:21] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/09 12:26:21] d2.data.common INFO: Serializing 500 elements to byte tensors and concatenating them all ...
[10/09 12:26:21] d2.data.common INFO: Serialized dataset takes 0.74 MiB
[10/09 12:26:21] d2.evaluation.evaluator INFO: Start inference on 500 batches
[10/09 13:30:05] detectron2 INFO: Rank of current process: 0. World size: 1
[10/09 13:30:06] detectron2 INFO: Environment info:
-------------------------------  -------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.8.19 | packaged by conda-forge | (default, Mar 20 2024, 12:47:35) [GCC 12.3.0]
numpy                            1.24.4
detectron2                       0.6 @/home/ids/gbrison/segmentation/segmentation/detectron2/detectron2
Compiler                         GCC 11.4
CUDA compiler                    CUDA 12.5
detectron2 arch flags            8.9
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.0+cu121 @/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0,1                          NVIDIA L40S (arch=8.9)
Driver version                   555.42.06
CUDA_HOME                        /usr/local/cuda
Pillow                           9.4.0
torchvision                      0.19.0+cu121 @/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torchvision
torchvision arch flags           5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.10.0
-------------------------------  -------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX512
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.0, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/09 13:30:06] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco/panoptic-segmentation/fcclip/r50_exp_1000_a_decoder.yaml', dist_url='tcp://127.0.0.1:51163', eval_only=False, machine_rank=0, num_gpus=1, num_machines=1, opts=[], resume=False)
[10/09 13:30:06] detectron2 INFO: Contents of args.config_file=configs/coco/panoptic-segmentation/fcclip/r50_exp_1000_a_decoder.yaml:
_BASE_: ./fcclip_convnext_large_eval_ade20k_r50.yaml

INPUT:
  MIN_SIZE_TEST: 1024
  MAX_SIZE_TEST: 2560

MODEL:
  SEM_SEG_HEAD:
    NUM_CLASSES: 19
  WEIGHTS: /tsi/hi-paris/GB/segmentation/models/fcclip_cocopan_r50.pth
OUTPUT_DIR: /tsi/hi-paris/GB/segmentation/results/In_vocab/r50_008_1000_19_a_decoder
DATASETS:
  TRAIN: ("openvocab_cityscapes_fine_panoptic_train",)
  TEST: ("openvocab_cityscapes_fine_panoptic_val",)
SOLVER:
  IMS_PER_BATCH: 8
  MAX_ITER: 1000
TEST:
  EVAL_PERIOD: 1000


[10/09 13:30:07] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - openvocab_cityscapes_fine_panoptic_val
  TRAIN:
  - openvocab_cityscapes_fine_panoptic_train
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: false
  CROP:
    ENABLED: false
    MINIMUM_INST_AREA: 1
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  DATASET_MAPPER_NAME: coco_panoptic_lsj
  FORMAT: RGB
  IMAGE_SIZE: 1024
  MASK_FORMAT: polygon
  MAX_SCALE: 2.0
  MAX_SIZE_TEST: 2560
  MAX_SIZE_TRAIN: 1333
  MIN_SCALE: 0.1
  MIN_SIZE_TEST: 1024
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: -1
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 0
    NAME: CLIP
  DEVICE: cuda
  FC_CLIP:
    CLIP_MODEL_NAME: RN50
    CLIP_PRETRAINED_WEIGHTS: openai
    EMBED_DIM: 1024
    ENSEMBLE_ON_VALID_MASK: true
    GEOMETRIC_ENSEMBLE_ALPHA: 0.4
    GEOMETRIC_ENSEMBLE_BETA: 0.8
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_FORMER:
    CLASS_WEIGHT: 2.0
    DEC_LAYERS: 10
    DEEP_SUPERVISION: true
    DICE_WEIGHT: 5.0
    DIM_FEEDFORWARD: 2048
    DROPOUT: 0.0
    ENC_LAYERS: 0
    ENFORCE_INPUT_PROJ: false
    HIDDEN_DIM: 256
    IMPORTANCE_SAMPLE_RATIO: 0.75
    MASK_WEIGHT: 5.0
    NHEADS: 8
    NO_OBJECT_WEIGHT: 0.1
    NUM_OBJECT_QUERIES: 250
    OVERSAMPLE_RATIO: 3.0
    PRE_NORM: false
    SIZE_DIVISIBILITY: 32
    TEST:
      INSTANCE_ON: true
      OBJECT_MASK_THRESHOLD: 0.0
      OVERLAP_THRESHOLD: 0.8
      PANOPTIC_ON: true
      SEMANTIC_ON: true
      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: false
    TRAIN_NUM_POINTS: 12544
    TRANSFORMER_DECODER_NAME: MultiScaleMaskedTransformerDecoder
    TRANSFORMER_IN_FEATURE: multi_scale_pixel_decoder
  MASK_ON: false
  META_ARCHITECTURE: FCCLIP
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 122.7709383
  - 116.7460125
  - 104.09373615
  PIXEL_STD:
  - 68.5005327
  - 66.6321579
  - 70.32316305
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 1
    - 1
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: basic
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 256
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES:
    - res3
    - res4
    - res5
    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8
    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    MASK_DIM: 256
    NAME: FCCLIPHead
    NORM: GN
    NUM_CLASSES: 19
    PIXEL_DECODER_NAME: MSDeformAttnPixelDecoder
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    TRANSFORMER_ENC_LAYERS: 6
    USE_DEPTHWISE_SEPARABLE_CONV: false
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    USE_CHECKPOINT: false
    WINDOW_SIZE: 7
  WEIGHTS: /tsi/hi-paris/GB/segmentation/models/fcclip_cocopan_r50.pth
OUTPUT_DIR: /tsi/hi-paris/GB/segmentation/results/In_vocab/r50_008_1000_19_a_decoder
SEED: -1
SOLVER:
  AMP:
    ENABLED: true
  BACKBONE_MULTIPLIER: 0.1
  BASE_LR: 0.0001
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 5000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.01
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 8
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 1000
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 327778
  - 355092
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 1000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[10/09 13:30:07] detectron2 INFO: Full config saved to /tsi/hi-paris/GB/segmentation/results/In_vocab/r50_008_1000_19_a_decoder/config.yaml
[10/09 13:30:07] d2.utils.env INFO: Using a generated random seed 8272078
[10/09 13:30:11] d2.engine.defaults INFO: Model:
FCCLIP(
  (backbone): CLIP(
    (clip_model): CLIP(
      (visual): ModifiedResNet(
        (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
        (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        (layer1): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
            (downsample): Sequential(
              (-1): AvgPool2d(kernel_size=1, stride=1, padding=0)
              (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
        )
        (layer2): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
            (downsample): Sequential(
              (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (3): Bottleneck(
            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
        )
        (layer3): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
            (downsample): Sequential(
              (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
              (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (3): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (4): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (5): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
        )
        (layer4): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
            (downsample): Sequential(
              (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
              (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
        )
        (attnpool): AttentionPool2d(
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (c_proj): Linear(in_features=2048, out_features=1024, bias=True)
        )
      )
      (transformer): Transformer(
        (resblocks): ModuleList(
          (0-11): 12 x ResidualAttentionBlock(
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ls_2): Identity()
          )
        )
      )
      (token_embedding): Embedding(49408, 512)
      (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
  )
  (sem_seg_head): FCCLIPHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0-8): 9 x SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0-8): 9 x CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0-8): 9 x FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(250, 256)
      (query_embed): Embedding(250, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0-2): 3 x Sequential()
      )
      (mask_embed): MLP(
        (layers): ModuleList(
          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (mask_pooling): MaskPooling()
      (_mask_pooling_proj): Sequential(
        (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
      )
      (class_embed): MLP(
        (layers): ModuleList(
          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=1024, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 19
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
  (conv1): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv2): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU()
  (mask_pooling): MaskPooling()
  (decoder_adapter): DecoderAdapter(
    (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
    (conv2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    (norm1): LayerNorm((64, 1, 1), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((256, 1, 1), eps=1e-05, elementwise_affine=True)
    (relu): ReLU()
  )
  (void_embedding): Embedding(1, 1024)
)
[10/09 13:30:11] fcclip.data.dataset_mappers.mask_former_semantic_dataset_mapper INFO: [MaskFormerPanopticDatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(800,), max_size=1333, sample_style='choice'), RandomFlip()]
[10/09 13:30:11] fcclip.data.datasets.register_cityscapes_panoptic INFO: 18 cities found in 'datasets/cityscapes/leftImg8bit/train'.
[10/09 13:30:11] d2.data.build INFO: Using training sampler TrainingSampler
[10/09 13:30:11] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/09 13:30:11] d2.data.common INFO: Serializing 2975 elements to byte tensors and concatenating them all ...
[10/09 13:30:11] d2.data.common INFO: Serialized dataset takes 4.12 MiB
[10/09 13:30:11] d2.data.build INFO: Making batched data loader with batch_size=8
[10/09 13:30:11] d2.solver.build WARNING: SOLVER.STEPS contains values larger than SOLVER.MAX_ITER. These values will be ignored.
[10/09 13:30:11] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /tsi/hi-paris/GB/segmentation/models/fcclip_cocopan_r50.pth ...
[10/09 13:30:11] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /tsi/hi-paris/GB/segmentation/models/fcclip_cocopan_r50.pth ...
[10/09 13:30:11] fvcore.common.checkpoint WARNING: Skip loading parameter 'criterion.empty_weight' to the model due to incompatible shapes: (134,) in the checkpoint but (20,) in the model! You might want to double check if this is expected.
[10/09 13:30:11] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.clip_model.ln_final.{bias, weight}[0m
[34mbackbone.clip_model.token_embedding.weight[0m
[34mbackbone.clip_model.transformer.resblocks.0.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.attnpool.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.attnpool.k_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.attnpool.positional_embedding[0m
[34mbackbone.clip_model.visual.attnpool.q_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.attnpool.v_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.conv1.weight[0m
[34mbackbone.clip_model.visual.conv2.weight[0m
[34mbackbone.clip_model.visual.conv3.weight[0m
[34mbackbone.clip_model.visual.layer1.0.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.0.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.0.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.0.conv1.weight[0m
[34mbackbone.clip_model.visual.layer1.0.conv2.weight[0m
[34mbackbone.clip_model.visual.layer1.0.conv3.weight[0m
[34mbackbone.clip_model.visual.layer1.0.downsample.0.weight[0m
[34mbackbone.clip_model.visual.layer1.0.downsample.1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.1.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.1.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.1.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.1.conv1.weight[0m
[34mbackbone.clip_model.visual.layer1.1.conv2.weight[0m
[34mbackbone.clip_model.visual.layer1.1.conv3.weight[0m
[34mbackbone.clip_model.visual.layer1.2.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.2.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.2.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.2.conv1.weight[0m
[34mbackbone.clip_model.visual.layer1.2.conv2.weight[0m
[34mbackbone.clip_model.visual.layer1.2.conv3.weight[0m
[34mbackbone.clip_model.visual.layer2.0.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.0.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.0.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.0.conv1.weight[0m
[34mbackbone.clip_model.visual.layer2.0.conv2.weight[0m
[34mbackbone.clip_model.visual.layer2.0.conv3.weight[0m
[34mbackbone.clip_model.visual.layer2.0.downsample.0.weight[0m
[34mbackbone.clip_model.visual.layer2.0.downsample.1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.1.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.1.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.1.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.1.conv1.weight[0m
[34mbackbone.clip_model.visual.layer2.1.conv2.weight[0m
[34mbackbone.clip_model.visual.layer2.1.conv3.weight[0m
[34mbackbone.clip_model.visual.layer2.2.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.2.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.2.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.2.conv1.weight[0m
[34mbackbone.clip_model.visual.layer2.2.conv2.weight[0m
[34mbackbone.clip_model.visual.layer2.2.conv3.weight[0m
[34mbackbone.clip_model.visual.layer2.3.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.3.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.3.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.3.conv1.weight[0m
[34mbackbone.clip_model.visual.layer2.3.conv2.weight[0m
[34mbackbone.clip_model.visual.layer2.3.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.0.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.0.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.0.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.0.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.0.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.0.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.0.downsample.0.weight[0m
[34mbackbone.clip_model.visual.layer3.0.downsample.1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.1.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.1.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.1.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.1.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.1.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.1.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.2.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.2.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.2.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.2.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.2.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.2.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.3.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.3.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.3.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.3.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.3.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.3.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.4.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.4.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.4.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.4.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.4.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.4.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.5.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.5.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.5.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.5.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.5.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.5.conv3.weight[0m
[34mbackbone.clip_model.visual.layer4.0.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.0.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.0.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.0.conv1.weight[0m
[34mbackbone.clip_model.visual.layer4.0.conv2.weight[0m
[34mbackbone.clip_model.visual.layer4.0.conv3.weight[0m
[34mbackbone.clip_model.visual.layer4.0.downsample.0.weight[0m
[34mbackbone.clip_model.visual.layer4.0.downsample.1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.1.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.1.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.1.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.1.conv1.weight[0m
[34mbackbone.clip_model.visual.layer4.1.conv2.weight[0m
[34mbackbone.clip_model.visual.layer4.1.conv3.weight[0m
[34mbackbone.clip_model.visual.layer4.2.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.2.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.2.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.2.conv1.weight[0m
[34mbackbone.clip_model.visual.layer4.2.conv2.weight[0m
[34mbackbone.clip_model.visual.layer4.2.conv3.weight[0m
[34mbackbone.clip_model.{logit_scale, positional_embedding, text_projection}[0m
[34mbn1.{bias, running_mean, running_var, weight}[0m
[34mbn2.{bias, running_mean, running_var, weight}[0m
[34mconv1.{bias, weight}[0m
[34mconv2.{bias, weight}[0m
[34mcriterion.empty_weight[0m
[34mdecoder_adapter.conv1.{bias, weight}[0m
[34mdecoder_adapter.conv2.{bias, weight}[0m
[34mdecoder_adapter.norm1.{bias, weight}[0m
[34mdecoder_adapter.norm2.{bias, weight}[0m
[10/09 13:30:11] d2.engine.train_loop INFO: Starting training from iteration 0
[10/09 13:30:36] d2.utils.events INFO:  eta: 0:17:41  iter: 19  total_loss: 32.28  loss_ce: 1.267  loss_mask: 0.2686  loss_dice: 1.474  loss_ce_0: 1.525  loss_mask_0: 0.2987  loss_dice_0: 1.826  loss_ce_1: 1.399  loss_mask_1: 0.2967  loss_dice_1: 1.668  loss_ce_2: 1.385  loss_mask_2: 0.2839  loss_dice_2: 1.605  loss_ce_3: 1.268  loss_mask_3: 0.2771  loss_dice_3: 1.513  loss_ce_4: 1.204  loss_mask_4: 0.2779  loss_dice_4: 1.509  loss_ce_5: 1.242  loss_mask_5: 0.275  loss_dice_5: 1.535  loss_ce_6: 1.238  loss_mask_6: 0.2748  loss_dice_6: 1.477  loss_ce_7: 1.244  loss_mask_7: 0.2718  loss_dice_7: 1.486  loss_ce_8: 1.293  loss_mask_8: 0.275  loss_dice_8: 1.517    time: 1.0935  last_time: 1.1535  data_time: 0.1218  last_data_time: 0.0952   lr: 0.0001  max_mem: 31562M
[10/09 13:30:58] d2.utils.events INFO:  eta: 0:17:35  iter: 39  total_loss: 25.45  loss_ce: 0.7568  loss_mask: 0.2311  loss_dice: 1.399  loss_ce_0: 0.8676  loss_mask_0: 0.2672  loss_dice_0: 1.727  loss_ce_1: 0.952  loss_mask_1: 0.262  loss_dice_1: 1.503  loss_ce_2: 0.8542  loss_mask_2: 0.239  loss_dice_2: 1.505  loss_ce_3: 0.7947  loss_mask_3: 0.2364  loss_dice_3: 1.462  loss_ce_4: 0.7633  loss_mask_4: 0.2337  loss_dice_4: 1.421  loss_ce_5: 0.769  loss_mask_5: 0.2352  loss_dice_5: 1.426  loss_ce_6: 0.7598  loss_mask_6: 0.2282  loss_dice_6: 1.414  loss_ce_7: 0.7804  loss_mask_7: 0.233  loss_dice_7: 1.39  loss_ce_8: 0.7539  loss_mask_8: 0.233  loss_dice_8: 1.391    time: 1.0982  last_time: 1.0749  data_time: 0.0794  last_data_time: 0.0912   lr: 0.0001  max_mem: 31562M
[10/09 13:31:20] d2.utils.events INFO:  eta: 0:17:09  iter: 59  total_loss: 24.13  loss_ce: 0.6888  loss_mask: 0.2345  loss_dice: 1.351  loss_ce_0: 0.7286  loss_mask_0: 0.2634  loss_dice_0: 1.654  loss_ce_1: 0.853  loss_mask_1: 0.2513  loss_dice_1: 1.488  loss_ce_2: 0.7831  loss_mask_2: 0.2386  loss_dice_2: 1.44  loss_ce_3: 0.7449  loss_mask_3: 0.2328  loss_dice_3: 1.401  loss_ce_4: 0.7322  loss_mask_4: 0.2308  loss_dice_4: 1.377  loss_ce_5: 0.6799  loss_mask_5: 0.231  loss_dice_5: 1.365  loss_ce_6: 0.7116  loss_mask_6: 0.2345  loss_dice_6: 1.345  loss_ce_7: 0.6993  loss_mask_7: 0.2302  loss_dice_7: 1.354  loss_ce_8: 0.7083  loss_mask_8: 0.2309  loss_dice_8: 1.338    time: 1.0920  last_time: 1.0758  data_time: 0.0747  last_data_time: 0.0677   lr: 0.0001  max_mem: 31562M
[10/09 13:31:41] d2.utils.events INFO:  eta: 0:16:38  iter: 79  total_loss: 23.34  loss_ce: 0.6667  loss_mask: 0.2195  loss_dice: 1.327  loss_ce_0: 0.7578  loss_mask_0: 0.2484  loss_dice_0: 1.595  loss_ce_1: 0.8972  loss_mask_1: 0.2348  loss_dice_1: 1.484  loss_ce_2: 0.7784  loss_mask_2: 0.2175  loss_dice_2: 1.41  loss_ce_3: 0.7283  loss_mask_3: 0.2119  loss_dice_3: 1.376  loss_ce_4: 0.7093  loss_mask_4: 0.2186  loss_dice_4: 1.329  loss_ce_5: 0.6602  loss_mask_5: 0.2209  loss_dice_5: 1.356  loss_ce_6: 0.6582  loss_mask_6: 0.2168  loss_dice_6: 1.347  loss_ce_7: 0.6631  loss_mask_7: 0.2185  loss_dice_7: 1.355  loss_ce_8: 0.6689  loss_mask_8: 0.2169  loss_dice_8: 1.298    time: 1.0831  last_time: 1.0565  data_time: 0.0702  last_data_time: 0.0765   lr: 0.0001  max_mem: 31562M
[10/09 13:32:03] d2.utils.events INFO:  eta: 0:16:06  iter: 99  total_loss: 23.13  loss_ce: 0.6345  loss_mask: 0.2011  loss_dice: 1.347  loss_ce_0: 0.7065  loss_mask_0: 0.2251  loss_dice_0: 1.591  loss_ce_1: 0.8085  loss_mask_1: 0.2155  loss_dice_1: 1.53  loss_ce_2: 0.7606  loss_mask_2: 0.2034  loss_dice_2: 1.455  loss_ce_3: 0.6757  loss_mask_3: 0.2035  loss_dice_3: 1.38  loss_ce_4: 0.644  loss_mask_4: 0.2023  loss_dice_4: 1.398  loss_ce_5: 0.6555  loss_mask_5: 0.2061  loss_dice_5: 1.364  loss_ce_6: 0.6368  loss_mask_6: 0.2012  loss_dice_6: 1.366  loss_ce_7: 0.6196  loss_mask_7: 0.2013  loss_dice_7: 1.361  loss_ce_8: 0.6131  loss_mask_8: 0.2013  loss_dice_8: 1.377    time: 1.0793  last_time: 1.1644  data_time: 0.0703  last_data_time: 0.1062   lr: 0.0001  max_mem: 31562M
[10/09 13:32:24] d2.utils.events INFO:  eta: 0:15:40  iter: 119  total_loss: 22.72  loss_ce: 0.6113  loss_mask: 0.2198  loss_dice: 1.316  loss_ce_0: 0.6601  loss_mask_0: 0.2651  loss_dice_0: 1.627  loss_ce_1: 0.783  loss_mask_1: 0.2345  loss_dice_1: 1.484  loss_ce_2: 0.7276  loss_mask_2: 0.2209  loss_dice_2: 1.396  loss_ce_3: 0.6691  loss_mask_3: 0.2216  loss_dice_3: 1.35  loss_ce_4: 0.6131  loss_mask_4: 0.2242  loss_dice_4: 1.359  loss_ce_5: 0.5759  loss_mask_5: 0.2209  loss_dice_5: 1.37  loss_ce_6: 0.5944  loss_mask_6: 0.2213  loss_dice_6: 1.366  loss_ce_7: 0.6321  loss_mask_7: 0.2169  loss_dice_7: 1.342  loss_ce_8: 0.6283  loss_mask_8: 0.219  loss_dice_8: 1.342    time: 1.0739  last_time: 1.0418  data_time: 0.0680  last_data_time: 0.0694   lr: 0.0001  max_mem: 31833M
[10/09 13:32:45] d2.utils.events INFO:  eta: 0:15:19  iter: 139  total_loss: 22.72  loss_ce: 0.593  loss_mask: 0.2019  loss_dice: 1.312  loss_ce_0: 0.6553  loss_mask_0: 0.2386  loss_dice_0: 1.574  loss_ce_1: 0.7782  loss_mask_1: 0.22  loss_dice_1: 1.478  loss_ce_2: 0.7059  loss_mask_2: 0.2103  loss_dice_2: 1.419  loss_ce_3: 0.6557  loss_mask_3: 0.2062  loss_dice_3: 1.323  loss_ce_4: 0.6529  loss_mask_4: 0.205  loss_dice_4: 1.373  loss_ce_5: 0.5969  loss_mask_5: 0.2025  loss_dice_5: 1.325  loss_ce_6: 0.6023  loss_mask_6: 0.2046  loss_dice_6: 1.337  loss_ce_7: 0.6038  loss_mask_7: 0.2034  loss_dice_7: 1.329  loss_ce_8: 0.6052  loss_mask_8: 0.2021  loss_dice_8: 1.316    time: 1.0746  last_time: 1.0826  data_time: 0.0766  last_data_time: 0.0866   lr: 0.0001  max_mem: 31833M
[10/09 13:33:07] d2.utils.events INFO:  eta: 0:14:58  iter: 159  total_loss: 23.1  loss_ce: 0.6563  loss_mask: 0.1882  loss_dice: 1.356  loss_ce_0: 0.678  loss_mask_0: 0.2143  loss_dice_0: 1.646  loss_ce_1: 0.8027  loss_mask_1: 0.2133  loss_dice_1: 1.552  loss_ce_2: 0.7247  loss_mask_2: 0.1981  loss_dice_2: 1.442  loss_ce_3: 0.7119  loss_mask_3: 0.1916  loss_dice_3: 1.385  loss_ce_4: 0.6481  loss_mask_4: 0.1928  loss_dice_4: 1.388  loss_ce_5: 0.6598  loss_mask_5: 0.1903  loss_dice_5: 1.38  loss_ce_6: 0.6814  loss_mask_6: 0.1909  loss_dice_6: 1.379  loss_ce_7: 0.6607  loss_mask_7: 0.1897  loss_dice_7: 1.349  loss_ce_8: 0.6597  loss_mask_8: 0.191  loss_dice_8: 1.323    time: 1.0744  last_time: 1.0892  data_time: 0.0736  last_data_time: 0.0712   lr: 0.0001  max_mem: 31833M
[10/09 13:33:28] d2.utils.events INFO:  eta: 0:14:36  iter: 179  total_loss: 22.96  loss_ce: 0.6041  loss_mask: 0.1858  loss_dice: 1.387  loss_ce_0: 0.6299  loss_mask_0: 0.2164  loss_dice_0: 1.603  loss_ce_1: 0.7618  loss_mask_1: 0.1995  loss_dice_1: 1.528  loss_ce_2: 0.689  loss_mask_2: 0.1919  loss_dice_2: 1.473  loss_ce_3: 0.6208  loss_mask_3: 0.1963  loss_dice_3: 1.428  loss_ce_4: 0.6227  loss_mask_4: 0.1903  loss_dice_4: 1.412  loss_ce_5: 0.6223  loss_mask_5: 0.1879  loss_dice_5: 1.385  loss_ce_6: 0.5826  loss_mask_6: 0.19  loss_dice_6: 1.366  loss_ce_7: 0.6018  loss_mask_7: 0.1872  loss_dice_7: 1.371  loss_ce_8: 0.6083  loss_mask_8: 0.1868  loss_dice_8: 1.408    time: 1.0730  last_time: 1.0396  data_time: 0.0704  last_data_time: 0.0831   lr: 0.0001  max_mem: 31833M
[10/09 13:33:49] d2.utils.events INFO:  eta: 0:14:14  iter: 199  total_loss: 22.18  loss_ce: 0.6202  loss_mask: 0.1892  loss_dice: 1.34  loss_ce_0: 0.636  loss_mask_0: 0.2148  loss_dice_0: 1.596  loss_ce_1: 0.7557  loss_mask_1: 0.1983  loss_dice_1: 1.466  loss_ce_2: 0.692  loss_mask_2: 0.1917  loss_dice_2: 1.452  loss_ce_3: 0.6596  loss_mask_3: 0.1909  loss_dice_3: 1.39  loss_ce_4: 0.6181  loss_mask_4: 0.187  loss_dice_4: 1.343  loss_ce_5: 0.5953  loss_mask_5: 0.1876  loss_dice_5: 1.367  loss_ce_6: 0.5677  loss_mask_6: 0.1894  loss_dice_6: 1.326  loss_ce_7: 0.5992  loss_mask_7: 0.1905  loss_dice_7: 1.324  loss_ce_8: 0.5845  loss_mask_8: 0.1892  loss_dice_8: 1.291    time: 1.0715  last_time: 1.0706  data_time: 0.0694  last_data_time: 0.0730   lr: 0.0001  max_mem: 32619M
[10/09 13:34:10] d2.utils.events INFO:  eta: 0:13:50  iter: 219  total_loss: 21.04  loss_ce: 0.5269  loss_mask: 0.19  loss_dice: 1.274  loss_ce_0: 0.6189  loss_mask_0: 0.2197  loss_dice_0: 1.509  loss_ce_1: 0.7031  loss_mask_1: 0.2102  loss_dice_1: 1.41  loss_ce_2: 0.6324  loss_mask_2: 0.1972  loss_dice_2: 1.327  loss_ce_3: 0.596  loss_mask_3: 0.1947  loss_dice_3: 1.307  loss_ce_4: 0.5427  loss_mask_4: 0.1951  loss_dice_4: 1.297  loss_ce_5: 0.5259  loss_mask_5: 0.1924  loss_dice_5: 1.276  loss_ce_6: 0.5383  loss_mask_6: 0.1909  loss_dice_6: 1.296  loss_ce_7: 0.5243  loss_mask_7: 0.1927  loss_dice_7: 1.281  loss_ce_8: 0.5215  loss_mask_8: 0.1885  loss_dice_8: 1.279    time: 1.0689  last_time: 1.0487  data_time: 0.0681  last_data_time: 0.0623   lr: 0.0001  max_mem: 32619M
[10/09 13:34:31] d2.utils.events INFO:  eta: 0:13:28  iter: 239  total_loss: 21.83  loss_ce: 0.5486  loss_mask: 0.2114  loss_dice: 1.344  loss_ce_0: 0.6555  loss_mask_0: 0.2454  loss_dice_0: 1.609  loss_ce_1: 0.7067  loss_mask_1: 0.2346  loss_dice_1: 1.472  loss_ce_2: 0.6412  loss_mask_2: 0.2184  loss_dice_2: 1.434  loss_ce_3: 0.616  loss_mask_3: 0.2143  loss_dice_3: 1.363  loss_ce_4: 0.5735  loss_mask_4: 0.2159  loss_dice_4: 1.358  loss_ce_5: 0.5718  loss_mask_5: 0.216  loss_dice_5: 1.355  loss_ce_6: 0.5465  loss_mask_6: 0.2128  loss_dice_6: 1.36  loss_ce_7: 0.5442  loss_mask_7: 0.212  loss_dice_7: 1.333  loss_ce_8: 0.575  loss_mask_8: 0.2156  loss_dice_8: 1.31    time: 1.0675  last_time: 1.0432  data_time: 0.0693  last_data_time: 0.0563   lr: 0.0001  max_mem: 32619M
[10/09 13:34:53] d2.utils.events INFO:  eta: 0:13:06  iter: 259  total_loss: 21.37  loss_ce: 0.5365  loss_mask: 0.1842  loss_dice: 1.286  loss_ce_0: 0.6234  loss_mask_0: 0.217  loss_dice_0: 1.538  loss_ce_1: 0.7286  loss_mask_1: 0.1993  loss_dice_1: 1.428  loss_ce_2: 0.6259  loss_mask_2: 0.1891  loss_dice_2: 1.383  loss_ce_3: 0.6095  loss_mask_3: 0.1903  loss_dice_3: 1.304  loss_ce_4: 0.6065  loss_mask_4: 0.1848  loss_dice_4: 1.317  loss_ce_5: 0.5615  loss_mask_5: 0.1883  loss_dice_5: 1.322  loss_ce_6: 0.5647  loss_mask_6: 0.1848  loss_dice_6: 1.303  loss_ce_7: 0.5546  loss_mask_7: 0.1855  loss_dice_7: 1.305  loss_ce_8: 0.5582  loss_mask_8: 0.1868  loss_dice_8: 1.267    time: 1.0669  last_time: 1.0282  data_time: 0.0677  last_data_time: 0.0585   lr: 0.0001  max_mem: 32619M
[10/09 13:35:14] d2.utils.events INFO:  eta: 0:12:44  iter: 279  total_loss: 21.27  loss_ce: 0.5398  loss_mask: 0.1868  loss_dice: 1.31  loss_ce_0: 0.6247  loss_mask_0: 0.2126  loss_dice_0: 1.54  loss_ce_1: 0.7391  loss_mask_1: 0.2023  loss_dice_1: 1.424  loss_ce_2: 0.6415  loss_mask_2: 0.1894  loss_dice_2: 1.377  loss_ce_3: 0.5945  loss_mask_3: 0.1894  loss_dice_3: 1.297  loss_ce_4: 0.5749  loss_mask_4: 0.1881  loss_dice_4: 1.305  loss_ce_5: 0.5839  loss_mask_5: 0.1839  loss_dice_5: 1.308  loss_ce_6: 0.548  loss_mask_6: 0.1868  loss_dice_6: 1.3  loss_ce_7: 0.5728  loss_mask_7: 0.1872  loss_dice_7: 1.265  loss_ce_8: 0.5498  loss_mask_8: 0.1884  loss_dice_8: 1.302    time: 1.0658  last_time: 1.0214  data_time: 0.0670  last_data_time: 0.0558   lr: 0.0001  max_mem: 32619M
[10/09 13:35:35] d2.utils.events INFO:  eta: 0:12:21  iter: 299  total_loss: 21.69  loss_ce: 0.5636  loss_mask: 0.1849  loss_dice: 1.293  loss_ce_0: 0.6045  loss_mask_0: 0.2136  loss_dice_0: 1.536  loss_ce_1: 0.7335  loss_mask_1: 0.1972  loss_dice_1: 1.407  loss_ce_2: 0.6253  loss_mask_2: 0.1913  loss_dice_2: 1.397  loss_ce_3: 0.5955  loss_mask_3: 0.1933  loss_dice_3: 1.335  loss_ce_4: 0.6037  loss_mask_4: 0.1909  loss_dice_4: 1.333  loss_ce_5: 0.564  loss_mask_5: 0.1901  loss_dice_5: 1.334  loss_ce_6: 0.5499  loss_mask_6: 0.1903  loss_dice_6: 1.307  loss_ce_7: 0.5653  loss_mask_7: 0.1902  loss_dice_7: 1.304  loss_ce_8: 0.5704  loss_mask_8: 0.1857  loss_dice_8: 1.262    time: 1.0652  last_time: 1.0824  data_time: 0.0652  last_data_time: 0.0761   lr: 0.0001  max_mem: 32619M
[10/09 13:35:56] d2.utils.events INFO:  eta: 0:11:59  iter: 319  total_loss: 21.25  loss_ce: 0.5575  loss_mask: 0.1892  loss_dice: 1.259  loss_ce_0: 0.6234  loss_mask_0: 0.2153  loss_dice_0: 1.516  loss_ce_1: 0.6892  loss_mask_1: 0.2061  loss_dice_1: 1.437  loss_ce_2: 0.6572  loss_mask_2: 0.1946  loss_dice_2: 1.365  loss_ce_3: 0.6285  loss_mask_3: 0.1921  loss_dice_3: 1.298  loss_ce_4: 0.5926  loss_mask_4: 0.1964  loss_dice_4: 1.298  loss_ce_5: 0.582  loss_mask_5: 0.1889  loss_dice_5: 1.288  loss_ce_6: 0.5817  loss_mask_6: 0.19  loss_dice_6: 1.233  loss_ce_7: 0.5635  loss_mask_7: 0.1924  loss_dice_7: 1.256  loss_ce_8: 0.5609  loss_mask_8: 0.1892  loss_dice_8: 1.252    time: 1.0650  last_time: 1.0501  data_time: 0.0810  last_data_time: 0.0591   lr: 0.0001  max_mem: 32619M
[10/09 13:36:18] d2.utils.events INFO:  eta: 0:11:39  iter: 339  total_loss: 22.54  loss_ce: 0.5953  loss_mask: 0.1993  loss_dice: 1.325  loss_ce_0: 0.6618  loss_mask_0: 0.2222  loss_dice_0: 1.627  loss_ce_1: 0.76  loss_mask_1: 0.2103  loss_dice_1: 1.531  loss_ce_2: 0.6847  loss_mask_2: 0.2029  loss_dice_2: 1.414  loss_ce_3: 0.6573  loss_mask_3: 0.2054  loss_dice_3: 1.347  loss_ce_4: 0.6158  loss_mask_4: 0.2046  loss_dice_4: 1.348  loss_ce_5: 0.6171  loss_mask_5: 0.1998  loss_dice_5: 1.339  loss_ce_6: 0.5971  loss_mask_6: 0.1999  loss_dice_6: 1.312  loss_ce_7: 0.6108  loss_mask_7: 0.201  loss_dice_7: 1.331  loss_ce_8: 0.5926  loss_mask_8: 0.1988  loss_dice_8: 1.321    time: 1.0655  last_time: 1.1117  data_time: 0.0691  last_data_time: 0.0823   lr: 0.0001  max_mem: 32619M
[10/09 13:36:39] d2.utils.events INFO:  eta: 0:11:18  iter: 359  total_loss: 21.3  loss_ce: 0.5237  loss_mask: 0.1889  loss_dice: 1.293  loss_ce_0: 0.6217  loss_mask_0: 0.2217  loss_dice_0: 1.521  loss_ce_1: 0.7416  loss_mask_1: 0.2055  loss_dice_1: 1.42  loss_ce_2: 0.6519  loss_mask_2: 0.1935  loss_dice_2: 1.372  loss_ce_3: 0.6024  loss_mask_3: 0.1912  loss_dice_3: 1.309  loss_ce_4: 0.5969  loss_mask_4: 0.1964  loss_dice_4: 1.278  loss_ce_5: 0.5682  loss_mask_5: 0.1925  loss_dice_5: 1.292  loss_ce_6: 0.5401  loss_mask_6: 0.1899  loss_dice_6: 1.279  loss_ce_7: 0.5686  loss_mask_7: 0.1898  loss_dice_7: 1.286  loss_ce_8: 0.5427  loss_mask_8: 0.1888  loss_dice_8: 1.285    time: 1.0663  last_time: 1.1055  data_time: 0.0688  last_data_time: 0.0556   lr: 0.0001  max_mem: 32619M
[10/09 13:37:02] d2.utils.events INFO:  eta: 0:10:59  iter: 379  total_loss: 22.04  loss_ce: 0.6145  loss_mask: 0.1706  loss_dice: 1.278  loss_ce_0: 0.6957  loss_mask_0: 0.2051  loss_dice_0: 1.539  loss_ce_1: 0.7641  loss_mask_1: 0.1999  loss_dice_1: 1.37  loss_ce_2: 0.7234  loss_mask_2: 0.1803  loss_dice_2: 1.362  loss_ce_3: 0.6839  loss_mask_3: 0.178  loss_dice_3: 1.333  loss_ce_4: 0.6247  loss_mask_4: 0.1784  loss_dice_4: 1.332  loss_ce_5: 0.5683  loss_mask_5: 0.1738  loss_dice_5: 1.311  loss_ce_6: 0.5749  loss_mask_6: 0.1745  loss_dice_6: 1.272  loss_ce_7: 0.5954  loss_mask_7: 0.1708  loss_dice_7: 1.276  loss_ce_8: 0.6061  loss_mask_8: 0.1734  loss_dice_8: 1.291    time: 1.0690  last_time: 1.1012  data_time: 0.0747  last_data_time: 0.0680   lr: 0.0001  max_mem: 32619M
[10/09 13:37:24] d2.utils.events INFO:  eta: 0:10:39  iter: 399  total_loss: 21.15  loss_ce: 0.5388  loss_mask: 0.1712  loss_dice: 1.299  loss_ce_0: 0.5977  loss_mask_0: 0.1999  loss_dice_0: 1.573  loss_ce_1: 0.6831  loss_mask_1: 0.1894  loss_dice_1: 1.458  loss_ce_2: 0.6293  loss_mask_2: 0.1759  loss_dice_2: 1.391  loss_ce_3: 0.5812  loss_mask_3: 0.18  loss_dice_3: 1.328  loss_ce_4: 0.5549  loss_mask_4: 0.1812  loss_dice_4: 1.335  loss_ce_5: 0.5414  loss_mask_5: 0.1749  loss_dice_5: 1.308  loss_ce_6: 0.5465  loss_mask_6: 0.1744  loss_dice_6: 1.322  loss_ce_7: 0.5181  loss_mask_7: 0.1743  loss_dice_7: 1.279  loss_ce_8: 0.523  loss_mask_8: 0.1728  loss_dice_8: 1.294    time: 1.0710  last_time: 1.0674  data_time: 0.0756  last_data_time: 0.0746   lr: 0.0001  max_mem: 32619M
[10/09 13:37:46] d2.utils.events INFO:  eta: 0:10:19  iter: 419  total_loss: 20.32  loss_ce: 0.4933  loss_mask: 0.1903  loss_dice: 1.252  loss_ce_0: 0.5767  loss_mask_0: 0.2172  loss_dice_0: 1.478  loss_ce_1: 0.6538  loss_mask_1: 0.2058  loss_dice_1: 1.386  loss_ce_2: 0.594  loss_mask_2: 0.1957  loss_dice_2: 1.359  loss_ce_3: 0.5301  loss_mask_3: 0.1981  loss_dice_3: 1.287  loss_ce_4: 0.5291  loss_mask_4: 0.1921  loss_dice_4: 1.297  loss_ce_5: 0.5086  loss_mask_5: 0.1925  loss_dice_5: 1.27  loss_ce_6: 0.5126  loss_mask_6: 0.1925  loss_dice_6: 1.284  loss_ce_7: 0.4813  loss_mask_7: 0.1906  loss_dice_7: 1.264  loss_ce_8: 0.4951  loss_mask_8: 0.1914  loss_dice_8: 1.236    time: 1.0721  last_time: 1.1359  data_time: 0.0716  last_data_time: 0.0821   lr: 0.0001  max_mem: 32619M
[10/09 13:38:07] d2.utils.events INFO:  eta: 0:09:57  iter: 439  total_loss: 20.44  loss_ce: 0.5062  loss_mask: 0.182  loss_dice: 1.264  loss_ce_0: 0.6241  loss_mask_0: 0.221  loss_dice_0: 1.483  loss_ce_1: 0.6718  loss_mask_1: 0.2057  loss_dice_1: 1.366  loss_ce_2: 0.6245  loss_mask_2: 0.1947  loss_dice_2: 1.343  loss_ce_3: 0.5698  loss_mask_3: 0.1935  loss_dice_3: 1.292  loss_ce_4: 0.5587  loss_mask_4: 0.1884  loss_dice_4: 1.271  loss_ce_5: 0.5531  loss_mask_5: 0.1811  loss_dice_5: 1.26  loss_ce_6: 0.5072  loss_mask_6: 0.1825  loss_dice_6: 1.273  loss_ce_7: 0.5013  loss_mask_7: 0.1829  loss_dice_7: 1.256  loss_ce_8: 0.5026  loss_mask_8: 0.1825  loss_dice_8: 1.274    time: 1.0717  last_time: 1.0463  data_time: 0.0666  last_data_time: 0.0816   lr: 0.0001  max_mem: 32619M
[10/09 13:38:29] d2.utils.events INFO:  eta: 0:09:35  iter: 459  total_loss: 21.38  loss_ce: 0.527  loss_mask: 0.1774  loss_dice: 1.301  loss_ce_0: 0.596  loss_mask_0: 0.2129  loss_dice_0: 1.56  loss_ce_1: 0.6791  loss_mask_1: 0.1926  loss_dice_1: 1.43  loss_ce_2: 0.6126  loss_mask_2: 0.184  loss_dice_2: 1.384  loss_ce_3: 0.5786  loss_mask_3: 0.1794  loss_dice_3: 1.362  loss_ce_4: 0.5582  loss_mask_4: 0.1768  loss_dice_4: 1.339  loss_ce_5: 0.521  loss_mask_5: 0.1774  loss_dice_5: 1.349  loss_ce_6: 0.5454  loss_mask_6: 0.1755  loss_dice_6: 1.308  loss_ce_7: 0.5189  loss_mask_7: 0.1778  loss_dice_7: 1.301  loss_ce_8: 0.5119  loss_mask_8: 0.1745  loss_dice_8: 1.279    time: 1.0713  last_time: 1.1173  data_time: 0.0684  last_data_time: 0.0884   lr: 0.0001  max_mem: 32619M
[10/09 13:38:50] d2.utils.events INFO:  eta: 0:09:14  iter: 479  total_loss: 20.88  loss_ce: 0.5152  loss_mask: 0.177  loss_dice: 1.289  loss_ce_0: 0.643  loss_mask_0: 0.211  loss_dice_0: 1.536  loss_ce_1: 0.7278  loss_mask_1: 0.1936  loss_dice_1: 1.396  loss_ce_2: 0.6224  loss_mask_2: 0.1804  loss_dice_2: 1.351  loss_ce_3: 0.5684  loss_mask_3: 0.1789  loss_dice_3: 1.315  loss_ce_4: 0.5536  loss_mask_4: 0.1785  loss_dice_4: 1.299  loss_ce_5: 0.5247  loss_mask_5: 0.179  loss_dice_5: 1.284  loss_ce_6: 0.5141  loss_mask_6: 0.1783  loss_dice_6: 1.306  loss_ce_7: 0.525  loss_mask_7: 0.1768  loss_dice_7: 1.278  loss_ce_8: 0.5086  loss_mask_8: 0.1762  loss_dice_8: 1.284    time: 1.0715  last_time: 1.1384  data_time: 0.0718  last_data_time: 0.1125   lr: 0.0001  max_mem: 32619M
[10/09 13:39:12] d2.utils.events INFO:  eta: 0:08:53  iter: 499  total_loss: 21.08  loss_ce: 0.5384  loss_mask: 0.193  loss_dice: 1.243  loss_ce_0: 0.6015  loss_mask_0: 0.2117  loss_dice_0: 1.461  loss_ce_1: 0.7179  loss_mask_1: 0.201  loss_dice_1: 1.381  loss_ce_2: 0.6485  loss_mask_2: 0.1922  loss_dice_2: 1.313  loss_ce_3: 0.5628  loss_mask_3: 0.1937  loss_dice_3: 1.285  loss_ce_4: 0.5592  loss_mask_4: 0.1917  loss_dice_4: 1.304  loss_ce_5: 0.5315  loss_mask_5: 0.1891  loss_dice_5: 1.327  loss_ce_6: 0.5275  loss_mask_6: 0.1908  loss_dice_6: 1.226  loss_ce_7: 0.5259  loss_mask_7: 0.1918  loss_dice_7: 1.271  loss_ce_8: 0.5139  loss_mask_8: 0.1931  loss_dice_8: 1.281    time: 1.0715  last_time: 1.0226  data_time: 0.0725  last_data_time: 0.0616   lr: 0.0001  max_mem: 32619M
[10/09 13:39:33] d2.utils.events INFO:  eta: 0:08:32  iter: 519  total_loss: 20.64  loss_ce: 0.5472  loss_mask: 0.1747  loss_dice: 1.289  loss_ce_0: 0.6366  loss_mask_0: 0.2014  loss_dice_0: 1.51  loss_ce_1: 0.7087  loss_mask_1: 0.1934  loss_dice_1: 1.397  loss_ce_2: 0.6578  loss_mask_2: 0.1812  loss_dice_2: 1.359  loss_ce_3: 0.5978  loss_mask_3: 0.1819  loss_dice_3: 1.307  loss_ce_4: 0.5651  loss_mask_4: 0.178  loss_dice_4: 1.329  loss_ce_5: 0.5556  loss_mask_5: 0.1768  loss_dice_5: 1.315  loss_ce_6: 0.5741  loss_mask_6: 0.1738  loss_dice_6: 1.277  loss_ce_7: 0.5531  loss_mask_7: 0.1769  loss_dice_7: 1.27  loss_ce_8: 0.5286  loss_mask_8: 0.1767  loss_dice_8: 1.285    time: 1.0723  last_time: 1.0335  data_time: 0.0756  last_data_time: 0.0748   lr: 0.0001  max_mem: 32619M
[10/09 13:39:55] d2.utils.events INFO:  eta: 0:08:10  iter: 539  total_loss: 21.27  loss_ce: 0.5293  loss_mask: 0.1734  loss_dice: 1.283  loss_ce_0: 0.6017  loss_mask_0: 0.2114  loss_dice_0: 1.489  loss_ce_1: 0.6389  loss_mask_1: 0.199  loss_dice_1: 1.443  loss_ce_2: 0.6195  loss_mask_2: 0.186  loss_dice_2: 1.368  loss_ce_3: 0.6039  loss_mask_3: 0.1823  loss_dice_3: 1.315  loss_ce_4: 0.5653  loss_mask_4: 0.1796  loss_dice_4: 1.338  loss_ce_5: 0.5458  loss_mask_5: 0.177  loss_dice_5: 1.322  loss_ce_6: 0.5104  loss_mask_6: 0.177  loss_dice_6: 1.296  loss_ce_7: 0.5257  loss_mask_7: 0.1768  loss_dice_7: 1.266  loss_ce_8: 0.5201  loss_mask_8: 0.1764  loss_dice_8: 1.281    time: 1.0715  last_time: 1.0558  data_time: 0.0658  last_data_time: 0.0696   lr: 0.0001  max_mem: 32619M
[10/09 13:40:16] d2.utils.events INFO:  eta: 0:07:49  iter: 559  total_loss: 20.02  loss_ce: 0.502  loss_mask: 0.1833  loss_dice: 1.231  loss_ce_0: 0.5826  loss_mask_0: 0.1992  loss_dice_0: 1.414  loss_ce_1: 0.628  loss_mask_1: 0.1851  loss_dice_1: 1.346  loss_ce_2: 0.562  loss_mask_2: 0.1749  loss_dice_2: 1.324  loss_ce_3: 0.5576  loss_mask_3: 0.179  loss_dice_3: 1.264  loss_ce_4: 0.4933  loss_mask_4: 0.1825  loss_dice_4: 1.269  loss_ce_5: 0.4913  loss_mask_5: 0.1787  loss_dice_5: 1.26  loss_ce_6: 0.5022  loss_mask_6: 0.1878  loss_dice_6: 1.221  loss_ce_7: 0.5242  loss_mask_7: 0.1834  loss_dice_7: 1.222  loss_ce_8: 0.506  loss_mask_8: 0.1772  loss_dice_8: 1.239    time: 1.0718  last_time: 1.0856  data_time: 0.0759  last_data_time: 0.0954   lr: 0.0001  max_mem: 32619M
[10/09 13:40:38] d2.utils.events INFO:  eta: 0:07:27  iter: 579  total_loss: 20.44  loss_ce: 0.5085  loss_mask: 0.1975  loss_dice: 1.223  loss_ce_0: 0.5944  loss_mask_0: 0.2275  loss_dice_0: 1.442  loss_ce_1: 0.6645  loss_mask_1: 0.2166  loss_dice_1: 1.371  loss_ce_2: 0.6189  loss_mask_2: 0.2111  loss_dice_2: 1.291  loss_ce_3: 0.5653  loss_mask_3: 0.2073  loss_dice_3: 1.261  loss_ce_4: 0.5575  loss_mask_4: 0.2065  loss_dice_4: 1.273  loss_ce_5: 0.5028  loss_mask_5: 0.2045  loss_dice_5: 1.286  loss_ce_6: 0.5476  loss_mask_6: 0.1998  loss_dice_6: 1.194  loss_ce_7: 0.5061  loss_mask_7: 0.2015  loss_dice_7: 1.241  loss_ce_8: 0.5086  loss_mask_8: 0.1981  loss_dice_8: 1.193    time: 1.0718  last_time: 1.0448  data_time: 0.0711  last_data_time: 0.0598   lr: 0.0001  max_mem: 32802M
[10/09 13:40:59] d2.utils.events INFO:  eta: 0:07:06  iter: 599  total_loss: 20.15  loss_ce: 0.5425  loss_mask: 0.1809  loss_dice: 1.219  loss_ce_0: 0.5768  loss_mask_0: 0.2159  loss_dice_0: 1.483  loss_ce_1: 0.6705  loss_mask_1: 0.2002  loss_dice_1: 1.362  loss_ce_2: 0.6425  loss_mask_2: 0.1947  loss_dice_2: 1.306  loss_ce_3: 0.5772  loss_mask_3: 0.1888  loss_dice_3: 1.252  loss_ce_4: 0.5594  loss_mask_4: 0.187  loss_dice_4: 1.236  loss_ce_5: 0.5473  loss_mask_5: 0.184  loss_dice_5: 1.221  loss_ce_6: 0.5031  loss_mask_6: 0.1812  loss_dice_6: 1.227  loss_ce_7: 0.5299  loss_mask_7: 0.1837  loss_dice_7: 1.211  loss_ce_8: 0.531  loss_mask_8: 0.1807  loss_dice_8: 1.232    time: 1.0720  last_time: 1.1662  data_time: 0.0739  last_data_time: 0.1022   lr: 0.0001  max_mem: 32802M
[10/09 13:41:21] d2.utils.events INFO:  eta: 0:06:45  iter: 619  total_loss: 21.15  loss_ce: 0.5508  loss_mask: 0.1713  loss_dice: 1.273  loss_ce_0: 0.5937  loss_mask_0: 0.196  loss_dice_0: 1.561  loss_ce_1: 0.6968  loss_mask_1: 0.1841  loss_dice_1: 1.43  loss_ce_2: 0.6545  loss_mask_2: 0.1774  loss_dice_2: 1.36  loss_ce_3: 0.5815  loss_mask_3: 0.1734  loss_dice_3: 1.312  loss_ce_4: 0.5641  loss_mask_4: 0.1712  loss_dice_4: 1.272  loss_ce_5: 0.5571  loss_mask_5: 0.1736  loss_dice_5: 1.286  loss_ce_6: 0.5526  loss_mask_6: 0.1702  loss_dice_6: 1.262  loss_ce_7: 0.5357  loss_mask_7: 0.1708  loss_dice_7: 1.296  loss_ce_8: 0.5532  loss_mask_8: 0.1708  loss_dice_8: 1.278    time: 1.0727  last_time: 1.0375  data_time: 0.0766  last_data_time: 0.0579   lr: 0.0001  max_mem: 32802M
[10/09 13:41:42] d2.utils.events INFO:  eta: 0:06:23  iter: 639  total_loss: 20.4  loss_ce: 0.5132  loss_mask: 0.1742  loss_dice: 1.262  loss_ce_0: 0.5679  loss_mask_0: 0.2011  loss_dice_0: 1.503  loss_ce_1: 0.6447  loss_mask_1: 0.1831  loss_dice_1: 1.374  loss_ce_2: 0.575  loss_mask_2: 0.1795  loss_dice_2: 1.34  loss_ce_3: 0.5382  loss_mask_3: 0.178  loss_dice_3: 1.294  loss_ce_4: 0.5327  loss_mask_4: 0.1758  loss_dice_4: 1.286  loss_ce_5: 0.4963  loss_mask_5: 0.1773  loss_dice_5: 1.267  loss_ce_6: 0.4924  loss_mask_6: 0.1752  loss_dice_6: 1.273  loss_ce_7: 0.5167  loss_mask_7: 0.1739  loss_dice_7: 1.228  loss_ce_8: 0.4884  loss_mask_8: 0.1738  loss_dice_8: 1.267    time: 1.0720  last_time: 1.0119  data_time: 0.0676  last_data_time: 0.0748   lr: 0.0001  max_mem: 32802M
[10/09 13:42:04] d2.utils.events INFO:  eta: 0:06:02  iter: 659  total_loss: 20.31  loss_ce: 0.5029  loss_mask: 0.1698  loss_dice: 1.278  loss_ce_0: 0.5979  loss_mask_0: 0.1911  loss_dice_0: 1.452  loss_ce_1: 0.6607  loss_mask_1: 0.1782  loss_dice_1: 1.382  loss_ce_2: 0.6051  loss_mask_2: 0.1699  loss_dice_2: 1.376  loss_ce_3: 0.561  loss_mask_3: 0.1694  loss_dice_3: 1.259  loss_ce_4: 0.5269  loss_mask_4: 0.1695  loss_dice_4: 1.319  loss_ce_5: 0.5367  loss_mask_5: 0.169  loss_dice_5: 1.313  loss_ce_6: 0.5572  loss_mask_6: 0.1689  loss_dice_6: 1.265  loss_ce_7: 0.5105  loss_mask_7: 0.1679  loss_dice_7: 1.302  loss_ce_8: 0.5314  loss_mask_8: 0.1678  loss_dice_8: 1.241    time: 1.0720  last_time: 1.0344  data_time: 0.0702  last_data_time: 0.0679   lr: 0.0001  max_mem: 32802M
[10/09 13:42:25] d2.utils.events INFO:  eta: 0:05:40  iter: 679  total_loss: 21.32  loss_ce: 0.518  loss_mask: 0.1745  loss_dice: 1.316  loss_ce_0: 0.5768  loss_mask_0: 0.2085  loss_dice_0: 1.594  loss_ce_1: 0.672  loss_mask_1: 0.1951  loss_dice_1: 1.442  loss_ce_2: 0.6145  loss_mask_2: 0.1837  loss_dice_2: 1.433  loss_ce_3: 0.5675  loss_mask_3: 0.1791  loss_dice_3: 1.331  loss_ce_4: 0.5573  loss_mask_4: 0.1757  loss_dice_4: 1.331  loss_ce_5: 0.5288  loss_mask_5: 0.1768  loss_dice_5: 1.321  loss_ce_6: 0.5144  loss_mask_6: 0.1753  loss_dice_6: 1.309  loss_ce_7: 0.5231  loss_mask_7: 0.1767  loss_dice_7: 1.309  loss_ce_8: 0.5273  loss_mask_8: 0.1765  loss_dice_8: 1.3    time: 1.0717  last_time: 1.0292  data_time: 0.0683  last_data_time: 0.0668   lr: 0.0001  max_mem: 32802M
[10/09 13:42:46] d2.utils.events INFO:  eta: 0:05:19  iter: 699  total_loss: 20.15  loss_ce: 0.5548  loss_mask: 0.1694  loss_dice: 1.218  loss_ce_0: 0.6183  loss_mask_0: 0.2026  loss_dice_0: 1.472  loss_ce_1: 0.6898  loss_mask_1: 0.1791  loss_dice_1: 1.368  loss_ce_2: 0.6381  loss_mask_2: 0.1716  loss_dice_2: 1.293  loss_ce_3: 0.5841  loss_mask_3: 0.1715  loss_dice_3: 1.213  loss_ce_4: 0.5686  loss_mask_4: 0.1715  loss_dice_4: 1.212  loss_ce_5: 0.5493  loss_mask_5: 0.1704  loss_dice_5: 1.201  loss_ce_6: 0.5345  loss_mask_6: 0.1733  loss_dice_6: 1.225  loss_ce_7: 0.5217  loss_mask_7: 0.1736  loss_dice_7: 1.208  loss_ce_8: 0.5287  loss_mask_8: 0.1741  loss_dice_8: 1.223    time: 1.0711  last_time: 1.0403  data_time: 0.0680  last_data_time: 0.0721   lr: 0.0001  max_mem: 32802M
[10/09 13:43:07] d2.utils.events INFO:  eta: 0:04:57  iter: 719  total_loss: 20.2  loss_ce: 0.504  loss_mask: 0.1706  loss_dice: 1.227  loss_ce_0: 0.5777  loss_mask_0: 0.2031  loss_dice_0: 1.401  loss_ce_1: 0.6667  loss_mask_1: 0.1754  loss_dice_1: 1.326  loss_ce_2: 0.5698  loss_mask_2: 0.1765  loss_dice_2: 1.331  loss_ce_3: 0.5531  loss_mask_3: 0.1713  loss_dice_3: 1.228  loss_ce_4: 0.5175  loss_mask_4: 0.1729  loss_dice_4: 1.234  loss_ce_5: 0.5025  loss_mask_5: 0.1702  loss_dice_5: 1.245  loss_ce_6: 0.4982  loss_mask_6: 0.1731  loss_dice_6: 1.253  loss_ce_7: 0.486  loss_mask_7: 0.1695  loss_dice_7: 1.208  loss_ce_8: 0.4897  loss_mask_8: 0.1704  loss_dice_8: 1.227    time: 1.0707  last_time: 1.0255  data_time: 0.0668  last_data_time: 0.0819   lr: 0.0001  max_mem: 32802M
[10/09 13:43:29] d2.utils.events INFO:  eta: 0:04:36  iter: 739  total_loss: 21.1  loss_ce: 0.51  loss_mask: 0.1618  loss_dice: 1.267  loss_ce_0: 0.5683  loss_mask_0: 0.1803  loss_dice_0: 1.544  loss_ce_1: 0.6829  loss_mask_1: 0.1766  loss_dice_1: 1.454  loss_ce_2: 0.6011  loss_mask_2: 0.1694  loss_dice_2: 1.41  loss_ce_3: 0.617  loss_mask_3: 0.1675  loss_dice_3: 1.308  loss_ce_4: 0.5623  loss_mask_4: 0.1681  loss_dice_4: 1.33  loss_ce_5: 0.5268  loss_mask_5: 0.1688  loss_dice_5: 1.301  loss_ce_6: 0.5095  loss_mask_6: 0.1678  loss_dice_6: 1.331  loss_ce_7: 0.5039  loss_mask_7: 0.167  loss_dice_7: 1.29  loss_ce_8: 0.5094  loss_mask_8: 0.1656  loss_dice_8: 1.305    time: 1.0709  last_time: 1.1388  data_time: 0.0704  last_data_time: 0.0719   lr: 0.0001  max_mem: 32802M
[10/09 13:43:50] d2.utils.events INFO:  eta: 0:04:15  iter: 759  total_loss: 20.57  loss_ce: 0.5088  loss_mask: 0.1617  loss_dice: 1.216  loss_ce_0: 0.5662  loss_mask_0: 0.1875  loss_dice_0: 1.428  loss_ce_1: 0.6994  loss_mask_1: 0.1748  loss_dice_1: 1.357  loss_ce_2: 0.586  loss_mask_2: 0.1681  loss_dice_2: 1.323  loss_ce_3: 0.5792  loss_mask_3: 0.1686  loss_dice_3: 1.255  loss_ce_4: 0.5384  loss_mask_4: 0.167  loss_dice_4: 1.245  loss_ce_5: 0.547  loss_mask_5: 0.1633  loss_dice_5: 1.246  loss_ce_6: 0.5112  loss_mask_6: 0.1672  loss_dice_6: 1.239  loss_ce_7: 0.5066  loss_mask_7: 0.1649  loss_dice_7: 1.206  loss_ce_8: 0.5027  loss_mask_8: 0.1657  loss_dice_8: 1.223    time: 1.0705  last_time: 1.0434  data_time: 0.0686  last_data_time: 0.0638   lr: 0.0001  max_mem: 32802M
[10/09 13:44:12] d2.utils.events INFO:  eta: 0:03:53  iter: 779  total_loss: 20.18  loss_ce: 0.5036  loss_mask: 0.1626  loss_dice: 1.234  loss_ce_0: 0.5765  loss_mask_0: 0.1873  loss_dice_0: 1.489  loss_ce_1: 0.6396  loss_mask_1: 0.1734  loss_dice_1: 1.354  loss_ce_2: 0.5796  loss_mask_2: 0.1689  loss_dice_2: 1.314  loss_ce_3: 0.5321  loss_mask_3: 0.1687  loss_dice_3: 1.253  loss_ce_4: 0.5234  loss_mask_4: 0.1675  loss_dice_4: 1.279  loss_ce_5: 0.4939  loss_mask_5: 0.1672  loss_dice_5: 1.257  loss_ce_6: 0.4963  loss_mask_6: 0.1661  loss_dice_6: 1.261  loss_ce_7: 0.5028  loss_mask_7: 0.1655  loss_dice_7: 1.247  loss_ce_8: 0.5086  loss_mask_8: 0.1639  loss_dice_8: 1.224    time: 1.0704  last_time: 1.0435  data_time: 0.0693  last_data_time: 0.0744   lr: 0.0001  max_mem: 32802M
[10/09 13:44:33] d2.utils.events INFO:  eta: 0:03:32  iter: 799  total_loss: 20.81  loss_ce: 0.4704  loss_mask: 0.1771  loss_dice: 1.276  loss_ce_0: 0.5847  loss_mask_0: 0.2128  loss_dice_0: 1.521  loss_ce_1: 0.6909  loss_mask_1: 0.1915  loss_dice_1: 1.438  loss_ce_2: 0.6165  loss_mask_2: 0.1822  loss_dice_2: 1.317  loss_ce_3: 0.5518  loss_mask_3: 0.1828  loss_dice_3: 1.321  loss_ce_4: 0.4977  loss_mask_4: 0.1785  loss_dice_4: 1.329  loss_ce_5: 0.5156  loss_mask_5: 0.1776  loss_dice_5: 1.289  loss_ce_6: 0.5082  loss_mask_6: 0.178  loss_dice_6: 1.264  loss_ce_7: 0.4962  loss_mask_7: 0.1758  loss_dice_7: 1.289  loss_ce_8: 0.4972  loss_mask_8: 0.1764  loss_dice_8: 1.287    time: 1.0703  last_time: 1.0335  data_time: 0.0693  last_data_time: 0.0550   lr: 0.0001  max_mem: 32802M
[10/09 13:44:54] d2.utils.events INFO:  eta: 0:03:11  iter: 819  total_loss: 20.55  loss_ce: 0.4719  loss_mask: 0.1891  loss_dice: 1.214  loss_ce_0: 0.5542  loss_mask_0: 0.2199  loss_dice_0: 1.484  loss_ce_1: 0.6467  loss_mask_1: 0.2009  loss_dice_1: 1.378  loss_ce_2: 0.5621  loss_mask_2: 0.201  loss_dice_2: 1.306  loss_ce_3: 0.5374  loss_mask_3: 0.1932  loss_dice_3: 1.253  loss_ce_4: 0.5447  loss_mask_4: 0.1935  loss_dice_4: 1.285  loss_ce_5: 0.5117  loss_mask_5: 0.1912  loss_dice_5: 1.246  loss_ce_6: 0.4924  loss_mask_6: 0.1919  loss_dice_6: 1.224  loss_ce_7: 0.4852  loss_mask_7: 0.193  loss_dice_7: 1.206  loss_ce_8: 0.4864  loss_mask_8: 0.1921  loss_dice_8: 1.246    time: 1.0700  last_time: 1.0512  data_time: 0.0697  last_data_time: 0.0812   lr: 0.0001  max_mem: 32802M
[10/09 13:45:15] d2.utils.events INFO:  eta: 0:02:49  iter: 839  total_loss: 20.5  loss_ce: 0.4873  loss_mask: 0.1835  loss_dice: 1.263  loss_ce_0: 0.6332  loss_mask_0: 0.205  loss_dice_0: 1.5  loss_ce_1: 0.6651  loss_mask_1: 0.2003  loss_dice_1: 1.392  loss_ce_2: 0.6409  loss_mask_2: 0.19  loss_dice_2: 1.311  loss_ce_3: 0.5721  loss_mask_3: 0.1855  loss_dice_3: 1.276  loss_ce_4: 0.5443  loss_mask_4: 0.1829  loss_dice_4: 1.261  loss_ce_5: 0.4948  loss_mask_5: 0.1827  loss_dice_5: 1.273  loss_ce_6: 0.5025  loss_mask_6: 0.1858  loss_dice_6: 1.248  loss_ce_7: 0.4993  loss_mask_7: 0.1841  loss_dice_7: 1.236  loss_ce_8: 0.5022  loss_mask_8: 0.1851  loss_dice_8: 1.242    time: 1.0696  last_time: 1.0282  data_time: 0.0673  last_data_time: 0.0675   lr: 0.0001  max_mem: 32802M
[10/09 13:45:37] d2.utils.events INFO:  eta: 0:02:28  iter: 859  total_loss: 20.18  loss_ce: 0.488  loss_mask: 0.1732  loss_dice: 1.253  loss_ce_0: 0.5857  loss_mask_0: 0.1973  loss_dice_0: 1.403  loss_ce_1: 0.6311  loss_mask_1: 0.1854  loss_dice_1: 1.371  loss_ce_2: 0.6162  loss_mask_2: 0.1808  loss_dice_2: 1.329  loss_ce_3: 0.563  loss_mask_3: 0.1782  loss_dice_3: 1.26  loss_ce_4: 0.5391  loss_mask_4: 0.1777  loss_dice_4: 1.24  loss_ce_5: 0.511  loss_mask_5: 0.174  loss_dice_5: 1.302  loss_ce_6: 0.5104  loss_mask_6: 0.1761  loss_dice_6: 1.233  loss_ce_7: 0.5007  loss_mask_7: 0.1739  loss_dice_7: 1.23  loss_ce_8: 0.5077  loss_mask_8: 0.1756  loss_dice_8: 1.245    time: 1.0697  last_time: 1.0737  data_time: 0.0704  last_data_time: 0.0682   lr: 0.0001  max_mem: 32802M
[10/09 13:45:58] d2.utils.events INFO:  eta: 0:02:07  iter: 879  total_loss: 20.4  loss_ce: 0.4751  loss_mask: 0.1632  loss_dice: 1.272  loss_ce_0: 0.5717  loss_mask_0: 0.1948  loss_dice_0: 1.509  loss_ce_1: 0.6089  loss_mask_1: 0.1746  loss_dice_1: 1.404  loss_ce_2: 0.5733  loss_mask_2: 0.1692  loss_dice_2: 1.347  loss_ce_3: 0.536  loss_mask_3: 0.1663  loss_dice_3: 1.275  loss_ce_4: 0.499  loss_mask_4: 0.1668  loss_dice_4: 1.296  loss_ce_5: 0.4842  loss_mask_5: 0.1649  loss_dice_5: 1.3  loss_ce_6: 0.4945  loss_mask_6: 0.1656  loss_dice_6: 1.285  loss_ce_7: 0.4631  loss_mask_7: 0.1652  loss_dice_7: 1.284  loss_ce_8: 0.4799  loss_mask_8: 0.1656  loss_dice_8: 1.249    time: 1.0697  last_time: 1.0813  data_time: 0.0705  last_data_time: 0.0696   lr: 0.0001  max_mem: 32802M
[10/09 13:46:20] d2.utils.events INFO:  eta: 0:01:46  iter: 899  total_loss: 19.46  loss_ce: 0.475  loss_mask: 0.1728  loss_dice: 1.191  loss_ce_0: 0.5326  loss_mask_0: 0.2004  loss_dice_0: 1.415  loss_ce_1: 0.6202  loss_mask_1: 0.185  loss_dice_1: 1.345  loss_ce_2: 0.5777  loss_mask_2: 0.1761  loss_dice_2: 1.31  loss_ce_3: 0.5048  loss_mask_3: 0.1752  loss_dice_3: 1.245  loss_ce_4: 0.4804  loss_mask_4: 0.1749  loss_dice_4: 1.226  loss_ce_5: 0.4968  loss_mask_5: 0.1719  loss_dice_5: 1.21  loss_ce_6: 0.4759  loss_mask_6: 0.173  loss_dice_6: 1.187  loss_ce_7: 0.466  loss_mask_7: 0.174  loss_dice_7: 1.226  loss_ce_8: 0.4688  loss_mask_8: 0.1742  loss_dice_8: 1.212    time: 1.0705  last_time: 1.1051  data_time: 0.0724  last_data_time: 0.0615   lr: 0.0001  max_mem: 32802M
[10/09 13:46:42] d2.utils.events INFO:  eta: 0:01:25  iter: 919  total_loss: 20.15  loss_ce: 0.5117  loss_mask: 0.1779  loss_dice: 1.227  loss_ce_0: 0.5735  loss_mask_0: 0.2063  loss_dice_0: 1.46  loss_ce_1: 0.6398  loss_mask_1: 0.193  loss_dice_1: 1.347  loss_ce_2: 0.5813  loss_mask_2: 0.1829  loss_dice_2: 1.318  loss_ce_3: 0.5737  loss_mask_3: 0.1809  loss_dice_3: 1.268  loss_ce_4: 0.4958  loss_mask_4: 0.1772  loss_dice_4: 1.253  loss_ce_5: 0.4816  loss_mask_5: 0.1789  loss_dice_5: 1.259  loss_ce_6: 0.5191  loss_mask_6: 0.173  loss_dice_6: 1.24  loss_ce_7: 0.5071  loss_mask_7: 0.176  loss_dice_7: 1.263  loss_ce_8: 0.493  loss_mask_8: 0.1761  loss_dice_8: 1.205    time: 1.0707  last_time: 1.0140  data_time: 0.0737  last_data_time: 0.0879   lr: 0.0001  max_mem: 32802M
[10/09 13:47:04] d2.utils.events INFO:  eta: 0:01:03  iter: 939  total_loss: 20.68  loss_ce: 0.477  loss_mask: 0.1749  loss_dice: 1.322  loss_ce_0: 0.572  loss_mask_0: 0.1956  loss_dice_0: 1.55  loss_ce_1: 0.6625  loss_mask_1: 0.1874  loss_dice_1: 1.4  loss_ce_2: 0.6074  loss_mask_2: 0.1755  loss_dice_2: 1.371  loss_ce_3: 0.5533  loss_mask_3: 0.1729  loss_dice_3: 1.304  loss_ce_4: 0.5385  loss_mask_4: 0.1733  loss_dice_4: 1.304  loss_ce_5: 0.523  loss_mask_5: 0.1754  loss_dice_5: 1.316  loss_ce_6: 0.5269  loss_mask_6: 0.1749  loss_dice_6: 1.278  loss_ce_7: 0.5076  loss_mask_7: 0.1755  loss_dice_7: 1.295  loss_ce_8: 0.4875  loss_mask_8: 0.1764  loss_dice_8: 1.327    time: 1.0708  last_time: 1.1072  data_time: 0.0685  last_data_time: 0.0700   lr: 0.0001  max_mem: 32802M
[10/09 13:47:26] d2.utils.events INFO:  eta: 0:00:42  iter: 959  total_loss: 21.02  loss_ce: 0.4824  loss_mask: 0.1635  loss_dice: 1.334  loss_ce_0: 0.5859  loss_mask_0: 0.1934  loss_dice_0: 1.526  loss_ce_1: 0.6523  loss_mask_1: 0.1763  loss_dice_1: 1.403  loss_ce_2: 0.6135  loss_mask_2: 0.171  loss_dice_2: 1.377  loss_ce_3: 0.5614  loss_mask_3: 0.1656  loss_dice_3: 1.334  loss_ce_4: 0.5495  loss_mask_4: 0.1675  loss_dice_4: 1.334  loss_ce_5: 0.536  loss_mask_5: 0.1643  loss_dice_5: 1.301  loss_ce_6: 0.5218  loss_mask_6: 0.1657  loss_dice_6: 1.341  loss_ce_7: 0.4782  loss_mask_7: 0.1649  loss_dice_7: 1.336  loss_ce_8: 0.4912  loss_mask_8: 0.1616  loss_dice_8: 1.332    time: 1.0716  last_time: 1.1123  data_time: 0.0796  last_data_time: 0.0784   lr: 0.0001  max_mem: 32802M
[10/09 13:47:47] d2.utils.events INFO:  eta: 0:00:21  iter: 979  total_loss: 19.41  loss_ce: 0.4671  loss_mask: 0.186  loss_dice: 1.188  loss_ce_0: 0.5695  loss_mask_0: 0.2119  loss_dice_0: 1.38  loss_ce_1: 0.6334  loss_mask_1: 0.1991  loss_dice_1: 1.304  loss_ce_2: 0.5845  loss_mask_2: 0.1941  loss_dice_2: 1.25  loss_ce_3: 0.528  loss_mask_3: 0.1921  loss_dice_3: 1.193  loss_ce_4: 0.5208  loss_mask_4: 0.1899  loss_dice_4: 1.216  loss_ce_5: 0.4854  loss_mask_5: 0.1904  loss_dice_5: 1.187  loss_ce_6: 0.4451  loss_mask_6: 0.1908  loss_dice_6: 1.193  loss_ce_7: 0.4709  loss_mask_7: 0.1892  loss_dice_7: 1.209  loss_ce_8: 0.4576  loss_mask_8: 0.1897  loss_dice_8: 1.184    time: 1.0712  last_time: 1.3278  data_time: 0.0653  last_data_time: 0.0763   lr: 0.0001  max_mem: 32802M
[10/09 13:48:08] fvcore.common.checkpoint INFO: Saving checkpoint to /tsi/hi-paris/GB/segmentation/results/In_vocab/r50_008_1000_19_a_decoder/model_final.pth
[10/09 13:48:12] d2.utils.events INFO:  eta: 0:00:00  iter: 999  total_loss: 19.92  loss_ce: 0.4829  loss_mask: 0.1644  loss_dice: 1.234  loss_ce_0: 0.559  loss_mask_0: 0.2004  loss_dice_0: 1.453  loss_ce_1: 0.6292  loss_mask_1: 0.1749  loss_dice_1: 1.347  loss_ce_2: 0.5757  loss_mask_2: 0.1711  loss_dice_2: 1.287  loss_ce_3: 0.5342  loss_mask_3: 0.1636  loss_dice_3: 1.245  loss_ce_4: 0.4933  loss_mask_4: 0.1652  loss_dice_4: 1.284  loss_ce_5: 0.5023  loss_mask_5: 0.1648  loss_dice_5: 1.258  loss_ce_6: 0.4721  loss_mask_6: 0.1656  loss_dice_6: 1.231  loss_ce_7: 0.4798  loss_mask_7: 0.1648  loss_dice_7: 1.241  loss_ce_8: 0.4826  loss_mask_8: 0.1647  loss_dice_8: 1.249    time: 1.0710  last_time: 1.0521  data_time: 0.0702  last_data_time: 0.0669   lr: 0.0001  max_mem: 32802M
[10/09 13:48:12] d2.engine.hooks INFO: Overall training speed: 998 iterations in 0:17:48 (1.0711 s / it)
[10/09 13:48:12] d2.engine.hooks INFO: Total training time: 0:17:55 (0:00:06 on hooks)
[10/09 13:48:12] fcclip.data.datasets.register_cityscapes_panoptic INFO: 3 cities found in 'datasets/cityscapes/leftImg8bit/val'.
[10/09 13:48:12] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(1024, 1024), max_size=2560, sample_style='choice')]
[10/09 13:48:12] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/09 13:48:12] d2.data.common INFO: Serializing 500 elements to byte tensors and concatenating them all ...
[10/09 13:48:12] d2.data.common INFO: Serialized dataset takes 0.74 MiB
[10/09 13:48:12] d2.evaluation.evaluator INFO: Start inference on 500 batches
[10/09 18:53:58] detectron2 INFO: Rank of current process: 0. World size: 1
[10/09 18:53:59] detectron2 INFO: Environment info:
-------------------------------  -------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.8.19 | packaged by conda-forge | (default, Mar 20 2024, 12:47:35) [GCC 12.3.0]
numpy                            1.24.4
detectron2                       0.6 @/home/ids/gbrison/segmentation/segmentation/detectron2/detectron2
Compiler                         GCC 11.4
CUDA compiler                    CUDA 12.5
detectron2 arch flags            8.9
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.0+cu121 @/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA L40S (arch=8.9)
Driver version                   560.35.03
CUDA_HOME                        /usr/local/cuda
Pillow                           9.4.0
torchvision                      0.19.0+cu121 @/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torchvision
torchvision arch flags           5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.10.0
-------------------------------  -------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX512
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.0, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/09 18:53:59] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco/panoptic-segmentation/fcclip/r50_exp_1000_a_decoder.yaml', dist_url='tcp://127.0.0.1:51163', eval_only=False, machine_rank=0, num_gpus=1, num_machines=1, opts=[], resume=False)
[10/09 18:53:59] detectron2 INFO: Contents of args.config_file=configs/coco/panoptic-segmentation/fcclip/r50_exp_1000_a_decoder.yaml:
_BASE_: ./fcclip_convnext_large_eval_ade20k_r50.yaml

INPUT:
  MIN_SIZE_TEST: 1024
  MAX_SIZE_TEST: 2560

MODEL:
  SEM_SEG_HEAD:
    NUM_CLASSES: 19
  WEIGHTS: /tsi/hi-paris/GB/segmentation/models/fcclip_cocopan_r50.pth
OUTPUT_DIR: /tsi/hi-paris/GB/segmentation/results/In_vocab/r50_008_1000_19_a_decoder
DATASETS:
  TRAIN: ("openvocab_cityscapes_fine_panoptic_train",)
  TEST: ("openvocab_cityscapes_fine_panoptic_val",)
SOLVER:
  IMS_PER_BATCH: 8
  MAX_ITER: 1000
TEST:
  EVAL_PERIOD: 1000


[10/09 18:53:59] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - openvocab_cityscapes_fine_panoptic_val
  TRAIN:
  - openvocab_cityscapes_fine_panoptic_train
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: false
  CROP:
    ENABLED: false
    MINIMUM_INST_AREA: 1
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  DATASET_MAPPER_NAME: coco_panoptic_lsj
  FORMAT: RGB
  IMAGE_SIZE: 1024
  MASK_FORMAT: polygon
  MAX_SCALE: 2.0
  MAX_SIZE_TEST: 2560
  MAX_SIZE_TRAIN: 1333
  MIN_SCALE: 0.1
  MIN_SIZE_TEST: 1024
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: -1
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 0
    NAME: CLIP
  DEVICE: cuda
  FC_CLIP:
    CLIP_MODEL_NAME: RN50
    CLIP_PRETRAINED_WEIGHTS: openai
    EMBED_DIM: 1024
    ENSEMBLE_ON_VALID_MASK: true
    GEOMETRIC_ENSEMBLE_ALPHA: 0.4
    GEOMETRIC_ENSEMBLE_BETA: 0.8
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_FORMER:
    CLASS_WEIGHT: 2.0
    DEC_LAYERS: 10
    DEEP_SUPERVISION: true
    DICE_WEIGHT: 5.0
    DIM_FEEDFORWARD: 2048
    DROPOUT: 0.0
    ENC_LAYERS: 0
    ENFORCE_INPUT_PROJ: false
    HIDDEN_DIM: 256
    IMPORTANCE_SAMPLE_RATIO: 0.75
    MASK_WEIGHT: 5.0
    NHEADS: 8
    NO_OBJECT_WEIGHT: 0.1
    NUM_OBJECT_QUERIES: 250
    OVERSAMPLE_RATIO: 3.0
    PRE_NORM: false
    SIZE_DIVISIBILITY: 32
    TEST:
      INSTANCE_ON: true
      OBJECT_MASK_THRESHOLD: 0.0
      OVERLAP_THRESHOLD: 0.8
      PANOPTIC_ON: true
      SEMANTIC_ON: true
      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: false
    TRAIN_NUM_POINTS: 12544
    TRANSFORMER_DECODER_NAME: MultiScaleMaskedTransformerDecoder
    TRANSFORMER_IN_FEATURE: multi_scale_pixel_decoder
  MASK_ON: false
  META_ARCHITECTURE: FCCLIP
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 122.7709383
  - 116.7460125
  - 104.09373615
  PIXEL_STD:
  - 68.5005327
  - 66.6321579
  - 70.32316305
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 1
    - 1
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: basic
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 256
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES:
    - res3
    - res4
    - res5
    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8
    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    MASK_DIM: 256
    NAME: FCCLIPHead
    NORM: GN
    NUM_CLASSES: 19
    PIXEL_DECODER_NAME: MSDeformAttnPixelDecoder
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    TRANSFORMER_ENC_LAYERS: 6
    USE_DEPTHWISE_SEPARABLE_CONV: false
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    USE_CHECKPOINT: false
    WINDOW_SIZE: 7
  WEIGHTS: /tsi/hi-paris/GB/segmentation/models/fcclip_cocopan_r50.pth
OUTPUT_DIR: /tsi/hi-paris/GB/segmentation/results/In_vocab/r50_008_1000_19_a_decoder
SEED: -1
SOLVER:
  AMP:
    ENABLED: true
  BACKBONE_MULTIPLIER: 0.1
  BASE_LR: 0.0001
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 5000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.01
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 8
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 1000
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 327778
  - 355092
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 1000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[10/09 18:53:59] detectron2 INFO: Full config saved to /tsi/hi-paris/GB/segmentation/results/In_vocab/r50_008_1000_19_a_decoder/config.yaml
[10/09 18:53:59] d2.utils.env INFO: Using a generated random seed 62691481
[10/09 18:54:03] d2.engine.defaults INFO: Model:
FCCLIP(
  (backbone): CLIP(
    (clip_model): CLIP(
      (visual): ModifiedResNet(
        (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
        (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        (layer1): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
            (downsample): Sequential(
              (-1): AvgPool2d(kernel_size=1, stride=1, padding=0)
              (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
        )
        (layer2): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
            (downsample): Sequential(
              (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (3): Bottleneck(
            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
        )
        (layer3): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
            (downsample): Sequential(
              (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
              (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (3): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (4): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (5): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
        )
        (layer4): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
            (downsample): Sequential(
              (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
              (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
        )
        (attnpool): AttentionPool2d(
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (c_proj): Linear(in_features=2048, out_features=1024, bias=True)
        )
      )
      (transformer): Transformer(
        (resblocks): ModuleList(
          (0-11): 12 x ResidualAttentionBlock(
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ls_2): Identity()
          )
        )
      )
      (token_embedding): Embedding(49408, 512)
      (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
  )
  (sem_seg_head): FCCLIPHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0-8): 9 x SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0-8): 9 x CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0-8): 9 x FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(250, 256)
      (query_embed): Embedding(250, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0-2): 3 x Sequential()
      )
      (mask_embed): MLP(
        (layers): ModuleList(
          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (mask_pooling): MaskPooling()
      (_mask_pooling_proj): Sequential(
        (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
      )
      (class_embed): MLP(
        (layers): ModuleList(
          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=1024, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 19
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
  (conv1): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv2): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU()
  (mask_pooling): MaskPooling()
  (decoder_adapter): DecoderAdapter(
    (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
    (conv2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    (norm1): LayerNorm((64, 1, 1), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((256, 1, 1), eps=1e-05, elementwise_affine=True)
    (relu): ReLU()
  )
  (void_embedding): Embedding(1, 1024)
)
[10/09 18:54:03] fcclip.data.dataset_mappers.mask_former_semantic_dataset_mapper INFO: [MaskFormerPanopticDatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(800,), max_size=1333, sample_style='choice'), RandomFlip()]
[10/09 18:54:03] fcclip.data.datasets.register_cityscapes_panoptic INFO: 18 cities found in 'datasets/cityscapes/leftImg8bit/train'.
[10/09 18:54:03] d2.data.build INFO: Using training sampler TrainingSampler
[10/09 18:54:03] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/09 18:54:03] d2.data.common INFO: Serializing 2975 elements to byte tensors and concatenating them all ...
[10/09 18:54:03] d2.data.common INFO: Serialized dataset takes 4.12 MiB
[10/09 18:54:03] d2.data.build INFO: Making batched data loader with batch_size=8
[10/09 18:54:03] d2.solver.build WARNING: SOLVER.STEPS contains values larger than SOLVER.MAX_ITER. These values will be ignored.
[10/09 18:54:03] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /tsi/hi-paris/GB/segmentation/models/fcclip_cocopan_r50.pth ...
[10/09 18:54:03] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /tsi/hi-paris/GB/segmentation/models/fcclip_cocopan_r50.pth ...
[10/09 18:54:06] fvcore.common.checkpoint WARNING: Skip loading parameter 'criterion.empty_weight' to the model due to incompatible shapes: (134,) in the checkpoint but (20,) in the model! You might want to double check if this is expected.
[10/09 18:54:06] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.clip_model.ln_final.{bias, weight}[0m
[34mbackbone.clip_model.token_embedding.weight[0m
[34mbackbone.clip_model.transformer.resblocks.0.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.attnpool.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.attnpool.k_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.attnpool.positional_embedding[0m
[34mbackbone.clip_model.visual.attnpool.q_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.attnpool.v_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.conv1.weight[0m
[34mbackbone.clip_model.visual.conv2.weight[0m
[34mbackbone.clip_model.visual.conv3.weight[0m
[34mbackbone.clip_model.visual.layer1.0.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.0.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.0.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.0.conv1.weight[0m
[34mbackbone.clip_model.visual.layer1.0.conv2.weight[0m
[34mbackbone.clip_model.visual.layer1.0.conv3.weight[0m
[34mbackbone.clip_model.visual.layer1.0.downsample.0.weight[0m
[34mbackbone.clip_model.visual.layer1.0.downsample.1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.1.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.1.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.1.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.1.conv1.weight[0m
[34mbackbone.clip_model.visual.layer1.1.conv2.weight[0m
[34mbackbone.clip_model.visual.layer1.1.conv3.weight[0m
[34mbackbone.clip_model.visual.layer1.2.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.2.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.2.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.2.conv1.weight[0m
[34mbackbone.clip_model.visual.layer1.2.conv2.weight[0m
[34mbackbone.clip_model.visual.layer1.2.conv3.weight[0m
[34mbackbone.clip_model.visual.layer2.0.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.0.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.0.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.0.conv1.weight[0m
[34mbackbone.clip_model.visual.layer2.0.conv2.weight[0m
[34mbackbone.clip_model.visual.layer2.0.conv3.weight[0m
[34mbackbone.clip_model.visual.layer2.0.downsample.0.weight[0m
[34mbackbone.clip_model.visual.layer2.0.downsample.1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.1.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.1.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.1.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.1.conv1.weight[0m
[34mbackbone.clip_model.visual.layer2.1.conv2.weight[0m
[34mbackbone.clip_model.visual.layer2.1.conv3.weight[0m
[34mbackbone.clip_model.visual.layer2.2.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.2.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.2.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.2.conv1.weight[0m
[34mbackbone.clip_model.visual.layer2.2.conv2.weight[0m
[34mbackbone.clip_model.visual.layer2.2.conv3.weight[0m
[34mbackbone.clip_model.visual.layer2.3.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.3.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.3.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.3.conv1.weight[0m
[34mbackbone.clip_model.visual.layer2.3.conv2.weight[0m
[34mbackbone.clip_model.visual.layer2.3.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.0.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.0.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.0.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.0.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.0.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.0.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.0.downsample.0.weight[0m
[34mbackbone.clip_model.visual.layer3.0.downsample.1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.1.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.1.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.1.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.1.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.1.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.1.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.2.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.2.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.2.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.2.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.2.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.2.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.3.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.3.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.3.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.3.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.3.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.3.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.4.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.4.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.4.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.4.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.4.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.4.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.5.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.5.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.5.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.5.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.5.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.5.conv3.weight[0m
[34mbackbone.clip_model.visual.layer4.0.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.0.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.0.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.0.conv1.weight[0m
[34mbackbone.clip_model.visual.layer4.0.conv2.weight[0m
[34mbackbone.clip_model.visual.layer4.0.conv3.weight[0m
[34mbackbone.clip_model.visual.layer4.0.downsample.0.weight[0m
[34mbackbone.clip_model.visual.layer4.0.downsample.1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.1.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.1.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.1.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.1.conv1.weight[0m
[34mbackbone.clip_model.visual.layer4.1.conv2.weight[0m
[34mbackbone.clip_model.visual.layer4.1.conv3.weight[0m
[34mbackbone.clip_model.visual.layer4.2.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.2.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.2.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.2.conv1.weight[0m
[34mbackbone.clip_model.visual.layer4.2.conv2.weight[0m
[34mbackbone.clip_model.visual.layer4.2.conv3.weight[0m
[34mbackbone.clip_model.{logit_scale, positional_embedding, text_projection}[0m
[34mbn1.{bias, running_mean, running_var, weight}[0m
[34mbn2.{bias, running_mean, running_var, weight}[0m
[34mconv1.{bias, weight}[0m
[34mconv2.{bias, weight}[0m
[34mcriterion.empty_weight[0m
[34mdecoder_adapter.conv1.{bias, weight}[0m
[34mdecoder_adapter.conv2.{bias, weight}[0m
[34mdecoder_adapter.norm1.{bias, weight}[0m
[34mdecoder_adapter.norm2.{bias, weight}[0m
[10/09 18:54:06] d2.engine.train_loop INFO: Starting training from iteration 0
[10/09 18:54:31] d2.utils.events INFO:  eta: 0:17:29  iter: 19  total_loss: 32.11  loss_ce: 1.245  loss_mask: 0.274  loss_dice: 1.497  loss_ce_0: 1.366  loss_mask_0: 0.3011  loss_dice_0: 1.802  loss_ce_1: 1.392  loss_mask_1: 0.2859  loss_dice_1: 1.697  loss_ce_2: 1.357  loss_mask_2: 0.2747  loss_dice_2: 1.619  loss_ce_3: 1.29  loss_mask_3: 0.2852  loss_dice_3: 1.579  loss_ce_4: 1.263  loss_mask_4: 0.2856  loss_dice_4: 1.561  loss_ce_5: 1.269  loss_mask_5: 0.2822  loss_dice_5: 1.548  loss_ce_6: 1.277  loss_mask_6: 0.2776  loss_dice_6: 1.561  loss_ce_7: 1.25  loss_mask_7: 0.2799  loss_dice_7: 1.519  loss_ce_8: 1.241  loss_mask_8: 0.2759  loss_dice_8: 1.516    time: 1.0781  last_time: 1.1316  data_time: 0.1058  last_data_time: 0.0546   lr: 0.0001  max_mem: 32601M
[10/09 18:54:52] d2.utils.events INFO:  eta: 0:17:02  iter: 39  total_loss: 26.15  loss_ce: 0.792  loss_mask: 0.247  loss_dice: 1.447  loss_ce_0: 0.7998  loss_mask_0: 0.2718  loss_dice_0: 1.799  loss_ce_1: 0.9598  loss_mask_1: 0.2679  loss_dice_1: 1.617  loss_ce_2: 0.8586  loss_mask_2: 0.2579  loss_dice_2: 1.563  loss_ce_3: 0.8467  loss_mask_3: 0.2553  loss_dice_3: 1.517  loss_ce_4: 0.8272  loss_mask_4: 0.2469  loss_dice_4: 1.53  loss_ce_5: 0.8215  loss_mask_5: 0.2475  loss_dice_5: 1.479  loss_ce_6: 0.8464  loss_mask_6: 0.2443  loss_dice_6: 1.433  loss_ce_7: 0.8016  loss_mask_7: 0.2484  loss_dice_7: 1.452  loss_ce_8: 0.7932  loss_mask_8: 0.2573  loss_dice_8: 1.471    time: 1.0666  last_time: 1.1017  data_time: 0.0708  last_data_time: 0.0610   lr: 0.0001  max_mem: 32601M
[10/09 18:55:14] d2.utils.events INFO:  eta: 0:16:42  iter: 59  total_loss: 24.57  loss_ce: 0.6686  loss_mask: 0.2096  loss_dice: 1.374  loss_ce_0: 0.7439  loss_mask_0: 0.2448  loss_dice_0: 1.672  loss_ce_1: 0.8795  loss_mask_1: 0.2238  loss_dice_1: 1.561  loss_ce_2: 0.7945  loss_mask_2: 0.2169  loss_dice_2: 1.495  loss_ce_3: 0.7712  loss_mask_3: 0.2148  loss_dice_3: 1.437  loss_ce_4: 0.711  loss_mask_4: 0.2157  loss_dice_4: 1.453  loss_ce_5: 0.7328  loss_mask_5: 0.211  loss_dice_5: 1.473  loss_ce_6: 0.712  loss_mask_6: 0.215  loss_dice_6: 1.401  loss_ce_7: 0.6965  loss_mask_7: 0.2112  loss_dice_7: 1.423  loss_ce_8: 0.6987  loss_mask_8: 0.2106  loss_dice_8: 1.391    time: 1.0684  last_time: 1.0426  data_time: 0.0738  last_data_time: 0.0574   lr: 0.0001  max_mem: 32601M
[10/09 18:55:35] d2.utils.events INFO:  eta: 0:16:18  iter: 79  total_loss: 23.46  loss_ce: 0.6469  loss_mask: 0.2168  loss_dice: 1.339  loss_ce_0: 0.6659  loss_mask_0: 0.2437  loss_dice_0: 1.645  loss_ce_1: 0.8176  loss_mask_1: 0.2224  loss_dice_1: 1.476  loss_ce_2: 0.7189  loss_mask_2: 0.2212  loss_dice_2: 1.378  loss_ce_3: 0.6816  loss_mask_3: 0.2222  loss_dice_3: 1.359  loss_ce_4: 0.678  loss_mask_4: 0.2215  loss_dice_4: 1.361  loss_ce_5: 0.653  loss_mask_5: 0.2197  loss_dice_5: 1.359  loss_ce_6: 0.6407  loss_mask_6: 0.2199  loss_dice_6: 1.336  loss_ce_7: 0.6407  loss_mask_7: 0.2178  loss_dice_7: 1.354  loss_ce_8: 0.6455  loss_mask_8: 0.2157  loss_dice_8: 1.307    time: 1.0630  last_time: 1.0532  data_time: 0.0667  last_data_time: 0.0753   lr: 0.0001  max_mem: 32601M
[10/09 18:55:56] d2.utils.events INFO:  eta: 0:15:51  iter: 99  total_loss: 22.68  loss_ce: 0.6254  loss_mask: 0.2124  loss_dice: 1.322  loss_ce_0: 0.7279  loss_mask_0: 0.25  loss_dice_0: 1.628  loss_ce_1: 0.8374  loss_mask_1: 0.2198  loss_dice_1: 1.442  loss_ce_2: 0.7404  loss_mask_2: 0.2143  loss_dice_2: 1.384  loss_ce_3: 0.713  loss_mask_3: 0.2112  loss_dice_3: 1.357  loss_ce_4: 0.6437  loss_mask_4: 0.2127  loss_dice_4: 1.344  loss_ce_5: 0.6546  loss_mask_5: 0.2133  loss_dice_5: 1.357  loss_ce_6: 0.6692  loss_mask_6: 0.2117  loss_dice_6: 1.301  loss_ce_7: 0.6581  loss_mask_7: 0.2139  loss_dice_7: 1.304  loss_ce_8: 0.6162  loss_mask_8: 0.2116  loss_dice_8: 1.29    time: 1.0586  last_time: 1.0489  data_time: 0.0625  last_data_time: 0.0627   lr: 0.0001  max_mem: 32601M
[10/09 18:56:17] d2.utils.events INFO:  eta: 0:15:27  iter: 119  total_loss: 22.81  loss_ce: 0.641  loss_mask: 0.2003  loss_dice: 1.361  loss_ce_0: 0.6703  loss_mask_0: 0.2217  loss_dice_0: 1.644  loss_ce_1: 0.8401  loss_mask_1: 0.2051  loss_dice_1: 1.539  loss_ce_2: 0.7357  loss_mask_2: 0.2073  loss_dice_2: 1.442  loss_ce_3: 0.683  loss_mask_3: 0.1991  loss_dice_3: 1.384  loss_ce_4: 0.6671  loss_mask_4: 0.1992  loss_dice_4: 1.394  loss_ce_5: 0.6611  loss_mask_5: 0.1995  loss_dice_5: 1.39  loss_ce_6: 0.6345  loss_mask_6: 0.2012  loss_dice_6: 1.342  loss_ce_7: 0.6053  loss_mask_7: 0.1997  loss_dice_7: 1.352  loss_ce_8: 0.6088  loss_mask_8: 0.1997  loss_dice_8: 1.35    time: 1.0572  last_time: 1.0761  data_time: 0.0684  last_data_time: 0.0782   lr: 0.0001  max_mem: 32601M
[10/09 18:56:38] d2.utils.events INFO:  eta: 0:15:08  iter: 139  total_loss: 23.26  loss_ce: 0.6036  loss_mask: 0.1984  loss_dice: 1.351  loss_ce_0: 0.6621  loss_mask_0: 0.2216  loss_dice_0: 1.609  loss_ce_1: 0.8166  loss_mask_1: 0.2082  loss_dice_1: 1.541  loss_ce_2: 0.7564  loss_mask_2: 0.2029  loss_dice_2: 1.428  loss_ce_3: 0.7139  loss_mask_3: 0.1993  loss_dice_3: 1.412  loss_ce_4: 0.6484  loss_mask_4: 0.2001  loss_dice_4: 1.388  loss_ce_5: 0.6431  loss_mask_5: 0.1989  loss_dice_5: 1.361  loss_ce_6: 0.61  loss_mask_6: 0.196  loss_dice_6: 1.397  loss_ce_7: 0.6076  loss_mask_7: 0.1937  loss_dice_7: 1.378  loss_ce_8: 0.6102  loss_mask_8: 0.1972  loss_dice_8: 1.394    time: 1.0581  last_time: 1.0451  data_time: 0.0742  last_data_time: 0.0720   lr: 0.0001  max_mem: 32601M
[10/09 18:57:00] d2.utils.events INFO:  eta: 0:14:47  iter: 159  total_loss: 21.68  loss_ce: 0.5726  loss_mask: 0.1808  loss_dice: 1.298  loss_ce_0: 0.6499  loss_mask_0: 0.2164  loss_dice_0: 1.579  loss_ce_1: 0.7948  loss_mask_1: 0.1952  loss_dice_1: 1.467  loss_ce_2: 0.7019  loss_mask_2: 0.1892  loss_dice_2: 1.431  loss_ce_3: 0.6642  loss_mask_3: 0.1848  loss_dice_3: 1.332  loss_ce_4: 0.6396  loss_mask_4: 0.1854  loss_dice_4: 1.381  loss_ce_5: 0.5848  loss_mask_5: 0.1839  loss_dice_5: 1.335  loss_ce_6: 0.5909  loss_mask_6: 0.1796  loss_dice_6: 1.317  loss_ce_7: 0.5914  loss_mask_7: 0.178  loss_dice_7: 1.306  loss_ce_8: 0.5749  loss_mask_8: 0.1794  loss_dice_8: 1.326    time: 1.0596  last_time: 1.0158  data_time: 0.0720  last_data_time: 0.0762   lr: 0.0001  max_mem: 32601M
[10/09 18:57:21] d2.utils.events INFO:  eta: 0:14:25  iter: 179  total_loss: 22.41  loss_ce: 0.5806  loss_mask: 0.2084  loss_dice: 1.271  loss_ce_0: 0.638  loss_mask_0: 0.2281  loss_dice_0: 1.603  loss_ce_1: 0.7685  loss_mask_1: 0.2202  loss_dice_1: 1.462  loss_ce_2: 0.7061  loss_mask_2: 0.2135  loss_dice_2: 1.385  loss_ce_3: 0.6585  loss_mask_3: 0.2109  loss_dice_3: 1.323  loss_ce_4: 0.6208  loss_mask_4: 0.2111  loss_dice_4: 1.313  loss_ce_5: 0.621  loss_mask_5: 0.2053  loss_dice_5: 1.298  loss_ce_6: 0.6085  loss_mask_6: 0.2051  loss_dice_6: 1.284  loss_ce_7: 0.5979  loss_mask_7: 0.2086  loss_dice_7: 1.301  loss_ce_8: 0.5984  loss_mask_8: 0.204  loss_dice_8: 1.271    time: 1.0599  last_time: 1.1343  data_time: 0.0705  last_data_time: 0.0715   lr: 0.0001  max_mem: 32601M
[10/09 18:57:42] d2.utils.events INFO:  eta: 0:14:03  iter: 199  total_loss: 22.1  loss_ce: 0.6119  loss_mask: 0.1971  loss_dice: 1.281  loss_ce_0: 0.6228  loss_mask_0: 0.2287  loss_dice_0: 1.526  loss_ce_1: 0.7668  loss_mask_1: 0.2069  loss_dice_1: 1.452  loss_ce_2: 0.7527  loss_mask_2: 0.2049  loss_dice_2: 1.362  loss_ce_3: 0.6405  loss_mask_3: 0.2042  loss_dice_3: 1.307  loss_ce_4: 0.6453  loss_mask_4: 0.2055  loss_dice_4: 1.293  loss_ce_5: 0.6558  loss_mask_5: 0.1991  loss_dice_5: 1.286  loss_ce_6: 0.6348  loss_mask_6: 0.1956  loss_dice_6: 1.284  loss_ce_7: 0.6147  loss_mask_7: 0.1962  loss_dice_7: 1.31  loss_ce_8: 0.6147  loss_mask_8: 0.1964  loss_dice_8: 1.249    time: 1.0597  last_time: 1.0280  data_time: 0.0687  last_data_time: 0.0664   lr: 0.0001  max_mem: 32601M
[10/09 18:58:04] d2.utils.events INFO:  eta: 0:13:47  iter: 219  total_loss: 23.69  loss_ce: 0.6034  loss_mask: 0.1898  loss_dice: 1.408  loss_ce_0: 0.6482  loss_mask_0: 0.2171  loss_dice_0: 1.674  loss_ce_1: 0.7832  loss_mask_1: 0.2071  loss_dice_1: 1.56  loss_ce_2: 0.7087  loss_mask_2: 0.1965  loss_dice_2: 1.525  loss_ce_3: 0.6617  loss_mask_3: 0.1936  loss_dice_3: 1.447  loss_ce_4: 0.6478  loss_mask_4: 0.1922  loss_dice_4: 1.421  loss_ce_5: 0.6325  loss_mask_5: 0.1915  loss_dice_5: 1.435  loss_ce_6: 0.6388  loss_mask_6: 0.191  loss_dice_6: 1.427  loss_ce_7: 0.5986  loss_mask_7: 0.1915  loss_dice_7: 1.427  loss_ce_8: 0.6142  loss_mask_8: 0.1907  loss_dice_8: 1.421    time: 1.0639  last_time: 1.1513  data_time: 0.0801  last_data_time: 0.0922   lr: 0.0001  max_mem: 32601M
[10/09 18:58:26] d2.utils.events INFO:  eta: 0:13:27  iter: 239  total_loss: 20.55  loss_ce: 0.4991  loss_mask: 0.1972  loss_dice: 1.261  loss_ce_0: 0.648  loss_mask_0: 0.225  loss_dice_0: 1.524  loss_ce_1: 0.6889  loss_mask_1: 0.2019  loss_dice_1: 1.355  loss_ce_2: 0.6604  loss_mask_2: 0.1966  loss_dice_2: 1.338  loss_ce_3: 0.5783  loss_mask_3: 0.206  loss_dice_3: 1.263  loss_ce_4: 0.565  loss_mask_4: 0.2045  loss_dice_4: 1.253  loss_ce_5: 0.5354  loss_mask_5: 0.2051  loss_dice_5: 1.294  loss_ce_6: 0.5429  loss_mask_6: 0.2011  loss_dice_6: 1.229  loss_ce_7: 0.5221  loss_mask_7: 0.2015  loss_dice_7: 1.24  loss_ce_8: 0.5182  loss_mask_8: 0.2  loss_dice_8: 1.233    time: 1.0660  last_time: 1.1436  data_time: 0.0671  last_data_time: 0.0831   lr: 0.0001  max_mem: 32601M
[10/09 18:58:48] d2.utils.events INFO:  eta: 0:13:05  iter: 259  total_loss: 21.05  loss_ce: 0.5692  loss_mask: 0.1898  loss_dice: 1.253  loss_ce_0: 0.6049  loss_mask_0: 0.2164  loss_dice_0: 1.494  loss_ce_1: 0.7463  loss_mask_1: 0.2011  loss_dice_1: 1.409  loss_ce_2: 0.6588  loss_mask_2: 0.194  loss_dice_2: 1.36  loss_ce_3: 0.5896  loss_mask_3: 0.1959  loss_dice_3: 1.332  loss_ce_4: 0.5791  loss_mask_4: 0.1938  loss_dice_4: 1.289  loss_ce_5: 0.5632  loss_mask_5: 0.1911  loss_dice_5: 1.255  loss_ce_6: 0.552  loss_mask_6: 0.1902  loss_dice_6: 1.293  loss_ce_7: 0.5393  loss_mask_7: 0.1897  loss_dice_7: 1.289  loss_ce_8: 0.5482  loss_mask_8: 0.1912  loss_dice_8: 1.267    time: 1.0665  last_time: 1.0240  data_time: 0.0699  last_data_time: 0.0568   lr: 0.0001  max_mem: 32601M
[10/09 18:59:09] d2.utils.events INFO:  eta: 0:12:45  iter: 279  total_loss: 21.81  loss_ce: 0.5459  loss_mask: 0.1959  loss_dice: 1.362  loss_ce_0: 0.6063  loss_mask_0: 0.2387  loss_dice_0: 1.532  loss_ce_1: 0.6963  loss_mask_1: 0.2133  loss_dice_1: 1.459  loss_ce_2: 0.6724  loss_mask_2: 0.2025  loss_dice_2: 1.42  loss_ce_3: 0.5885  loss_mask_3: 0.1995  loss_dice_3: 1.353  loss_ce_4: 0.5778  loss_mask_4: 0.1988  loss_dice_4: 1.308  loss_ce_5: 0.5325  loss_mask_5: 0.2021  loss_dice_5: 1.379  loss_ce_6: 0.5484  loss_mask_6: 0.1962  loss_dice_6: 1.356  loss_ce_7: 0.5836  loss_mask_7: 0.1969  loss_dice_7: 1.338  loss_ce_8: 0.532  loss_mask_8: 0.1972  loss_dice_8: 1.342    time: 1.0666  last_time: 1.0630  data_time: 0.0689  last_data_time: 0.0722   lr: 0.0001  max_mem: 32601M
[10/09 18:59:30] d2.utils.events INFO:  eta: 0:12:23  iter: 299  total_loss: 20.55  loss_ce: 0.4861  loss_mask: 0.1964  loss_dice: 1.237  loss_ce_0: 0.6025  loss_mask_0: 0.2295  loss_dice_0: 1.471  loss_ce_1: 0.6593  loss_mask_1: 0.2068  loss_dice_1: 1.342  loss_ce_2: 0.6108  loss_mask_2: 0.2013  loss_dice_2: 1.276  loss_ce_3: 0.5639  loss_mask_3: 0.1992  loss_dice_3: 1.266  loss_ce_4: 0.5325  loss_mask_4: 0.1955  loss_dice_4: 1.227  loss_ce_5: 0.5386  loss_mask_5: 0.1963  loss_dice_5: 1.258  loss_ce_6: 0.5345  loss_mask_6: 0.1976  loss_dice_6: 1.234  loss_ce_7: 0.5217  loss_mask_7: 0.1971  loss_dice_7: 1.222  loss_ce_8: 0.4969  loss_mask_8: 0.198  loss_dice_8: 1.214    time: 1.0662  last_time: 1.1199  data_time: 0.0673  last_data_time: 0.0739   lr: 0.0001  max_mem: 32601M
[10/09 18:59:52] d2.utils.events INFO:  eta: 0:12:02  iter: 319  total_loss: 21.27  loss_ce: 0.5213  loss_mask: 0.1923  loss_dice: 1.24  loss_ce_0: 0.6526  loss_mask_0: 0.2169  loss_dice_0: 1.509  loss_ce_1: 0.7305  loss_mask_1: 0.2036  loss_dice_1: 1.419  loss_ce_2: 0.639  loss_mask_2: 0.1992  loss_dice_2: 1.361  loss_ce_3: 0.6033  loss_mask_3: 0.1976  loss_dice_3: 1.307  loss_ce_4: 0.5778  loss_mask_4: 0.1958  loss_dice_4: 1.266  loss_ce_5: 0.5671  loss_mask_5: 0.1966  loss_dice_5: 1.292  loss_ce_6: 0.5564  loss_mask_6: 0.1928  loss_dice_6: 1.273  loss_ce_7: 0.561  loss_mask_7: 0.194  loss_dice_7: 1.257  loss_ce_8: 0.5401  loss_mask_8: 0.1918  loss_dice_8: 1.285    time: 1.0663  last_time: 1.0982  data_time: 0.0689  last_data_time: 0.0805   lr: 0.0001  max_mem: 32601M
[10/09 19:00:13] d2.utils.events INFO:  eta: 0:11:41  iter: 339  total_loss: 22.01  loss_ce: 0.5684  loss_mask: 0.1833  loss_dice: 1.323  loss_ce_0: 0.6306  loss_mask_0: 0.2121  loss_dice_0: 1.592  loss_ce_1: 0.7271  loss_mask_1: 0.1938  loss_dice_1: 1.456  loss_ce_2: 0.6614  loss_mask_2: 0.1815  loss_dice_2: 1.432  loss_ce_3: 0.6248  loss_mask_3: 0.1839  loss_dice_3: 1.374  loss_ce_4: 0.5862  loss_mask_4: 0.1843  loss_dice_4: 1.373  loss_ce_5: 0.571  loss_mask_5: 0.1841  loss_dice_5: 1.342  loss_ce_6: 0.5512  loss_mask_6: 0.1832  loss_dice_6: 1.357  loss_ce_7: 0.5457  loss_mask_7: 0.1845  loss_dice_7: 1.316  loss_ce_8: 0.5652  loss_mask_8: 0.1836  loss_dice_8: 1.336    time: 1.0674  last_time: 1.0725  data_time: 0.0726  last_data_time: 0.0794   lr: 0.0001  max_mem: 32601M
[10/09 19:00:35] d2.utils.events INFO:  eta: 0:11:20  iter: 359  total_loss: 20.79  loss_ce: 0.5382  loss_mask: 0.1845  loss_dice: 1.261  loss_ce_0: 0.5758  loss_mask_0: 0.2102  loss_dice_0: 1.519  loss_ce_1: 0.6886  loss_mask_1: 0.1952  loss_dice_1: 1.415  loss_ce_2: 0.6245  loss_mask_2: 0.1916  loss_dice_2: 1.323  loss_ce_3: 0.5745  loss_mask_3: 0.1914  loss_dice_3: 1.267  loss_ce_4: 0.5442  loss_mask_4: 0.188  loss_dice_4: 1.286  loss_ce_5: 0.5397  loss_mask_5: 0.1866  loss_dice_5: 1.252  loss_ce_6: 0.5287  loss_mask_6: 0.1882  loss_dice_6: 1.284  loss_ce_7: 0.5205  loss_mask_7: 0.1875  loss_dice_7: 1.241  loss_ce_8: 0.5311  loss_mask_8: 0.1868  loss_dice_8: 1.267    time: 1.0679  last_time: 1.0699  data_time: 0.0717  last_data_time: 0.0587   lr: 0.0001  max_mem: 32601M
[10/09 19:00:57] d2.utils.events INFO:  eta: 0:10:59  iter: 379  total_loss: 22.17  loss_ce: 0.5615  loss_mask: 0.182  loss_dice: 1.298  loss_ce_0: 0.6308  loss_mask_0: 0.2146  loss_dice_0: 1.522  loss_ce_1: 0.7181  loss_mask_1: 0.1914  loss_dice_1: 1.471  loss_ce_2: 0.6747  loss_mask_2: 0.1878  loss_dice_2: 1.382  loss_ce_3: 0.5496  loss_mask_3: 0.1831  loss_dice_3: 1.359  loss_ce_4: 0.5592  loss_mask_4: 0.1807  loss_dice_4: 1.372  loss_ce_5: 0.583  loss_mask_5: 0.1817  loss_dice_5: 1.325  loss_ce_6: 0.5841  loss_mask_6: 0.1836  loss_dice_6: 1.309  loss_ce_7: 0.5509  loss_mask_7: 0.1834  loss_dice_7: 1.3  loss_ce_8: 0.5922  loss_mask_8: 0.1809  loss_dice_8: 1.345    time: 1.0684  last_time: 1.1528  data_time: 0.0732  last_data_time: 0.0745   lr: 0.0001  max_mem: 32601M
[10/09 19:01:18] d2.utils.events INFO:  eta: 0:10:39  iter: 399  total_loss: 20.85  loss_ce: 0.5608  loss_mask: 0.1778  loss_dice: 1.251  loss_ce_0: 0.5951  loss_mask_0: 0.1979  loss_dice_0: 1.551  loss_ce_1: 0.747  loss_mask_1: 0.1873  loss_dice_1: 1.382  loss_ce_2: 0.6486  loss_mask_2: 0.1814  loss_dice_2: 1.309  loss_ce_3: 0.5806  loss_mask_3: 0.1786  loss_dice_3: 1.304  loss_ce_4: 0.5678  loss_mask_4: 0.1768  loss_dice_4: 1.267  loss_ce_5: 0.5495  loss_mask_5: 0.1809  loss_dice_5: 1.276  loss_ce_6: 0.5286  loss_mask_6: 0.1769  loss_dice_6: 1.251  loss_ce_7: 0.5352  loss_mask_7: 0.1783  loss_dice_7: 1.293  loss_ce_8: 0.5198  loss_mask_8: 0.1754  loss_dice_8: 1.258    time: 1.0693  last_time: 1.0473  data_time: 0.0745  last_data_time: 0.0817   lr: 0.0001  max_mem: 32601M
[10/09 19:01:40] d2.utils.events INFO:  eta: 0:10:18  iter: 419  total_loss: 21.21  loss_ce: 0.5238  loss_mask: 0.1875  loss_dice: 1.331  loss_ce_0: 0.5963  loss_mask_0: 0.2035  loss_dice_0: 1.582  loss_ce_1: 0.7508  loss_mask_1: 0.191  loss_dice_1: 1.447  loss_ce_2: 0.6257  loss_mask_2: 0.1852  loss_dice_2: 1.373  loss_ce_3: 0.5697  loss_mask_3: 0.1861  loss_dice_3: 1.384  loss_ce_4: 0.5462  loss_mask_4: 0.1888  loss_dice_4: 1.321  loss_ce_5: 0.551  loss_mask_5: 0.1876  loss_dice_5: 1.349  loss_ce_6: 0.5321  loss_mask_6: 0.1885  loss_dice_6: 1.321  loss_ce_7: 0.5194  loss_mask_7: 0.1901  loss_dice_7: 1.342  loss_ce_8: 0.5389  loss_mask_8: 0.1871  loss_dice_8: 1.338    time: 1.0698  last_time: 1.0857  data_time: 0.0710  last_data_time: 0.0822   lr: 0.0001  max_mem: 32601M
[10/09 19:02:02] d2.utils.events INFO:  eta: 0:09:57  iter: 439  total_loss: 20.88  loss_ce: 0.4978  loss_mask: 0.191  loss_dice: 1.315  loss_ce_0: 0.5816  loss_mask_0: 0.2312  loss_dice_0: 1.56  loss_ce_1: 0.6959  loss_mask_1: 0.2096  loss_dice_1: 1.457  loss_ce_2: 0.621  loss_mask_2: 0.205  loss_dice_2: 1.376  loss_ce_3: 0.6116  loss_mask_3: 0.1935  loss_dice_3: 1.337  loss_ce_4: 0.5306  loss_mask_4: 0.1915  loss_dice_4: 1.341  loss_ce_5: 0.5226  loss_mask_5: 0.1923  loss_dice_5: 1.284  loss_ce_6: 0.5424  loss_mask_6: 0.1889  loss_dice_6: 1.263  loss_ce_7: 0.5228  loss_mask_7: 0.1888  loss_dice_7: 1.288  loss_ce_8: 0.5201  loss_mask_8: 0.1881  loss_dice_8: 1.264    time: 1.0702  last_time: 1.0735  data_time: 0.0682  last_data_time: 0.0718   lr: 0.0001  max_mem: 32601M
[10/09 19:02:23] d2.utils.events INFO:  eta: 0:09:35  iter: 459  total_loss: 21.73  loss_ce: 0.5522  loss_mask: 0.1804  loss_dice: 1.255  loss_ce_0: 0.6318  loss_mask_0: 0.2176  loss_dice_0: 1.511  loss_ce_1: 0.7495  loss_mask_1: 0.192  loss_dice_1: 1.481  loss_ce_2: 0.6396  loss_mask_2: 0.1833  loss_dice_2: 1.389  loss_ce_3: 0.5985  loss_mask_3: 0.1802  loss_dice_3: 1.35  loss_ce_4: 0.5544  loss_mask_4: 0.1789  loss_dice_4: 1.344  loss_ce_5: 0.5888  loss_mask_5: 0.1805  loss_dice_5: 1.306  loss_ce_6: 0.5924  loss_mask_6: 0.1783  loss_dice_6: 1.284  loss_ce_7: 0.5836  loss_mask_7: 0.1787  loss_dice_7: 1.284  loss_ce_8: 0.5616  loss_mask_8: 0.1799  loss_dice_8: 1.308    time: 1.0697  last_time: 1.0510  data_time: 0.0702  last_data_time: 0.0724   lr: 0.0001  max_mem: 32601M
[10/09 19:02:44] d2.utils.events INFO:  eta: 0:09:14  iter: 479  total_loss: 21.55  loss_ce: 0.5332  loss_mask: 0.1763  loss_dice: 1.286  loss_ce_0: 0.6615  loss_mask_0: 0.2037  loss_dice_0: 1.511  loss_ce_1: 0.6812  loss_mask_1: 0.1941  loss_dice_1: 1.406  loss_ce_2: 0.6553  loss_mask_2: 0.1845  loss_dice_2: 1.358  loss_ce_3: 0.6215  loss_mask_3: 0.1849  loss_dice_3: 1.288  loss_ce_4: 0.5766  loss_mask_4: 0.1811  loss_dice_4: 1.281  loss_ce_5: 0.5731  loss_mask_5: 0.1791  loss_dice_5: 1.296  loss_ce_6: 0.5441  loss_mask_6: 0.1773  loss_dice_6: 1.238  loss_ce_7: 0.5464  loss_mask_7: 0.1789  loss_dice_7: 1.274  loss_ce_8: 0.556  loss_mask_8: 0.1758  loss_dice_8: 1.264    time: 1.0694  last_time: 1.0273  data_time: 0.0704  last_data_time: 0.0809   lr: 0.0001  max_mem: 32601M
[10/09 19:03:06] d2.utils.events INFO:  eta: 0:08:52  iter: 499  total_loss: 20.35  loss_ce: 0.5482  loss_mask: 0.1852  loss_dice: 1.218  loss_ce_0: 0.5922  loss_mask_0: 0.216  loss_dice_0: 1.424  loss_ce_1: 0.6881  loss_mask_1: 0.1947  loss_dice_1: 1.377  loss_ce_2: 0.5993  loss_mask_2: 0.191  loss_dice_2: 1.344  loss_ce_3: 0.5731  loss_mask_3: 0.1935  loss_dice_3: 1.289  loss_ce_4: 0.5489  loss_mask_4: 0.1888  loss_dice_4: 1.246  loss_ce_5: 0.5492  loss_mask_5: 0.1859  loss_dice_5: 1.268  loss_ce_6: 0.5501  loss_mask_6: 0.1858  loss_dice_6: 1.255  loss_ce_7: 0.5098  loss_mask_7: 0.1863  loss_dice_7: 1.233  loss_ce_8: 0.5268  loss_mask_8: 0.1856  loss_dice_8: 1.257    time: 1.0695  last_time: 1.0224  data_time: 0.0732  last_data_time: 0.0671   lr: 0.0001  max_mem: 32601M
[10/09 19:03:27] d2.utils.events INFO:  eta: 0:08:31  iter: 519  total_loss: 20.33  loss_ce: 0.5706  loss_mask: 0.1698  loss_dice: 1.208  loss_ce_0: 0.6586  loss_mask_0: 0.1998  loss_dice_0: 1.451  loss_ce_1: 0.7186  loss_mask_1: 0.1899  loss_dice_1: 1.352  loss_ce_2: 0.6336  loss_mask_2: 0.1793  loss_dice_2: 1.312  loss_ce_3: 0.5832  loss_mask_3: 0.1751  loss_dice_3: 1.258  loss_ce_4: 0.5617  loss_mask_4: 0.17  loss_dice_4: 1.244  loss_ce_5: 0.5816  loss_mask_5: 0.1679  loss_dice_5: 1.235  loss_ce_6: 0.5826  loss_mask_6: 0.1685  loss_dice_6: 1.224  loss_ce_7: 0.5585  loss_mask_7: 0.1685  loss_dice_7: 1.233  loss_ce_8: 0.5394  loss_mask_8: 0.1702  loss_dice_8: 1.242    time: 1.0695  last_time: 1.1164  data_time: 0.0703  last_data_time: 0.0797   lr: 0.0001  max_mem: 32601M
[10/09 19:03:49] d2.utils.events INFO:  eta: 0:08:10  iter: 539  total_loss: 22.18  loss_ce: 0.5607  loss_mask: 0.1891  loss_dice: 1.335  loss_ce_0: 0.5995  loss_mask_0: 0.2069  loss_dice_0: 1.616  loss_ce_1: 0.7322  loss_mask_1: 0.1958  loss_dice_1: 1.488  loss_ce_2: 0.6805  loss_mask_2: 0.191  loss_dice_2: 1.426  loss_ce_3: 0.6158  loss_mask_3: 0.1906  loss_dice_3: 1.392  loss_ce_4: 0.5648  loss_mask_4: 0.1921  loss_dice_4: 1.382  loss_ce_5: 0.5649  loss_mask_5: 0.1924  loss_dice_5: 1.405  loss_ce_6: 0.5645  loss_mask_6: 0.1863  loss_dice_6: 1.367  loss_ce_7: 0.5516  loss_mask_7: 0.1897  loss_dice_7: 1.344  loss_ce_8: 0.5621  loss_mask_8: 0.1868  loss_dice_8: 1.368    time: 1.0698  last_time: 1.0649  data_time: 0.0726  last_data_time: 0.0709   lr: 0.0001  max_mem: 32601M
[10/09 19:04:10] d2.utils.events INFO:  eta: 0:07:49  iter: 559  total_loss: 20.88  loss_ce: 0.5173  loss_mask: 0.1792  loss_dice: 1.306  loss_ce_0: 0.5668  loss_mask_0: 0.217  loss_dice_0: 1.575  loss_ce_1: 0.6871  loss_mask_1: 0.1993  loss_dice_1: 1.438  loss_ce_2: 0.6255  loss_mask_2: 0.1834  loss_dice_2: 1.359  loss_ce_3: 0.5552  loss_mask_3: 0.1814  loss_dice_3: 1.304  loss_ce_4: 0.5374  loss_mask_4: 0.1782  loss_dice_4: 1.316  loss_ce_5: 0.5171  loss_mask_5: 0.1773  loss_dice_5: 1.302  loss_ce_6: 0.5427  loss_mask_6: 0.1797  loss_dice_6: 1.284  loss_ce_7: 0.4959  loss_mask_7: 0.1777  loss_dice_7: 1.288  loss_ce_8: 0.5003  loss_mask_8: 0.1775  loss_dice_8: 1.298    time: 1.0701  last_time: 1.0899  data_time: 0.0727  last_data_time: 0.0708   lr: 0.0001  max_mem: 32601M
[10/09 19:04:32] d2.utils.events INFO:  eta: 0:07:27  iter: 579  total_loss: 20.94  loss_ce: 0.5146  loss_mask: 0.1826  loss_dice: 1.256  loss_ce_0: 0.5613  loss_mask_0: 0.2114  loss_dice_0: 1.482  loss_ce_1: 0.7207  loss_mask_1: 0.1978  loss_dice_1: 1.429  loss_ce_2: 0.6133  loss_mask_2: 0.1889  loss_dice_2: 1.33  loss_ce_3: 0.581  loss_mask_3: 0.1889  loss_dice_3: 1.321  loss_ce_4: 0.5101  loss_mask_4: 0.1875  loss_dice_4: 1.275  loss_ce_5: 0.548  loss_mask_5: 0.182  loss_dice_5: 1.307  loss_ce_6: 0.5109  loss_mask_6: 0.1822  loss_dice_6: 1.28  loss_ce_7: 0.5248  loss_mask_7: 0.1807  loss_dice_7: 1.266  loss_ce_8: 0.5216  loss_mask_8: 0.1806  loss_dice_8: 1.254    time: 1.0701  last_time: 1.0667  data_time: 0.0682  last_data_time: 0.0485   lr: 0.0001  max_mem: 32601M
[10/09 19:04:53] d2.utils.events INFO:  eta: 0:07:06  iter: 599  total_loss: 20.38  loss_ce: 0.5312  loss_mask: 0.1832  loss_dice: 1.226  loss_ce_0: 0.5948  loss_mask_0: 0.2208  loss_dice_0: 1.501  loss_ce_1: 0.654  loss_mask_1: 0.2033  loss_dice_1: 1.386  loss_ce_2: 0.584  loss_mask_2: 0.1986  loss_dice_2: 1.303  loss_ce_3: 0.5574  loss_mask_3: 0.1955  loss_dice_3: 1.263  loss_ce_4: 0.5348  loss_mask_4: 0.1944  loss_dice_4: 1.282  loss_ce_5: 0.5183  loss_mask_5: 0.1858  loss_dice_5: 1.234  loss_ce_6: 0.5264  loss_mask_6: 0.1875  loss_dice_6: 1.228  loss_ce_7: 0.5121  loss_mask_7: 0.1868  loss_dice_7: 1.3  loss_ce_8: 0.5265  loss_mask_8: 0.1857  loss_dice_8: 1.24    time: 1.0703  last_time: 1.0451  data_time: 0.0735  last_data_time: 0.0737   lr: 0.0001  max_mem: 32716M
[10/09 19:05:15] d2.utils.events INFO:  eta: 0:06:45  iter: 619  total_loss: 22.06  loss_ce: 0.5643  loss_mask: 0.1753  loss_dice: 1.362  loss_ce_0: 0.6169  loss_mask_0: 0.213  loss_dice_0: 1.6  loss_ce_1: 0.6806  loss_mask_1: 0.2023  loss_dice_1: 1.479  loss_ce_2: 0.6591  loss_mask_2: 0.186  loss_dice_2: 1.434  loss_ce_3: 0.6003  loss_mask_3: 0.1803  loss_dice_3: 1.393  loss_ce_4: 0.5761  loss_mask_4: 0.1811  loss_dice_4: 1.361  loss_ce_5: 0.56  loss_mask_5: 0.177  loss_dice_5: 1.415  loss_ce_6: 0.5825  loss_mask_6: 0.1769  loss_dice_6: 1.336  loss_ce_7: 0.546  loss_mask_7: 0.1777  loss_dice_7: 1.338  loss_ce_8: 0.5572  loss_mask_8: 0.176  loss_dice_8: 1.333    time: 1.0710  last_time: 1.0416  data_time: 0.0738  last_data_time: 0.0497   lr: 0.0001  max_mem: 32716M
[10/09 19:05:37] d2.utils.events INFO:  eta: 0:06:24  iter: 639  total_loss: 20.46  loss_ce: 0.5357  loss_mask: 0.1683  loss_dice: 1.21  loss_ce_0: 0.6483  loss_mask_0: 0.2025  loss_dice_0: 1.448  loss_ce_1: 0.6839  loss_mask_1: 0.1828  loss_dice_1: 1.353  loss_ce_2: 0.6503  loss_mask_2: 0.1786  loss_dice_2: 1.275  loss_ce_3: 0.5925  loss_mask_3: 0.1744  loss_dice_3: 1.247  loss_ce_4: 0.5582  loss_mask_4: 0.1699  loss_dice_4: 1.243  loss_ce_5: 0.5618  loss_mask_5: 0.1687  loss_dice_5: 1.21  loss_ce_6: 0.5594  loss_mask_6: 0.1719  loss_dice_6: 1.237  loss_ce_7: 0.5365  loss_mask_7: 0.1689  loss_dice_7: 1.212  loss_ce_8: 0.5338  loss_mask_8: 0.168  loss_dice_8: 1.225    time: 1.0709  last_time: 1.0483  data_time: 0.0713  last_data_time: 0.0794   lr: 0.0001  max_mem: 32716M
[10/09 19:05:58] d2.utils.events INFO:  eta: 0:06:02  iter: 659  total_loss: 20.77  loss_ce: 0.5411  loss_mask: 0.187  loss_dice: 1.263  loss_ce_0: 0.595  loss_mask_0: 0.2035  loss_dice_0: 1.503  loss_ce_1: 0.6822  loss_mask_1: 0.1986  loss_dice_1: 1.405  loss_ce_2: 0.6171  loss_mask_2: 0.1904  loss_dice_2: 1.373  loss_ce_3: 0.5703  loss_mask_3: 0.1884  loss_dice_3: 1.307  loss_ce_4: 0.524  loss_mask_4: 0.1873  loss_dice_4: 1.297  loss_ce_5: 0.5475  loss_mask_5: 0.1869  loss_dice_5: 1.254  loss_ce_6: 0.5356  loss_mask_6: 0.1871  loss_dice_6: 1.258  loss_ce_7: 0.5228  loss_mask_7: 0.1851  loss_dice_7: 1.244  loss_ce_8: 0.5136  loss_mask_8: 0.186  loss_dice_8: 1.281    time: 1.0708  last_time: 1.0625  data_time: 0.0697  last_data_time: 0.0702   lr: 0.0001  max_mem: 32716M
[10/09 19:06:20] d2.utils.events INFO:  eta: 0:05:41  iter: 679  total_loss: 21.06  loss_ce: 0.5036  loss_mask: 0.176  loss_dice: 1.299  loss_ce_0: 0.6159  loss_mask_0: 0.1999  loss_dice_0: 1.559  loss_ce_1: 0.6769  loss_mask_1: 0.1921  loss_dice_1: 1.454  loss_ce_2: 0.6465  loss_mask_2: 0.1811  loss_dice_2: 1.345  loss_ce_3: 0.5992  loss_mask_3: 0.1809  loss_dice_3: 1.312  loss_ce_4: 0.5233  loss_mask_4: 0.1803  loss_dice_4: 1.319  loss_ce_5: 0.5441  loss_mask_5: 0.1755  loss_dice_5: 1.31  loss_ce_6: 0.5248  loss_mask_6: 0.1766  loss_dice_6: 1.289  loss_ce_7: 0.5263  loss_mask_7: 0.1766  loss_dice_7: 1.303  loss_ce_8: 0.5405  loss_mask_8: 0.1725  loss_dice_8: 1.284    time: 1.0710  last_time: 1.0657  data_time: 0.0720  last_data_time: 0.0767   lr: 0.0001  max_mem: 32716M
[10/09 19:06:41] d2.utils.events INFO:  eta: 0:05:19  iter: 699  total_loss: 20.68  loss_ce: 0.5609  loss_mask: 0.1864  loss_dice: 1.247  loss_ce_0: 0.5951  loss_mask_0: 0.2068  loss_dice_0: 1.495  loss_ce_1: 0.7227  loss_mask_1: 0.1968  loss_dice_1: 1.394  loss_ce_2: 0.6575  loss_mask_2: 0.187  loss_dice_2: 1.371  loss_ce_3: 0.6069  loss_mask_3: 0.1871  loss_dice_3: 1.3  loss_ce_4: 0.6328  loss_mask_4: 0.1856  loss_dice_4: 1.289  loss_ce_5: 0.5796  loss_mask_5: 0.1831  loss_dice_5: 1.3  loss_ce_6: 0.5754  loss_mask_6: 0.1882  loss_dice_6: 1.277  loss_ce_7: 0.5348  loss_mask_7: 0.1858  loss_dice_7: 1.273  loss_ce_8: 0.557  loss_mask_8: 0.1838  loss_dice_8: 1.25    time: 1.0712  last_time: 1.0893  data_time: 0.0710  last_data_time: 0.0832   lr: 0.0001  max_mem: 32716M
[10/09 19:07:03] d2.utils.events INFO:  eta: 0:04:58  iter: 719  total_loss: 20.25  loss_ce: 0.4791  loss_mask: 0.1736  loss_dice: 1.259  loss_ce_0: 0.5572  loss_mask_0: 0.1935  loss_dice_0: 1.549  loss_ce_1: 0.6795  loss_mask_1: 0.1812  loss_dice_1: 1.421  loss_ce_2: 0.621  loss_mask_2: 0.1772  loss_dice_2: 1.329  loss_ce_3: 0.5417  loss_mask_3: 0.1789  loss_dice_3: 1.283  loss_ce_4: 0.5108  loss_mask_4: 0.1757  loss_dice_4: 1.309  loss_ce_5: 0.5194  loss_mask_5: 0.1737  loss_dice_5: 1.273  loss_ce_6: 0.4945  loss_mask_6: 0.1731  loss_dice_6: 1.276  loss_ce_7: 0.4891  loss_mask_7: 0.1749  loss_dice_7: 1.28  loss_ce_8: 0.4989  loss_mask_8: 0.1716  loss_dice_8: 1.252    time: 1.0716  last_time: 1.0348  data_time: 0.0746  last_data_time: 0.0718   lr: 0.0001  max_mem: 32716M
[10/09 19:07:25] d2.utils.events INFO:  eta: 0:04:37  iter: 739  total_loss: 19.7  loss_ce: 0.4876  loss_mask: 0.1774  loss_dice: 1.204  loss_ce_0: 0.5762  loss_mask_0: 0.2045  loss_dice_0: 1.469  loss_ce_1: 0.6782  loss_mask_1: 0.1891  loss_dice_1: 1.353  loss_ce_2: 0.6251  loss_mask_2: 0.1814  loss_dice_2: 1.281  loss_ce_3: 0.5637  loss_mask_3: 0.1762  loss_dice_3: 1.265  loss_ce_4: 0.5397  loss_mask_4: 0.1771  loss_dice_4: 1.211  loss_ce_5: 0.4923  loss_mask_5: 0.1737  loss_dice_5: 1.212  loss_ce_6: 0.5047  loss_mask_6: 0.1746  loss_dice_6: 1.188  loss_ce_7: 0.5048  loss_mask_7: 0.1739  loss_dice_7: 1.189  loss_ce_8: 0.4975  loss_mask_8: 0.1761  loss_dice_8: 1.209    time: 1.0722  last_time: 1.1568  data_time: 0.0714  last_data_time: 0.0695   lr: 0.0001  max_mem: 32716M
[10/09 19:07:46] d2.utils.events INFO:  eta: 0:04:16  iter: 759  total_loss: 20.66  loss_ce: 0.4726  loss_mask: 0.1872  loss_dice: 1.242  loss_ce_0: 0.5754  loss_mask_0: 0.2153  loss_dice_0: 1.476  loss_ce_1: 0.6816  loss_mask_1: 0.1984  loss_dice_1: 1.382  loss_ce_2: 0.6055  loss_mask_2: 0.1946  loss_dice_2: 1.386  loss_ce_3: 0.5449  loss_mask_3: 0.1894  loss_dice_3: 1.28  loss_ce_4: 0.527  loss_mask_4: 0.189  loss_dice_4: 1.323  loss_ce_5: 0.5142  loss_mask_5: 0.1881  loss_dice_5: 1.278  loss_ce_6: 0.4504  loss_mask_6: 0.1898  loss_dice_6: 1.283  loss_ce_7: 0.459  loss_mask_7: 0.1859  loss_dice_7: 1.254  loss_ce_8: 0.4518  loss_mask_8: 0.1873  loss_dice_8: 1.264    time: 1.0719  last_time: 1.0424  data_time: 0.0723  last_data_time: 0.0668   lr: 0.0001  max_mem: 32716M
[10/09 19:08:08] d2.utils.events INFO:  eta: 0:03:54  iter: 779  total_loss: 20.55  loss_ce: 0.4698  loss_mask: 0.1568  loss_dice: 1.231  loss_ce_0: 0.5654  loss_mask_0: 0.1905  loss_dice_0: 1.526  loss_ce_1: 0.6629  loss_mask_1: 0.1788  loss_dice_1: 1.452  loss_ce_2: 0.6027  loss_mask_2: 0.1715  loss_dice_2: 1.34  loss_ce_3: 0.5586  loss_mask_3: 0.1728  loss_dice_3: 1.301  loss_ce_4: 0.5352  loss_mask_4: 0.1619  loss_dice_4: 1.26  loss_ce_5: 0.5351  loss_mask_5: 0.1601  loss_dice_5: 1.304  loss_ce_6: 0.4913  loss_mask_6: 0.1588  loss_dice_6: 1.266  loss_ce_7: 0.4959  loss_mask_7: 0.1592  loss_dice_7: 1.243  loss_ce_8: 0.4889  loss_mask_8: 0.1582  loss_dice_8: 1.263    time: 1.0722  last_time: 1.0842  data_time: 0.0724  last_data_time: 0.0709   lr: 0.0001  max_mem: 32716M
[10/09 19:08:29] d2.utils.events INFO:  eta: 0:03:33  iter: 799  total_loss: 19.96  loss_ce: 0.4918  loss_mask: 0.1682  loss_dice: 1.195  loss_ce_0: 0.5783  loss_mask_0: 0.1851  loss_dice_0: 1.391  loss_ce_1: 0.6702  loss_mask_1: 0.1805  loss_dice_1: 1.335  loss_ce_2: 0.6068  loss_mask_2: 0.1739  loss_dice_2: 1.279  loss_ce_3: 0.5497  loss_mask_3: 0.1754  loss_dice_3: 1.223  loss_ce_4: 0.5263  loss_mask_4: 0.1729  loss_dice_4: 1.207  loss_ce_5: 0.5077  loss_mask_5: 0.1727  loss_dice_5: 1.219  loss_ce_6: 0.5016  loss_mask_6: 0.1683  loss_dice_6: 1.207  loss_ce_7: 0.4991  loss_mask_7: 0.1696  loss_dice_7: 1.199  loss_ce_8: 0.4848  loss_mask_8: 0.169  loss_dice_8: 1.178    time: 1.0717  last_time: 1.0548  data_time: 0.0722  last_data_time: 0.0714   lr: 0.0001  max_mem: 32716M
[10/09 19:08:51] d2.utils.events INFO:  eta: 0:03:12  iter: 819  total_loss: 19.91  loss_ce: 0.4889  loss_mask: 0.1657  loss_dice: 1.204  loss_ce_0: 0.624  loss_mask_0: 0.1841  loss_dice_0: 1.423  loss_ce_1: 0.6573  loss_mask_1: 0.1746  loss_dice_1: 1.352  loss_ce_2: 0.597  loss_mask_2: 0.1709  loss_dice_2: 1.278  loss_ce_3: 0.575  loss_mask_3: 0.1697  loss_dice_3: 1.23  loss_ce_4: 0.5354  loss_mask_4: 0.1661  loss_dice_4: 1.212  loss_ce_5: 0.543  loss_mask_5: 0.1691  loss_dice_5: 1.224  loss_ce_6: 0.5359  loss_mask_6: 0.1658  loss_dice_6: 1.223  loss_ce_7: 0.5032  loss_mask_7: 0.1644  loss_dice_7: 1.214  loss_ce_8: 0.5304  loss_mask_8: 0.1655  loss_dice_8: 1.203    time: 1.0723  last_time: 1.1472  data_time: 0.0726  last_data_time: 0.0799   lr: 0.0001  max_mem: 32716M
[10/09 19:09:13] d2.utils.events INFO:  eta: 0:02:50  iter: 839  total_loss: 19.67  loss_ce: 0.4811  loss_mask: 0.1761  loss_dice: 1.204  loss_ce_0: 0.5816  loss_mask_0: 0.2022  loss_dice_0: 1.425  loss_ce_1: 0.6166  loss_mask_1: 0.1866  loss_dice_1: 1.31  loss_ce_2: 0.5702  loss_mask_2: 0.1783  loss_dice_2: 1.294  loss_ce_3: 0.5573  loss_mask_3: 0.1795  loss_dice_3: 1.225  loss_ce_4: 0.5012  loss_mask_4: 0.1761  loss_dice_4: 1.251  loss_ce_5: 0.5258  loss_mask_5: 0.1746  loss_dice_5: 1.212  loss_ce_6: 0.5112  loss_mask_6: 0.1753  loss_dice_6: 1.213  loss_ce_7: 0.4527  loss_mask_7: 0.1755  loss_dice_7: 1.234  loss_ce_8: 0.5213  loss_mask_8: 0.1751  loss_dice_8: 1.196    time: 1.0722  last_time: 1.0262  data_time: 0.0736  last_data_time: 0.0588   lr: 0.0001  max_mem: 32716M
[10/09 19:09:34] d2.utils.events INFO:  eta: 0:02:29  iter: 859  total_loss: 19.56  loss_ce: 0.4809  loss_mask: 0.1713  loss_dice: 1.21  loss_ce_0: 0.5457  loss_mask_0: 0.1982  loss_dice_0: 1.471  loss_ce_1: 0.6079  loss_mask_1: 0.1884  loss_dice_1: 1.355  loss_ce_2: 0.5426  loss_mask_2: 0.1785  loss_dice_2: 1.277  loss_ce_3: 0.5258  loss_mask_3: 0.1751  loss_dice_3: 1.227  loss_ce_4: 0.5082  loss_mask_4: 0.1746  loss_dice_4: 1.214  loss_ce_5: 0.4828  loss_mask_5: 0.1712  loss_dice_5: 1.22  loss_ce_6: 0.4861  loss_mask_6: 0.1704  loss_dice_6: 1.194  loss_ce_7: 0.4707  loss_mask_7: 0.1698  loss_dice_7: 1.181  loss_ce_8: 0.4654  loss_mask_8: 0.1683  loss_dice_8: 1.203    time: 1.0722  last_time: 1.0997  data_time: 0.0733  last_data_time: 0.0850   lr: 0.0001  max_mem: 32716M
[10/09 19:09:56] d2.utils.events INFO:  eta: 0:02:08  iter: 879  total_loss: 20.73  loss_ce: 0.4858  loss_mask: 0.1685  loss_dice: 1.27  loss_ce_0: 0.569  loss_mask_0: 0.1966  loss_dice_0: 1.536  loss_ce_1: 0.6622  loss_mask_1: 0.1841  loss_dice_1: 1.391  loss_ce_2: 0.5854  loss_mask_2: 0.1772  loss_dice_2: 1.322  loss_ce_3: 0.5577  loss_mask_3: 0.174  loss_dice_3: 1.316  loss_ce_4: 0.5119  loss_mask_4: 0.1703  loss_dice_4: 1.304  loss_ce_5: 0.5419  loss_mask_5: 0.1699  loss_dice_5: 1.289  loss_ce_6: 0.4851  loss_mask_6: 0.1704  loss_dice_6: 1.258  loss_ce_7: 0.4964  loss_mask_7: 0.1661  loss_dice_7: 1.303  loss_ce_8: 0.4892  loss_mask_8: 0.1677  loss_dice_8: 1.282    time: 1.0726  last_time: 1.0494  data_time: 0.0738  last_data_time: 0.0678   lr: 0.0001  max_mem: 32716M
[10/09 19:10:17] d2.utils.events INFO:  eta: 0:01:46  iter: 899  total_loss: 20.66  loss_ce: 0.5166  loss_mask: 0.1743  loss_dice: 1.269  loss_ce_0: 0.5926  loss_mask_0: 0.2047  loss_dice_0: 1.469  loss_ce_1: 0.653  loss_mask_1: 0.1858  loss_dice_1: 1.399  loss_ce_2: 0.5942  loss_mask_2: 0.1781  loss_dice_2: 1.373  loss_ce_3: 0.56  loss_mask_3: 0.1829  loss_dice_3: 1.304  loss_ce_4: 0.5316  loss_mask_4: 0.1782  loss_dice_4: 1.289  loss_ce_5: 0.5236  loss_mask_5: 0.178  loss_dice_5: 1.284  loss_ce_6: 0.5048  loss_mask_6: 0.1756  loss_dice_6: 1.276  loss_ce_7: 0.5189  loss_mask_7: 0.175  loss_dice_7: 1.271  loss_ce_8: 0.5177  loss_mask_8: 0.1741  loss_dice_8: 1.265    time: 1.0722  last_time: 1.0130  data_time: 0.0659  last_data_time: 0.0629   lr: 0.0001  max_mem: 32716M
[10/09 19:10:39] d2.utils.events INFO:  eta: 0:01:25  iter: 919  total_loss: 20.23  loss_ce: 0.5047  loss_mask: 0.171  loss_dice: 1.23  loss_ce_0: 0.5993  loss_mask_0: 0.2017  loss_dice_0: 1.505  loss_ce_1: 0.6582  loss_mask_1: 0.1785  loss_dice_1: 1.33  loss_ce_2: 0.6208  loss_mask_2: 0.1772  loss_dice_2: 1.265  loss_ce_3: 0.5691  loss_mask_3: 0.1768  loss_dice_3: 1.264  loss_ce_4: 0.5204  loss_mask_4: 0.1772  loss_dice_4: 1.25  loss_ce_5: 0.5231  loss_mask_5: 0.1751  loss_dice_5: 1.289  loss_ce_6: 0.5447  loss_mask_6: 0.1715  loss_dice_6: 1.234  loss_ce_7: 0.5082  loss_mask_7: 0.1737  loss_dice_7: 1.223  loss_ce_8: 0.5048  loss_mask_8: 0.1716  loss_dice_8: 1.216    time: 1.0726  last_time: 1.0652  data_time: 0.0698  last_data_time: 0.0625   lr: 0.0001  max_mem: 32716M
[10/09 19:11:01] d2.utils.events INFO:  eta: 0:01:04  iter: 939  total_loss: 20.29  loss_ce: 0.4981  loss_mask: 0.1847  loss_dice: 1.252  loss_ce_0: 0.5847  loss_mask_0: 0.2069  loss_dice_0: 1.471  loss_ce_1: 0.6363  loss_mask_1: 0.1906  loss_dice_1: 1.379  loss_ce_2: 0.5866  loss_mask_2: 0.1871  loss_dice_2: 1.34  loss_ce_3: 0.5403  loss_mask_3: 0.1895  loss_dice_3: 1.299  loss_ce_4: 0.5086  loss_mask_4: 0.1882  loss_dice_4: 1.265  loss_ce_5: 0.5103  loss_mask_5: 0.186  loss_dice_5: 1.294  loss_ce_6: 0.484  loss_mask_6: 0.1835  loss_dice_6: 1.282  loss_ce_7: 0.4785  loss_mask_7: 0.1834  loss_dice_7: 1.234  loss_ce_8: 0.466  loss_mask_8: 0.1859  loss_dice_8: 1.268    time: 1.0726  last_time: 1.0656  data_time: 0.0732  last_data_time: 0.0644   lr: 0.0001  max_mem: 32716M
[10/09 19:11:22] d2.utils.events INFO:  eta: 0:00:42  iter: 959  total_loss: 19.64  loss_ce: 0.4799  loss_mask: 0.1948  loss_dice: 1.176  loss_ce_0: 0.5868  loss_mask_0: 0.2169  loss_dice_0: 1.368  loss_ce_1: 0.609  loss_mask_1: 0.2069  loss_dice_1: 1.315  loss_ce_2: 0.5569  loss_mask_2: 0.1979  loss_dice_2: 1.243  loss_ce_3: 0.5221  loss_mask_3: 0.1935  loss_dice_3: 1.224  loss_ce_4: 0.504  loss_mask_4: 0.1898  loss_dice_4: 1.217  loss_ce_5: 0.4944  loss_mask_5: 0.1879  loss_dice_5: 1.221  loss_ce_6: 0.4767  loss_mask_6: 0.1942  loss_dice_6: 1.224  loss_ce_7: 0.4747  loss_mask_7: 0.1951  loss_dice_7: 1.178  loss_ce_8: 0.476  loss_mask_8: 0.1944  loss_dice_8: 1.186    time: 1.0724  last_time: 1.0408  data_time: 0.0643  last_data_time: 0.0797   lr: 0.0001  max_mem: 32716M
[10/09 19:11:43] d2.utils.events INFO:  eta: 0:00:21  iter: 979  total_loss: 21.24  loss_ce: 0.571  loss_mask: 0.1691  loss_dice: 1.262  loss_ce_0: 0.6038  loss_mask_0: 0.2036  loss_dice_0: 1.49  loss_ce_1: 0.6675  loss_mask_1: 0.18  loss_dice_1: 1.413  loss_ce_2: 0.6273  loss_mask_2: 0.1793  loss_dice_2: 1.376  loss_ce_3: 0.6346  loss_mask_3: 0.1742  loss_dice_3: 1.296  loss_ce_4: 0.5849  loss_mask_4: 0.1687  loss_dice_4: 1.285  loss_ce_5: 0.5639  loss_mask_5: 0.1713  loss_dice_5: 1.302  loss_ce_6: 0.5652  loss_mask_6: 0.1637  loss_dice_6: 1.255  loss_ce_7: 0.5436  loss_mask_7: 0.168  loss_dice_7: 1.271  loss_ce_8: 0.5641  loss_mask_8: 0.1694  loss_dice_8: 1.229    time: 1.0724  last_time: 1.0285  data_time: 0.0772  last_data_time: 0.0744   lr: 0.0001  max_mem: 32716M
[10/09 19:12:05] fvcore.common.checkpoint INFO: Saving checkpoint to /tsi/hi-paris/GB/segmentation/results/In_vocab/r50_008_1000_19_a_decoder/model_final.pth
[10/09 19:12:09] d2.utils.events INFO:  eta: 0:00:00  iter: 999  total_loss: 20.84  loss_ce: 0.5102  loss_mask: 0.1699  loss_dice: 1.328  loss_ce_0: 0.5715  loss_mask_0: 0.2022  loss_dice_0: 1.59  loss_ce_1: 0.6236  loss_mask_1: 0.1852  loss_dice_1: 1.5  loss_ce_2: 0.5879  loss_mask_2: 0.1721  loss_dice_2: 1.446  loss_ce_3: 0.5334  loss_mask_3: 0.1747  loss_dice_3: 1.386  loss_ce_4: 0.5139  loss_mask_4: 0.1715  loss_dice_4: 1.368  loss_ce_5: 0.5202  loss_mask_5: 0.1702  loss_dice_5: 1.332  loss_ce_6: 0.5245  loss_mask_6: 0.1699  loss_dice_6: 1.317  loss_ce_7: 0.4906  loss_mask_7: 0.1707  loss_dice_7: 1.322  loss_ce_8: 0.4968  loss_mask_8: 0.1668  loss_dice_8: 1.315    time: 1.0728  last_time: 1.1080  data_time: 0.0762  last_data_time: 0.0653   lr: 0.0001  max_mem: 32716M
[10/09 19:12:09] d2.engine.hooks INFO: Overall training speed: 998 iterations in 0:17:50 (1.0728 s / it)
[10/09 19:12:09] d2.engine.hooks INFO: Total training time: 0:17:57 (0:00:07 on hooks)
[10/09 19:12:09] fcclip.data.datasets.register_cityscapes_panoptic INFO: 3 cities found in 'datasets/cityscapes/leftImg8bit/val'.
[10/09 19:12:09] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(1024, 1024), max_size=2560, sample_style='choice')]
[10/09 19:12:09] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/09 19:12:09] d2.data.common INFO: Serializing 500 elements to byte tensors and concatenating them all ...
[10/09 19:12:09] d2.data.common INFO: Serialized dataset takes 0.74 MiB
[10/09 19:12:09] d2.evaluation.evaluator INFO: Start inference on 500 batches
[10/09 19:12:12] d2.evaluation.evaluator INFO: Inference done 11/500. Dataloading: 0.0050 s/iter. Inference: 0.1388 s/iter. Eval: 0.0649 s/iter. Total: 0.2088 s/iter. ETA=0:01:42
[10/09 19:12:17] d2.evaluation.evaluator INFO: Inference done 36/500. Dataloading: 0.0054 s/iter. Inference: 0.1385 s/iter. Eval: 0.0629 s/iter. Total: 0.2068 s/iter. ETA=0:01:35
[10/09 19:12:22] d2.evaluation.evaluator INFO: Inference done 60/500. Dataloading: 0.0054 s/iter. Inference: 0.1400 s/iter. Eval: 0.0629 s/iter. Total: 0.2083 s/iter. ETA=0:01:31
[10/09 19:12:28] d2.evaluation.evaluator INFO: Inference done 83/500. Dataloading: 0.0053 s/iter. Inference: 0.1437 s/iter. Eval: 0.0629 s/iter. Total: 0.2119 s/iter. ETA=0:01:28
[10/09 19:12:33] d2.evaluation.evaluator INFO: Inference done 105/500. Dataloading: 0.0052 s/iter. Inference: 0.1485 s/iter. Eval: 0.0625 s/iter. Total: 0.2162 s/iter. ETA=0:01:25
[10/09 19:12:38] d2.evaluation.evaluator INFO: Inference done 127/500. Dataloading: 0.0051 s/iter. Inference: 0.1506 s/iter. Eval: 0.0624 s/iter. Total: 0.2183 s/iter. ETA=0:01:21
[10/09 19:12:43] d2.evaluation.evaluator INFO: Inference done 150/500. Dataloading: 0.0051 s/iter. Inference: 0.1512 s/iter. Eval: 0.0628 s/iter. Total: 0.2192 s/iter. ETA=0:01:16
[10/09 19:12:48] d2.evaluation.evaluator INFO: Inference done 173/500. Dataloading: 0.0051 s/iter. Inference: 0.1512 s/iter. Eval: 0.0629 s/iter. Total: 0.2193 s/iter. ETA=0:01:11
[10/09 19:12:53] d2.evaluation.evaluator INFO: Inference done 196/500. Dataloading: 0.0051 s/iter. Inference: 0.1518 s/iter. Eval: 0.0632 s/iter. Total: 0.2201 s/iter. ETA=0:01:06
[10/09 19:12:58] d2.evaluation.evaluator INFO: Inference done 218/500. Dataloading: 0.0050 s/iter. Inference: 0.1530 s/iter. Eval: 0.0629 s/iter. Total: 0.2210 s/iter. ETA=0:01:02
[10/09 19:13:03] d2.evaluation.evaluator INFO: Inference done 242/500. Dataloading: 0.0052 s/iter. Inference: 0.1516 s/iter. Eval: 0.0630 s/iter. Total: 0.2199 s/iter. ETA=0:00:56
[10/09 19:13:08] d2.evaluation.evaluator INFO: Inference done 264/500. Dataloading: 0.0051 s/iter. Inference: 0.1531 s/iter. Eval: 0.0630 s/iter. Total: 0.2213 s/iter. ETA=0:00:52
[10/09 19:13:13] d2.evaluation.evaluator INFO: Inference done 287/500. Dataloading: 0.0051 s/iter. Inference: 0.1534 s/iter. Eval: 0.0631 s/iter. Total: 0.2216 s/iter. ETA=0:00:47
[10/09 19:13:19] d2.evaluation.evaluator INFO: Inference done 311/500. Dataloading: 0.0051 s/iter. Inference: 0.1529 s/iter. Eval: 0.0631 s/iter. Total: 0.2212 s/iter. ETA=0:00:41
[10/09 19:13:24] d2.evaluation.evaluator INFO: Inference done 333/500. Dataloading: 0.0051 s/iter. Inference: 0.1536 s/iter. Eval: 0.0629 s/iter. Total: 0.2217 s/iter. ETA=0:00:37
[10/09 19:13:29] d2.evaluation.evaluator INFO: Inference done 354/500. Dataloading: 0.0051 s/iter. Inference: 0.1551 s/iter. Eval: 0.0628 s/iter. Total: 0.2230 s/iter. ETA=0:00:32
[10/09 19:13:34] d2.evaluation.evaluator INFO: Inference done 378/500. Dataloading: 0.0050 s/iter. Inference: 0.1545 s/iter. Eval: 0.0629 s/iter. Total: 0.2225 s/iter. ETA=0:00:27
[10/09 19:13:39] d2.evaluation.evaluator INFO: Inference done 401/500. Dataloading: 0.0050 s/iter. Inference: 0.1546 s/iter. Eval: 0.0628 s/iter. Total: 0.2226 s/iter. ETA=0:00:22
[10/09 19:13:44] d2.evaluation.evaluator INFO: Inference done 424/500. Dataloading: 0.0050 s/iter. Inference: 0.1543 s/iter. Eval: 0.0630 s/iter. Total: 0.2224 s/iter. ETA=0:00:16
[10/09 19:13:49] d2.evaluation.evaluator INFO: Inference done 446/500. Dataloading: 0.0050 s/iter. Inference: 0.1548 s/iter. Eval: 0.0629 s/iter. Total: 0.2227 s/iter. ETA=0:00:12
[10/09 19:13:54] d2.evaluation.evaluator INFO: Inference done 468/500. Dataloading: 0.0050 s/iter. Inference: 0.1549 s/iter. Eval: 0.0631 s/iter. Total: 0.2231 s/iter. ETA=0:00:07
[10/09 19:13:59] d2.evaluation.evaluator INFO: Inference done 490/500. Dataloading: 0.0050 s/iter. Inference: 0.1552 s/iter. Eval: 0.0631 s/iter. Total: 0.2234 s/iter. ETA=0:00:02
[10/09 19:14:02] d2.evaluation.evaluator INFO: Total inference time: 0:01:50.542179 (0.223318 s / iter per device, on 1 devices)
[10/09 19:14:02] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:01:16 (0.154903 s / iter per device, on 1 devices)
[10/09 19:14:02] d2.evaluation.panoptic_evaluation INFO: Writing all panoptic predictions to /tmp/panoptic_evalvj99azwg ...
[10/09 19:14:25] d2.evaluation.panoptic_evaluation INFO: Panoptic Evaluation Results:
|        |   PQ   |   SQ   |   RQ   |  #categories  |
|:------:|:------:|:------:|:------:|:-------------:|
|  All   | 55.812 | 79.379 | 69.027 |      19       |
| Things | 46.837 | 78.601 | 59.460 |       8       |
| Stuff  | 62.339 | 79.945 | 75.985 |      11       |
[10/09 19:14:25] d2.evaluation.panoptic_evaluation INFO: Panoptic Evaluation Results:
|          |   PQ   |   SQ   |   RQ   |  #categories  |
|:--------:|:------:|:------:|:------:|:-------------:|
| class_7  | 96.954 | 97.256 | 99.690 |     Stuff     |
| class_8  | 71.659 | 82.878 | 86.463 |     Stuff     |
| class_11 | 85.886 | 88.217 | 97.358 |     Stuff     |
| class_12 | 42.361 | 79.028 | 53.602 |     Stuff     |
| class_13 | 37.383 | 74.384 | 50.256 |     Stuff     |
| class_17 | 39.065 | 62.248 | 62.757 |     Stuff     |
| class_19 | 41.524 | 66.687 | 62.267 |     Stuff     |
| class_20 | 60.173 | 74.373 | 80.906 |     Stuff     |
| class_21 | 88.616 | 89.350 | 99.179 |     Stuff     |
| class_22 | 35.765 | 74.294 | 48.140 |     Stuff     |
| class_23 | 86.341 | 90.678 | 95.216 |     Stuff     |
| class_24 | 47.854 | 74.546 | 64.194 |    Things     |
| class_25 | 45.812 | 71.246 | 64.301 |    Things     |
| class_26 | 61.032 | 80.889 | 75.452 |    Things     |
| class_27 | 42.846 | 85.287 | 50.237 |    Things     |
| class_28 | 58.172 | 90.055 | 64.596 |    Things     |
| class_31 | 47.902 | 82.740 | 57.895 |    Things     |
| class_32 | 31.147 | 73.579 | 42.331 |    Things     |
| class_33 | 39.934 | 70.464 | 56.673 |    Things     |
[10/09 19:14:25] d2.engine.defaults INFO: Evaluation results for openvocab_cityscapes_fine_panoptic_val in csv format:
[10/09 19:14:25] d2.evaluation.testing INFO: copypaste: Task: panoptic_seg
[10/09 19:14:25] d2.evaluation.testing INFO: copypaste: PQ,SQ,RQ,PQ_th,SQ_th,RQ_th,PQ_st,SQ_st,RQ_st,PQ_7,SQ_7,RQ_7,PQ_8,SQ_8,RQ_8,PQ_11,SQ_11,RQ_11,PQ_12,SQ_12,RQ_12,PQ_13,SQ_13,RQ_13,PQ_17,SQ_17,RQ_17,PQ_19,SQ_19,RQ_19,PQ_20,SQ_20,RQ_20,PQ_21,SQ_21,RQ_21,PQ_22,SQ_22,RQ_22,PQ_23,SQ_23,RQ_23,PQ_24,SQ_24,RQ_24,PQ_25,SQ_25,RQ_25,PQ_26,SQ_26,RQ_26,PQ_27,SQ_27,RQ_27,PQ_28,SQ_28,RQ_28,PQ_31,SQ_31,RQ_31,PQ_32,SQ_32,RQ_32,PQ_33,SQ_33,RQ_33
[10/09 19:14:25] d2.evaluation.testing INFO: copypaste: 55.8118,79.3789,69.0270,46.8373,78.6007,59.4598,62.3388,79.9448,75.9850,96.9541,97.2558,99.6898,71.6585,82.8778,86.4629,85.8865,88.2174,97.3577,42.3606,79.0276,53.6023,37.3828,74.3842,50.2564,39.0652,62.2481,62.7572,41.5243,66.6871,62.2673,60.1726,74.3733,80.9061,88.6162,89.3501,99.1786,35.7649,74.2935,48.1400,86.3406,90.6783,95.2164,47.8539,74.5457,64.1941,45.8117,71.2461,64.3006,61.0320,80.8887,75.4518,42.8457,85.2872,50.2370,58.1719,90.0545,64.5963,47.9024,82.7405,57.8947,31.1469,73.5790,42.3313,39.9341,70.4643,56.6728
