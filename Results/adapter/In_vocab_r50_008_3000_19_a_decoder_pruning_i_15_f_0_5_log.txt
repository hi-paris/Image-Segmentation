[10/19 11:31:06] detectron2 INFO: Rank of current process: 0. World size: 1
[10/19 11:31:07] detectron2 INFO: Environment info:
-------------------------------  -------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.8.19 | packaged by conda-forge | (default, Mar 20 2024, 12:47:35) [GCC 12.3.0]
numpy                            1.24.4
detectron2                       0.6 @/home/ids/gbrison/segmentation/segmentation/detectron2/detectron2
Compiler                         GCC 11.4
CUDA compiler                    CUDA 12.5
detectron2 arch flags            8.9
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.0+cu121 @/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0,1                          NVIDIA L40S (arch=8.9)
Driver version                   560.35.03
CUDA_HOME                        /usr/local/cuda
Pillow                           9.4.0
torchvision                      0.19.0+cu121 @/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torchvision
torchvision arch flags           5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.10.0
-------------------------------  -------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX512
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.0, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/19 11:31:07] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco/panoptic-segmentation/fcclip/r50_exp_3000_a_decoder_pruning_i_15_f_0_5.yaml', dist_url='tcp://127.0.0.1:51163', eval_only=False, machine_rank=0, num_gpus=1, num_machines=1, opts=[], resume=False)
[10/19 11:31:07] detectron2 INFO: Contents of args.config_file=configs/coco/panoptic-segmentation/fcclip/r50_exp_3000_a_decoder_pruning_i_15_f_0_5.yaml:
_BASE_: ./fcclip_convnext_large_eval_ade20k_r50.yaml

INPUT:
  MIN_SIZE_TEST: 1024
  MAX_SIZE_TEST: 2560

MODEL:
  SEM_SEG_HEAD:
    NUM_CLASSES: 19
  WEIGHTS: /tsi/hi-paris/GB/segmentation/models/fcclip_cocopan_r50.pth
OUTPUT_DIR: /tsi/hi-paris/GB/segmentation/results/In_vocab/r50_008_3000_19_a_decoder_pruning_i_15_f_0_5
DATASETS:
  TRAIN: ("openvocab_cityscapes_fine_panoptic_train",)
  TEST: ("openvocab_cityscapes_fine_panoptic_val",)
SOLVER:
  IMS_PER_BATCH: 8
  MAX_ITER: 3000
TEST:
  EVAL_PERIOD: 3000


[10/19 11:31:07] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - openvocab_cityscapes_fine_panoptic_val
  TRAIN:
  - openvocab_cityscapes_fine_panoptic_train
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: false
  CROP:
    ENABLED: false
    MINIMUM_INST_AREA: 1
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  DATASET_MAPPER_NAME: coco_panoptic_lsj
  FORMAT: RGB
  IMAGE_SIZE: 1024
  MASK_FORMAT: polygon
  MAX_SCALE: 2.0
  MAX_SIZE_TEST: 2560
  MAX_SIZE_TRAIN: 1333
  MIN_SCALE: 0.1
  MIN_SIZE_TEST: 1024
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: -1
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 0
    NAME: CLIP
  DEVICE: cuda
  FC_CLIP:
    CLIP_MODEL_NAME: RN50
    CLIP_PRETRAINED_WEIGHTS: openai
    EMBED_DIM: 1024
    ENSEMBLE_ON_VALID_MASK: true
    GEOMETRIC_ENSEMBLE_ALPHA: 0.4
    GEOMETRIC_ENSEMBLE_BETA: 0.8
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_FORMER:
    CLASS_WEIGHT: 2.0
    DEC_LAYERS: 10
    DEEP_SUPERVISION: true
    DICE_WEIGHT: 5.0
    DIM_FEEDFORWARD: 2048
    DROPOUT: 0.0
    ENC_LAYERS: 0
    ENFORCE_INPUT_PROJ: false
    HIDDEN_DIM: 256
    IMPORTANCE_SAMPLE_RATIO: 0.75
    MASK_WEIGHT: 5.0
    NHEADS: 8
    NO_OBJECT_WEIGHT: 0.1
    NUM_OBJECT_QUERIES: 250
    OVERSAMPLE_RATIO: 3.0
    PRE_NORM: false
    SIZE_DIVISIBILITY: 32
    TEST:
      INSTANCE_ON: true
      OBJECT_MASK_THRESHOLD: 0.0
      OVERLAP_THRESHOLD: 0.8
      PANOPTIC_ON: true
      SEMANTIC_ON: true
      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: false
    TRAIN_NUM_POINTS: 12544
    TRANSFORMER_DECODER_NAME: MultiScaleMaskedTransformerDecoder
    TRANSFORMER_IN_FEATURE: multi_scale_pixel_decoder
  MASK_ON: false
  META_ARCHITECTURE: FCCLIP
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 122.7709383
  - 116.7460125
  - 104.09373615
  PIXEL_STD:
  - 68.5005327
  - 66.6321579
  - 70.32316305
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 1
    - 1
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: basic
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 256
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES:
    - res3
    - res4
    - res5
    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8
    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    MASK_DIM: 256
    NAME: FCCLIPHead
    NORM: GN
    NUM_CLASSES: 19
    PIXEL_DECODER_NAME: MSDeformAttnPixelDecoder
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    TRANSFORMER_ENC_LAYERS: 6
    USE_DEPTHWISE_SEPARABLE_CONV: false
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    USE_CHECKPOINT: false
    WINDOW_SIZE: 7
  WEIGHTS: /tsi/hi-paris/GB/segmentation/models/fcclip_cocopan_r50.pth
OUTPUT_DIR: /tsi/hi-paris/GB/segmentation/results/In_vocab/r50_008_3000_19_a_decoder_pruning_i_15_f_0_5
SEED: -1
SOLVER:
  AMP:
    ENABLED: true
  BACKBONE_MULTIPLIER: 0.1
  BASE_LR: 0.0001
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 5000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.01
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 8
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 3000
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 327778
  - 355092
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 3000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[10/19 11:31:07] detectron2 INFO: Full config saved to /tsi/hi-paris/GB/segmentation/results/In_vocab/r50_008_3000_19_a_decoder_pruning_i_15_f_0_5/config.yaml
[10/19 11:31:07] d2.utils.env INFO: Using a generated random seed 8150379
[10/19 11:31:10] d2.engine.defaults INFO: Model:
FCCLIP(
  (backbone): CLIP(
    (clip_model): CLIP(
      (visual): ModifiedResNet(
        (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
        (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        (layer1): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
            (downsample): Sequential(
              (-1): AvgPool2d(kernel_size=1, stride=1, padding=0)
              (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
        )
        (layer2): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
            (downsample): Sequential(
              (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (3): Bottleneck(
            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
        )
        (layer3): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
            (downsample): Sequential(
              (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
              (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (3): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (4): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (5): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
        )
        (layer4): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
            (downsample): Sequential(
              (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
              (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
        )
        (attnpool): AttentionPool2d(
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (c_proj): Linear(in_features=2048, out_features=1024, bias=True)
        )
      )
      (transformer): Transformer(
        (resblocks): ModuleList(
          (0-11): 12 x ResidualAttentionBlock(
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ls_2): Identity()
          )
        )
      )
      (token_embedding): Embedding(49408, 512)
      (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
  )
  (sem_seg_head): FCCLIPHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0-8): 9 x SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0-8): 9 x CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0-8): 9 x FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(250, 256)
      (query_embed): Embedding(250, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0-2): 3 x Sequential()
      )
      (mask_embed): MLP(
        (layers): ModuleList(
          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (mask_pooling): MaskPooling()
      (_mask_pooling_proj): Sequential(
        (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
      )
      (class_embed): MLP(
        (layers): ModuleList(
          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=1024, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 19
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
  (conv1): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv2): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU()
  (mask_pooling): MaskPooling()
  (decoder_adapter): DecoderAdapter(
    (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
    (conv2): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
    (norm1): LayerNorm((128, 1, 1), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((256, 1, 1), eps=1e-05, elementwise_affine=True)
    (relu): ReLU()
  )
  (void_embedding): Embedding(1, 1024)
)
[10/19 11:31:10] fcclip.data.dataset_mappers.mask_former_semantic_dataset_mapper INFO: [MaskFormerPanopticDatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(800,), max_size=1333, sample_style='choice'), RandomFlip()]
[10/19 11:31:11] fcclip.data.datasets.register_cityscapes_panoptic INFO: 18 cities found in 'datasets/cityscapes/leftImg8bit/train'.
[10/19 11:31:11] d2.data.build INFO: Using training sampler TrainingSampler
[10/19 11:31:11] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/19 11:31:11] d2.data.common INFO: Serializing 2975 elements to byte tensors and concatenating them all ...
[10/19 11:31:11] d2.data.common INFO: Serialized dataset takes 4.12 MiB
[10/19 11:31:11] d2.data.build INFO: Making batched data loader with batch_size=8
[10/19 11:31:11] d2.solver.build WARNING: SOLVER.STEPS contains values larger than SOLVER.MAX_ITER. These values will be ignored.
[10/19 11:31:11] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /tsi/hi-paris/GB/segmentation/models/fcclip_cocopan_r50.pth ...
[10/19 11:31:11] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /tsi/hi-paris/GB/segmentation/models/fcclip_cocopan_r50.pth ...
[10/19 11:31:14] fvcore.common.checkpoint WARNING: Skip loading parameter 'criterion.empty_weight' to the model due to incompatible shapes: (134,) in the checkpoint but (20,) in the model! You might want to double check if this is expected.
[10/19 11:31:14] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.clip_model.ln_final.{bias, weight}[0m
[34mbackbone.clip_model.token_embedding.weight[0m
[34mbackbone.clip_model.transformer.resblocks.0.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.attnpool.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.attnpool.k_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.attnpool.positional_embedding[0m
[34mbackbone.clip_model.visual.attnpool.q_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.attnpool.v_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.conv1.weight[0m
[34mbackbone.clip_model.visual.conv2.weight[0m
[34mbackbone.clip_model.visual.conv3.weight[0m
[34mbackbone.clip_model.visual.layer1.0.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.0.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.0.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.0.conv1.weight[0m
[34mbackbone.clip_model.visual.layer1.0.conv2.weight[0m
[34mbackbone.clip_model.visual.layer1.0.conv3.weight[0m
[34mbackbone.clip_model.visual.layer1.0.downsample.0.weight[0m
[34mbackbone.clip_model.visual.layer1.0.downsample.1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.1.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.1.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.1.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.1.conv1.weight[0m
[34mbackbone.clip_model.visual.layer1.1.conv2.weight[0m
[34mbackbone.clip_model.visual.layer1.1.conv3.weight[0m
[34mbackbone.clip_model.visual.layer1.2.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.2.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.2.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.2.conv1.weight[0m
[34mbackbone.clip_model.visual.layer1.2.conv2.weight[0m
[34mbackbone.clip_model.visual.layer1.2.conv3.weight[0m
[34mbackbone.clip_model.visual.layer2.0.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.0.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.0.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.0.conv1.weight[0m
[34mbackbone.clip_model.visual.layer2.0.conv2.weight[0m
[34mbackbone.clip_model.visual.layer2.0.conv3.weight[0m
[34mbackbone.clip_model.visual.layer2.0.downsample.0.weight[0m
[34mbackbone.clip_model.visual.layer2.0.downsample.1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.1.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.1.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.1.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.1.conv1.weight[0m
[34mbackbone.clip_model.visual.layer2.1.conv2.weight[0m
[34mbackbone.clip_model.visual.layer2.1.conv3.weight[0m
[34mbackbone.clip_model.visual.layer2.2.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.2.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.2.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.2.conv1.weight[0m
[34mbackbone.clip_model.visual.layer2.2.conv2.weight[0m
[34mbackbone.clip_model.visual.layer2.2.conv3.weight[0m
[34mbackbone.clip_model.visual.layer2.3.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.3.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.3.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.3.conv1.weight[0m
[34mbackbone.clip_model.visual.layer2.3.conv2.weight[0m
[34mbackbone.clip_model.visual.layer2.3.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.0.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.0.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.0.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.0.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.0.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.0.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.0.downsample.0.weight[0m
[34mbackbone.clip_model.visual.layer3.0.downsample.1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.1.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.1.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.1.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.1.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.1.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.1.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.2.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.2.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.2.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.2.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.2.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.2.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.3.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.3.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.3.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.3.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.3.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.3.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.4.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.4.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.4.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.4.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.4.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.4.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.5.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.5.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.5.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.5.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.5.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.5.conv3.weight[0m
[34mbackbone.clip_model.visual.layer4.0.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.0.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.0.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.0.conv1.weight[0m
[34mbackbone.clip_model.visual.layer4.0.conv2.weight[0m
[34mbackbone.clip_model.visual.layer4.0.conv3.weight[0m
[34mbackbone.clip_model.visual.layer4.0.downsample.0.weight[0m
[34mbackbone.clip_model.visual.layer4.0.downsample.1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.1.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.1.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.1.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.1.conv1.weight[0m
[34mbackbone.clip_model.visual.layer4.1.conv2.weight[0m
[34mbackbone.clip_model.visual.layer4.1.conv3.weight[0m
[34mbackbone.clip_model.visual.layer4.2.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.2.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.2.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.2.conv1.weight[0m
[34mbackbone.clip_model.visual.layer4.2.conv2.weight[0m
[34mbackbone.clip_model.visual.layer4.2.conv3.weight[0m
[34mbackbone.clip_model.{logit_scale, positional_embedding, text_projection}[0m
[34mbn1.{bias, running_mean, running_var, weight}[0m
[34mbn2.{bias, running_mean, running_var, weight}[0m
[34mconv1.{bias, weight}[0m
[34mconv2.{bias, weight}[0m
[34mcriterion.empty_weight[0m
[34mdecoder_adapter.conv1.{bias, weight}[0m
[34mdecoder_adapter.conv2.{bias, weight}[0m
[34mdecoder_adapter.norm1.{bias, weight}[0m
[34mdecoder_adapter.norm2.{bias, weight}[0m
[10/19 11:31:14] d2.engine.train_loop INFO: Starting training from iteration 0
[10/19 11:31:37] d2.utils.events INFO:  eta: 0:50:10  iter: 19  total_loss: 29.27  loss_ce: 1.088  loss_mask: 0.3117  loss_dice: 1.412  loss_ce_0: 1.346  loss_mask_0: 0.3306  loss_dice_0: 1.692  loss_ce_1: 1.342  loss_mask_1: 0.321  loss_dice_1: 1.569  loss_ce_2: 1.202  loss_mask_2: 0.3139  loss_dice_2: 1.516  loss_ce_3: 1.123  loss_mask_3: 0.3158  loss_dice_3: 1.464  loss_ce_4: 1.081  loss_mask_4: 0.3252  loss_dice_4: 1.452  loss_ce_5: 1.123  loss_mask_5: 0.3147  loss_dice_5: 1.47  loss_ce_6: 1.115  loss_mask_6: 0.3233  loss_dice_6: 1.42  loss_ce_7: 1.143  loss_mask_7: 0.3168  loss_dice_7: 1.437  loss_ce_8: 1.103  loss_mask_8: 0.3167  loss_dice_8: 1.41    time: 1.0232  last_time: 1.0000  data_time: 0.0963  last_data_time: 0.0469   lr: 0.0001  max_mem: 31411M
[10/19 11:31:58] d2.utils.events INFO:  eta: 0:51:14  iter: 39  total_loss: 25.85  loss_ce: 0.8113  loss_mask: 0.2334  loss_dice: 1.46  loss_ce_0: 0.9041  loss_mask_0: 0.2814  loss_dice_0: 1.787  loss_ce_1: 0.9575  loss_mask_1: 0.258  loss_dice_1: 1.634  loss_ce_2: 0.8911  loss_mask_2: 0.2405  loss_dice_2: 1.52  loss_ce_3: 0.8578  loss_mask_3: 0.2332  loss_dice_3: 1.46  loss_ce_4: 0.8334  loss_mask_4: 0.2327  loss_dice_4: 1.497  loss_ce_5: 0.835  loss_mask_5: 0.235  loss_dice_5: 1.478  loss_ce_6: 0.7898  loss_mask_6: 0.2307  loss_dice_6: 1.467  loss_ce_7: 0.7925  loss_mask_7: 0.2354  loss_dice_7: 1.464  loss_ce_8: 0.8152  loss_mask_8: 0.2329  loss_dice_8: 1.434    time: 1.0370  last_time: 1.0444  data_time: 0.0734  last_data_time: 0.0508   lr: 0.0001  max_mem: 31411M
[10/19 11:32:19] d2.utils.events INFO:  eta: 0:50:53  iter: 59  total_loss: 24.65  loss_ce: 0.6627  loss_mask: 0.209  loss_dice: 1.414  loss_ce_0: 0.7338  loss_mask_0: 0.246  loss_dice_0: 1.778  loss_ce_1: 0.8488  loss_mask_1: 0.2284  loss_dice_1: 1.61  loss_ce_2: 0.7994  loss_mask_2: 0.2194  loss_dice_2: 1.519  loss_ce_3: 0.7254  loss_mask_3: 0.2169  loss_dice_3: 1.439  loss_ce_4: 0.696  loss_mask_4: 0.217  loss_dice_4: 1.434  loss_ce_5: 0.6865  loss_mask_5: 0.2163  loss_dice_5: 1.42  loss_ce_6: 0.6638  loss_mask_6: 0.2104  loss_dice_6: 1.406  loss_ce_7: 0.6708  loss_mask_7: 0.2086  loss_dice_7: 1.408  loss_ce_8: 0.6841  loss_mask_8: 0.2122  loss_dice_8: 1.383    time: 1.0409  last_time: 1.0674  data_time: 0.0732  last_data_time: 0.0661   lr: 0.0001  max_mem: 31411M
[10/19 11:32:40] d2.utils.events INFO:  eta: 0:50:36  iter: 79  total_loss: 24.01  loss_ce: 0.6508  loss_mask: 0.1877  loss_dice: 1.377  loss_ce_0: 0.6966  loss_mask_0: 0.2188  loss_dice_0: 1.704  loss_ce_1: 0.856  loss_mask_1: 0.2017  loss_dice_1: 1.564  loss_ce_2: 0.7841  loss_mask_2: 0.1982  loss_dice_2: 1.474  loss_ce_3: 0.7037  loss_mask_3: 0.1893  loss_dice_3: 1.404  loss_ce_4: 0.6989  loss_mask_4: 0.1883  loss_dice_4: 1.409  loss_ce_5: 0.6681  loss_mask_5: 0.1874  loss_dice_5: 1.406  loss_ce_6: 0.6702  loss_mask_6: 0.1869  loss_dice_6: 1.392  loss_ce_7: 0.6949  loss_mask_7: 0.1882  loss_dice_7: 1.364  loss_ce_8: 0.6567  loss_mask_8: 0.1907  loss_dice_8: 1.389    time: 1.0418  last_time: 1.0375  data_time: 0.0732  last_data_time: 0.0784   lr: 0.0001  max_mem: 31411M
[10/19 11:33:01] d2.utils.events INFO:  eta: 0:50:22  iter: 99  total_loss: 23.08  loss_ce: 0.6186  loss_mask: 0.2031  loss_dice: 1.367  loss_ce_0: 0.7268  loss_mask_0: 0.2379  loss_dice_0: 1.603  loss_ce_1: 0.7945  loss_mask_1: 0.2156  loss_dice_1: 1.535  loss_ce_2: 0.7188  loss_mask_2: 0.2096  loss_dice_2: 1.452  loss_ce_3: 0.647  loss_mask_3: 0.2097  loss_dice_3: 1.394  loss_ce_4: 0.6361  loss_mask_4: 0.2098  loss_dice_4: 1.388  loss_ce_5: 0.6289  loss_mask_5: 0.206  loss_dice_5: 1.38  loss_ce_6: 0.6173  loss_mask_6: 0.2063  loss_dice_6: 1.356  loss_ce_7: 0.6132  loss_mask_7: 0.2052  loss_dice_7: 1.341  loss_ce_8: 0.6169  loss_mask_8: 0.2059  loss_dice_8: 1.338    time: 1.0432  last_time: 1.0903  data_time: 0.0728  last_data_time: 0.0810   lr: 0.0001  max_mem: 31411M
[10/19 11:33:21] d2.utils.events INFO:  eta: 0:49:51  iter: 119  total_loss: 22.58  loss_ce: 0.6269  loss_mask: 0.2157  loss_dice: 1.303  loss_ce_0: 0.7135  loss_mask_0: 0.255  loss_dice_0: 1.563  loss_ce_1: 0.8096  loss_mask_1: 0.2397  loss_dice_1: 1.458  loss_ce_2: 0.7086  loss_mask_2: 0.2218  loss_dice_2: 1.383  loss_ce_3: 0.6426  loss_mask_3: 0.2261  loss_dice_3: 1.37  loss_ce_4: 0.6068  loss_mask_4: 0.2268  loss_dice_4: 1.352  loss_ce_5: 0.6402  loss_mask_5: 0.2253  loss_dice_5: 1.312  loss_ce_6: 0.6306  loss_mask_6: 0.2177  loss_dice_6: 1.329  loss_ce_7: 0.6312  loss_mask_7: 0.2185  loss_dice_7: 1.3  loss_ce_8: 0.6355  loss_mask_8: 0.216  loss_dice_8: 1.302    time: 1.0396  last_time: 1.0688  data_time: 0.0619  last_data_time: 0.0810   lr: 0.0001  max_mem: 31411M
[10/19 11:33:43] d2.utils.events INFO:  eta: 0:49:34  iter: 139  total_loss: 23.33  loss_ce: 0.6174  loss_mask: 0.2044  loss_dice: 1.403  loss_ce_0: 0.6856  loss_mask_0: 0.2342  loss_dice_0: 1.667  loss_ce_1: 0.7933  loss_mask_1: 0.2159  loss_dice_1: 1.522  loss_ce_2: 0.6898  loss_mask_2: 0.2072  loss_dice_2: 1.488  loss_ce_3: 0.6267  loss_mask_3: 0.2068  loss_dice_3: 1.432  loss_ce_4: 0.6262  loss_mask_4: 0.2081  loss_dice_4: 1.426  loss_ce_5: 0.6293  loss_mask_5: 0.2025  loss_dice_5: 1.391  loss_ce_6: 0.6384  loss_mask_6: 0.2046  loss_dice_6: 1.397  loss_ce_7: 0.6259  loss_mask_7: 0.2052  loss_dice_7: 1.398  loss_ce_8: 0.6131  loss_mask_8: 0.2034  loss_dice_8: 1.384    time: 1.0430  last_time: 0.9779  data_time: 0.0747  last_data_time: 0.0506   lr: 0.0001  max_mem: 32638M
[10/19 11:34:04] d2.utils.events INFO:  eta: 0:49:16  iter: 159  total_loss: 22.23  loss_ce: 0.5968  loss_mask: 0.1839  loss_dice: 1.334  loss_ce_0: 0.6966  loss_mask_0: 0.2248  loss_dice_0: 1.595  loss_ce_1: 0.817  loss_mask_1: 0.1967  loss_dice_1: 1.465  loss_ce_2: 0.6819  loss_mask_2: 0.19  loss_dice_2: 1.422  loss_ce_3: 0.6453  loss_mask_3: 0.1846  loss_dice_3: 1.369  loss_ce_4: 0.6212  loss_mask_4: 0.1875  loss_dice_4: 1.372  loss_ce_5: 0.633  loss_mask_5: 0.187  loss_dice_5: 1.352  loss_ce_6: 0.6129  loss_mask_6: 0.1856  loss_dice_6: 1.369  loss_ce_7: 0.6092  loss_mask_7: 0.1828  loss_dice_7: 1.334  loss_ce_8: 0.612  loss_mask_8: 0.184  loss_dice_8: 1.333    time: 1.0433  last_time: 1.0858  data_time: 0.0717  last_data_time: 0.0781   lr: 0.0001  max_mem: 32638M
[10/19 11:34:25] d2.utils.events INFO:  eta: 0:48:54  iter: 179  total_loss: 22.48  loss_ce: 0.6006  loss_mask: 0.2058  loss_dice: 1.344  loss_ce_0: 0.6646  loss_mask_0: 0.2305  loss_dice_0: 1.61  loss_ce_1: 0.7835  loss_mask_1: 0.2153  loss_dice_1: 1.528  loss_ce_2: 0.7053  loss_mask_2: 0.2084  loss_dice_2: 1.424  loss_ce_3: 0.619  loss_mask_3: 0.2027  loss_dice_3: 1.397  loss_ce_4: 0.5977  loss_mask_4: 0.2018  loss_dice_4: 1.402  loss_ce_5: 0.6056  loss_mask_5: 0.1991  loss_dice_5: 1.332  loss_ce_6: 0.5868  loss_mask_6: 0.2001  loss_dice_6: 1.318  loss_ce_7: 0.5924  loss_mask_7: 0.2031  loss_dice_7: 1.353  loss_ce_8: 0.5924  loss_mask_8: 0.2031  loss_dice_8: 1.361    time: 1.0434  last_time: 1.0624  data_time: 0.0702  last_data_time: 0.0733   lr: 0.0001  max_mem: 32638M
[10/19 11:34:45] d2.utils.events INFO:  eta: 0:48:32  iter: 199  total_loss: 21.4  loss_ce: 0.5576  loss_mask: 0.2016  loss_dice: 1.253  loss_ce_0: 0.6171  loss_mask_0: 0.2313  loss_dice_0: 1.516  loss_ce_1: 0.7332  loss_mask_1: 0.2186  loss_dice_1: 1.448  loss_ce_2: 0.6655  loss_mask_2: 0.21  loss_dice_2: 1.391  loss_ce_3: 0.6113  loss_mask_3: 0.206  loss_dice_3: 1.338  loss_ce_4: 0.5746  loss_mask_4: 0.2078  loss_dice_4: 1.282  loss_ce_5: 0.5627  loss_mask_5: 0.2026  loss_dice_5: 1.297  loss_ce_6: 0.5223  loss_mask_6: 0.2017  loss_dice_6: 1.28  loss_ce_7: 0.5476  loss_mask_7: 0.2001  loss_dice_7: 1.279  loss_ce_8: 0.5394  loss_mask_8: 0.2011  loss_dice_8: 1.267    time: 1.0420  last_time: 1.0394  data_time: 0.0648  last_data_time: 0.0804   lr: 0.0001  max_mem: 32638M
[10/19 11:35:06] d2.utils.events INFO:  eta: 0:48:12  iter: 219  total_loss: 21.04  loss_ce: 0.5453  loss_mask: 0.1899  loss_dice: 1.278  loss_ce_0: 0.6553  loss_mask_0: 0.2166  loss_dice_0: 1.502  loss_ce_1: 0.7385  loss_mask_1: 0.2051  loss_dice_1: 1.424  loss_ce_2: 0.6431  loss_mask_2: 0.1939  loss_dice_2: 1.353  loss_ce_3: 0.5846  loss_mask_3: 0.1911  loss_dice_3: 1.268  loss_ce_4: 0.5479  loss_mask_4: 0.1933  loss_dice_4: 1.319  loss_ce_5: 0.5549  loss_mask_5: 0.1948  loss_dice_5: 1.282  loss_ce_6: 0.535  loss_mask_6: 0.1926  loss_dice_6: 1.249  loss_ce_7: 0.5371  loss_mask_7: 0.1921  loss_dice_7: 1.293  loss_ce_8: 0.5264  loss_mask_8: 0.1905  loss_dice_8: 1.258    time: 1.0432  last_time: 1.0234  data_time: 0.0738  last_data_time: 0.0765   lr: 0.0001  max_mem: 32638M
[10/19 11:35:27] d2.utils.events INFO:  eta: 0:47:50  iter: 239  total_loss: 21.06  loss_ce: 0.5531  loss_mask: 0.1941  loss_dice: 1.262  loss_ce_0: 0.6255  loss_mask_0: 0.2293  loss_dice_0: 1.51  loss_ce_1: 0.7219  loss_mask_1: 0.2085  loss_dice_1: 1.396  loss_ce_2: 0.6595  loss_mask_2: 0.1991  loss_dice_2: 1.338  loss_ce_3: 0.598  loss_mask_3: 0.2001  loss_dice_3: 1.295  loss_ce_4: 0.552  loss_mask_4: 0.1999  loss_dice_4: 1.318  loss_ce_5: 0.5679  loss_mask_5: 0.1946  loss_dice_5: 1.244  loss_ce_6: 0.5635  loss_mask_6: 0.1902  loss_dice_6: 1.256  loss_ce_7: 0.6045  loss_mask_7: 0.1911  loss_dice_7: 1.268  loss_ce_8: 0.5716  loss_mask_8: 0.1955  loss_dice_8: 1.277    time: 1.0428  last_time: 1.0109  data_time: 0.0685  last_data_time: 0.0444   lr: 0.0001  max_mem: 32638M
[10/19 11:35:48] d2.utils.events INFO:  eta: 0:47:29  iter: 259  total_loss: 22.31  loss_ce: 0.5395  loss_mask: 0.1935  loss_dice: 1.326  loss_ce_0: 0.6927  loss_mask_0: 0.2308  loss_dice_0: 1.618  loss_ce_1: 0.762  loss_mask_1: 0.2077  loss_dice_1: 1.473  loss_ce_2: 0.6886  loss_mask_2: 0.1905  loss_dice_2: 1.407  loss_ce_3: 0.6046  loss_mask_3: 0.1925  loss_dice_3: 1.335  loss_ce_4: 0.5664  loss_mask_4: 0.191  loss_dice_4: 1.32  loss_ce_5: 0.5718  loss_mask_5: 0.1894  loss_dice_5: 1.319  loss_ce_6: 0.5391  loss_mask_6: 0.1972  loss_dice_6: 1.313  loss_ce_7: 0.5398  loss_mask_7: 0.1891  loss_dice_7: 1.297  loss_ce_8: 0.5806  loss_mask_8: 0.1898  loss_dice_8: 1.312    time: 1.0427  last_time: 1.0520  data_time: 0.0686  last_data_time: 0.0819   lr: 0.0001  max_mem: 32638M
[10/19 11:36:09] d2.utils.events INFO:  eta: 0:47:09  iter: 279  total_loss: 22.41  loss_ce: 0.5511  loss_mask: 0.1761  loss_dice: 1.336  loss_ce_0: 0.6263  loss_mask_0: 0.1999  loss_dice_0: 1.626  loss_ce_1: 0.7342  loss_mask_1: 0.1902  loss_dice_1: 1.533  loss_ce_2: 0.6673  loss_mask_2: 0.1842  loss_dice_2: 1.454  loss_ce_3: 0.6013  loss_mask_3: 0.179  loss_dice_3: 1.383  loss_ce_4: 0.6014  loss_mask_4: 0.1786  loss_dice_4: 1.399  loss_ce_5: 0.5423  loss_mask_5: 0.1804  loss_dice_5: 1.401  loss_ce_6: 0.5726  loss_mask_6: 0.1804  loss_dice_6: 1.354  loss_ce_7: 0.5485  loss_mask_7: 0.1797  loss_dice_7: 1.352  loss_ce_8: 0.5609  loss_mask_8: 0.1777  loss_dice_8: 1.359    time: 1.0441  last_time: 1.0976  data_time: 0.0751  last_data_time: 0.0817   lr: 0.0001  max_mem: 32638M
[10/19 11:36:30] d2.utils.events INFO:  eta: 0:46:48  iter: 299  total_loss: 20.42  loss_ce: 0.4645  loss_mask: 0.1867  loss_dice: 1.257  loss_ce_0: 0.6026  loss_mask_0: 0.2138  loss_dice_0: 1.498  loss_ce_1: 0.7032  loss_mask_1: 0.205  loss_dice_1: 1.405  loss_ce_2: 0.6073  loss_mask_2: 0.1825  loss_dice_2: 1.326  loss_ce_3: 0.5583  loss_mask_3: 0.1814  loss_dice_3: 1.309  loss_ce_4: 0.541  loss_mask_4: 0.1909  loss_dice_4: 1.291  loss_ce_5: 0.5217  loss_mask_5: 0.1894  loss_dice_5: 1.281  loss_ce_6: 0.5084  loss_mask_6: 0.1928  loss_dice_6: 1.271  loss_ce_7: 0.4849  loss_mask_7: 0.1933  loss_dice_7: 1.247  loss_ce_8: 0.4899  loss_mask_8: 0.1911  loss_dice_8: 1.267    time: 1.0439  last_time: 1.0120  data_time: 0.0722  last_data_time: 0.0611   lr: 0.0001  max_mem: 32638M
[10/19 11:36:52] d2.utils.events INFO:  eta: 0:46:27  iter: 319  total_loss: 22.48  loss_ce: 0.6076  loss_mask: 0.2019  loss_dice: 1.337  loss_ce_0: 0.6721  loss_mask_0: 0.2234  loss_dice_0: 1.549  loss_ce_1: 0.7815  loss_mask_1: 0.2146  loss_dice_1: 1.485  loss_ce_2: 0.7287  loss_mask_2: 0.2101  loss_dice_2: 1.397  loss_ce_3: 0.6618  loss_mask_3: 0.2102  loss_dice_3: 1.353  loss_ce_4: 0.6465  loss_mask_4: 0.2098  loss_dice_4: 1.334  loss_ce_5: 0.6254  loss_mask_5: 0.2048  loss_dice_5: 1.378  loss_ce_6: 0.6229  loss_mask_6: 0.2039  loss_dice_6: 1.331  loss_ce_7: 0.5994  loss_mask_7: 0.2031  loss_dice_7: 1.332  loss_ce_8: 0.612  loss_mask_8: 0.2019  loss_dice_8: 1.345    time: 1.0447  last_time: 1.0683  data_time: 0.0813  last_data_time: 0.0532   lr: 0.0001  max_mem: 32638M
[10/19 11:37:12] d2.utils.events INFO:  eta: 0:46:06  iter: 339  total_loss: 21.15  loss_ce: 0.5274  loss_mask: 0.1819  loss_dice: 1.302  loss_ce_0: 0.6291  loss_mask_0: 0.2158  loss_dice_0: 1.483  loss_ce_1: 0.6797  loss_mask_1: 0.1986  loss_dice_1: 1.427  loss_ce_2: 0.6494  loss_mask_2: 0.1893  loss_dice_2: 1.383  loss_ce_3: 0.5574  loss_mask_3: 0.1844  loss_dice_3: 1.327  loss_ce_4: 0.5729  loss_mask_4: 0.1835  loss_dice_4: 1.288  loss_ce_5: 0.5566  loss_mask_5: 0.183  loss_dice_5: 1.313  loss_ce_6: 0.5367  loss_mask_6: 0.1857  loss_dice_6: 1.303  loss_ce_7: 0.5825  loss_mask_7: 0.1817  loss_dice_7: 1.271  loss_ce_8: 0.537  loss_mask_8: 0.1824  loss_dice_8: 1.274    time: 1.0443  last_time: 1.0595  data_time: 0.0688  last_data_time: 0.0544   lr: 0.0001  max_mem: 32638M
[10/19 11:37:33] d2.utils.events INFO:  eta: 0:45:45  iter: 359  total_loss: 22.1  loss_ce: 0.6055  loss_mask: 0.1896  loss_dice: 1.307  loss_ce_0: 0.6801  loss_mask_0: 0.2188  loss_dice_0: 1.524  loss_ce_1: 0.7415  loss_mask_1: 0.2007  loss_dice_1: 1.449  loss_ce_2: 0.7012  loss_mask_2: 0.1923  loss_dice_2: 1.406  loss_ce_3: 0.6591  loss_mask_3: 0.1918  loss_dice_3: 1.353  loss_ce_4: 0.6644  loss_mask_4: 0.1893  loss_dice_4: 1.371  loss_ce_5: 0.6038  loss_mask_5: 0.1896  loss_dice_5: 1.336  loss_ce_6: 0.6177  loss_mask_6: 0.1884  loss_dice_6: 1.32  loss_ce_7: 0.6268  loss_mask_7: 0.1932  loss_dice_7: 1.314  loss_ce_8: 0.6014  loss_mask_8: 0.1925  loss_dice_8: 1.303    time: 1.0440  last_time: 1.0252  data_time: 0.0696  last_data_time: 0.0738   lr: 0.0001  max_mem: 32638M
[10/19 11:37:55] d2.utils.events INFO:  eta: 0:45:27  iter: 379  total_loss: 21.41  loss_ce: 0.5735  loss_mask: 0.1909  loss_dice: 1.261  loss_ce_0: 0.6371  loss_mask_0: 0.2119  loss_dice_0: 1.585  loss_ce_1: 0.722  loss_mask_1: 0.1938  loss_dice_1: 1.457  loss_ce_2: 0.6545  loss_mask_2: 0.1934  loss_dice_2: 1.387  loss_ce_3: 0.6206  loss_mask_3: 0.1942  loss_dice_3: 1.317  loss_ce_4: 0.6033  loss_mask_4: 0.1915  loss_dice_4: 1.288  loss_ce_5: 0.5937  loss_mask_5: 0.1923  loss_dice_5: 1.309  loss_ce_6: 0.5922  loss_mask_6: 0.1909  loss_dice_6: 1.294  loss_ce_7: 0.562  loss_mask_7: 0.1914  loss_dice_7: 1.276  loss_ce_8: 0.563  loss_mask_8: 0.1929  loss_dice_8: 1.273    time: 1.0456  last_time: 1.0822  data_time: 0.0714  last_data_time: 0.0658   lr: 0.0001  max_mem: 32638M
[10/19 11:38:16] d2.utils.events INFO:  eta: 0:45:07  iter: 399  total_loss: 22.08  loss_ce: 0.5638  loss_mask: 0.1868  loss_dice: 1.346  loss_ce_0: 0.6301  loss_mask_0: 0.2186  loss_dice_0: 1.643  loss_ce_1: 0.7325  loss_mask_1: 0.1966  loss_dice_1: 1.483  loss_ce_2: 0.6627  loss_mask_2: 0.1915  loss_dice_2: 1.403  loss_ce_3: 0.5724  loss_mask_3: 0.1897  loss_dice_3: 1.372  loss_ce_4: 0.5678  loss_mask_4: 0.1884  loss_dice_4: 1.408  loss_ce_5: 0.5567  loss_mask_5: 0.1857  loss_dice_5: 1.345  loss_ce_6: 0.5398  loss_mask_6: 0.1865  loss_dice_6: 1.348  loss_ce_7: 0.5571  loss_mask_7: 0.1855  loss_dice_7: 1.345  loss_ce_8: 0.5629  loss_mask_8: 0.1864  loss_dice_8: 1.325    time: 1.0460  last_time: 1.0822  data_time: 0.0680  last_data_time: 0.0705   lr: 0.0001  max_mem: 32638M
[10/19 11:38:37] d2.utils.events INFO:  eta: 0:44:48  iter: 419  total_loss: 21.13  loss_ce: 0.5361  loss_mask: 0.1699  loss_dice: 1.294  loss_ce_0: 0.6524  loss_mask_0: 0.2003  loss_dice_0: 1.561  loss_ce_1: 0.7328  loss_mask_1: 0.1855  loss_dice_1: 1.446  loss_ce_2: 0.6507  loss_mask_2: 0.1743  loss_dice_2: 1.356  loss_ce_3: 0.5779  loss_mask_3: 0.173  loss_dice_3: 1.322  loss_ce_4: 0.578  loss_mask_4: 0.1729  loss_dice_4: 1.322  loss_ce_5: 0.5555  loss_mask_5: 0.173  loss_dice_5: 1.343  loss_ce_6: 0.5215  loss_mask_6: 0.1721  loss_dice_6: 1.279  loss_ce_7: 0.5351  loss_mask_7: 0.1702  loss_dice_7: 1.275  loss_ce_8: 0.5286  loss_mask_8: 0.1697  loss_dice_8: 1.279    time: 1.0470  last_time: 1.1295  data_time: 0.0762  last_data_time: 0.0644   lr: 0.0001  max_mem: 32638M
[10/19 11:38:58] d2.utils.events INFO:  eta: 0:44:28  iter: 439  total_loss: 20.9  loss_ce: 0.5222  loss_mask: 0.1847  loss_dice: 1.228  loss_ce_0: 0.6011  loss_mask_0: 0.2067  loss_dice_0: 1.457  loss_ce_1: 0.7087  loss_mask_1: 0.1978  loss_dice_1: 1.37  loss_ce_2: 0.6481  loss_mask_2: 0.1878  loss_dice_2: 1.333  loss_ce_3: 0.6014  loss_mask_3: 0.1921  loss_dice_3: 1.277  loss_ce_4: 0.5568  loss_mask_4: 0.1907  loss_dice_4: 1.244  loss_ce_5: 0.5535  loss_mask_5: 0.187  loss_dice_5: 1.268  loss_ce_6: 0.5666  loss_mask_6: 0.1844  loss_dice_6: 1.258  loss_ce_7: 0.5265  loss_mask_7: 0.1852  loss_dice_7: 1.243  loss_ce_8: 0.5454  loss_mask_8: 0.1847  loss_dice_8: 1.207    time: 1.0472  last_time: 1.0891  data_time: 0.0709  last_data_time: 0.0699   lr: 0.0001  max_mem: 32638M
[10/19 11:39:20] d2.utils.events INFO:  eta: 0:44:08  iter: 459  total_loss: 20.64  loss_ce: 0.4737  loss_mask: 0.1804  loss_dice: 1.278  loss_ce_0: 0.606  loss_mask_0: 0.2141  loss_dice_0: 1.505  loss_ce_1: 0.6634  loss_mask_1: 0.1946  loss_dice_1: 1.389  loss_ce_2: 0.6155  loss_mask_2: 0.188  loss_dice_2: 1.365  loss_ce_3: 0.5978  loss_mask_3: 0.1796  loss_dice_3: 1.299  loss_ce_4: 0.5388  loss_mask_4: 0.182  loss_dice_4: 1.308  loss_ce_5: 0.5331  loss_mask_5: 0.1812  loss_dice_5: 1.298  loss_ce_6: 0.5148  loss_mask_6: 0.1786  loss_dice_6: 1.288  loss_ce_7: 0.5115  loss_mask_7: 0.1776  loss_dice_7: 1.241  loss_ce_8: 0.4893  loss_mask_8: 0.1768  loss_dice_8: 1.308    time: 1.0476  last_time: 1.0553  data_time: 0.0726  last_data_time: 0.0752   lr: 0.0001  max_mem: 32638M
[10/19 11:39:41] d2.utils.events INFO:  eta: 0:43:48  iter: 479  total_loss: 20.64  loss_ce: 0.5113  loss_mask: 0.1836  loss_dice: 1.247  loss_ce_0: 0.6032  loss_mask_0: 0.2126  loss_dice_0: 1.523  loss_ce_1: 0.6974  loss_mask_1: 0.1917  loss_dice_1: 1.388  loss_ce_2: 0.632  loss_mask_2: 0.1865  loss_dice_2: 1.334  loss_ce_3: 0.5518  loss_mask_3: 0.1914  loss_dice_3: 1.28  loss_ce_4: 0.5472  loss_mask_4: 0.1881  loss_dice_4: 1.29  loss_ce_5: 0.485  loss_mask_5: 0.1899  loss_dice_5: 1.273  loss_ce_6: 0.5078  loss_mask_6: 0.1922  loss_dice_6: 1.279  loss_ce_7: 0.47  loss_mask_7: 0.1887  loss_dice_7: 1.263  loss_ce_8: 0.4729  loss_mask_8: 0.1841  loss_dice_8: 1.269    time: 1.0480  last_time: 1.0050  data_time: 0.0667  last_data_time: 0.0538   lr: 0.0001  max_mem: 32638M
[10/19 11:40:02] d2.utils.events INFO:  eta: 0:43:31  iter: 499  total_loss: 21.74  loss_ce: 0.5302  loss_mask: 0.1757  loss_dice: 1.29  loss_ce_0: 0.5836  loss_mask_0: 0.2011  loss_dice_0: 1.522  loss_ce_1: 0.7109  loss_mask_1: 0.1893  loss_dice_1: 1.41  loss_ce_2: 0.6166  loss_mask_2: 0.1793  loss_dice_2: 1.406  loss_ce_3: 0.5618  loss_mask_3: 0.179  loss_dice_3: 1.325  loss_ce_4: 0.5869  loss_mask_4: 0.1754  loss_dice_4: 1.342  loss_ce_5: 0.5576  loss_mask_5: 0.1741  loss_dice_5: 1.309  loss_ce_6: 0.541  loss_mask_6: 0.1725  loss_dice_6: 1.32  loss_ce_7: 0.5354  loss_mask_7: 0.1702  loss_dice_7: 1.302  loss_ce_8: 0.5266  loss_mask_8: 0.1726  loss_dice_8: 1.295    time: 1.0486  last_time: 1.0615  data_time: 0.0756  last_data_time: 0.0800   lr: 0.0001  max_mem: 32638M
[10/19 11:40:23] d2.utils.events INFO:  eta: 0:43:11  iter: 519  total_loss: 21.21  loss_ce: 0.534  loss_mask: 0.1892  loss_dice: 1.279  loss_ce_0: 0.6202  loss_mask_0: 0.214  loss_dice_0: 1.502  loss_ce_1: 0.7347  loss_mask_1: 0.2023  loss_dice_1: 1.407  loss_ce_2: 0.6517  loss_mask_2: 0.193  loss_dice_2: 1.336  loss_ce_3: 0.5929  loss_mask_3: 0.1926  loss_dice_3: 1.307  loss_ce_4: 0.5686  loss_mask_4: 0.194  loss_dice_4: 1.279  loss_ce_5: 0.5539  loss_mask_5: 0.1866  loss_dice_5: 1.279  loss_ce_6: 0.5326  loss_mask_6: 0.1895  loss_dice_6: 1.293  loss_ce_7: 0.5481  loss_mask_7: 0.1847  loss_dice_7: 1.272  loss_ce_8: 0.5348  loss_mask_8: 0.1846  loss_dice_8: 1.265    time: 1.0486  last_time: 1.0474  data_time: 0.0689  last_data_time: 0.0733   lr: 0.0001  max_mem: 32638M
[10/19 11:40:44] d2.utils.events INFO:  eta: 0:42:53  iter: 539  total_loss: 21.67  loss_ce: 0.5599  loss_mask: 0.1869  loss_dice: 1.278  loss_ce_0: 0.6126  loss_mask_0: 0.2147  loss_dice_0: 1.505  loss_ce_1: 0.7107  loss_mask_1: 0.1963  loss_dice_1: 1.428  loss_ce_2: 0.6574  loss_mask_2: 0.193  loss_dice_2: 1.36  loss_ce_3: 0.6011  loss_mask_3: 0.1872  loss_dice_3: 1.326  loss_ce_4: 0.5641  loss_mask_4: 0.1917  loss_dice_4: 1.309  loss_ce_5: 0.5613  loss_mask_5: 0.1877  loss_dice_5: 1.363  loss_ce_6: 0.583  loss_mask_6: 0.1865  loss_dice_6: 1.27  loss_ce_7: 0.5694  loss_mask_7: 0.1886  loss_dice_7: 1.265  loss_ce_8: 0.5786  loss_mask_8: 0.1865  loss_dice_8: 1.278    time: 1.0492  last_time: 1.0972  data_time: 0.0722  last_data_time: 0.0886   lr: 0.0001  max_mem: 32638M
[10/19 11:41:06] d2.utils.events INFO:  eta: 0:42:33  iter: 559  total_loss: 20.31  loss_ce: 0.5071  loss_mask: 0.1773  loss_dice: 1.227  loss_ce_0: 0.577  loss_mask_0: 0.2156  loss_dice_0: 1.46  loss_ce_1: 0.6668  loss_mask_1: 0.193  loss_dice_1: 1.385  loss_ce_2: 0.6134  loss_mask_2: 0.1896  loss_dice_2: 1.294  loss_ce_3: 0.5468  loss_mask_3: 0.1813  loss_dice_3: 1.268  loss_ce_4: 0.5358  loss_mask_4: 0.178  loss_dice_4: 1.251  loss_ce_5: 0.5169  loss_mask_5: 0.1808  loss_dice_5: 1.254  loss_ce_6: 0.5135  loss_mask_6: 0.1789  loss_dice_6: 1.221  loss_ce_7: 0.5153  loss_mask_7: 0.1779  loss_dice_7: 1.247  loss_ce_8: 0.512  loss_mask_8: 0.1781  loss_dice_8: 1.236    time: 1.0493  last_time: 1.0904  data_time: 0.0678  last_data_time: 0.0828   lr: 0.0001  max_mem: 32638M
[10/19 11:41:27] d2.utils.events INFO:  eta: 0:42:12  iter: 579  total_loss: 19.66  loss_ce: 0.5115  loss_mask: 0.1793  loss_dice: 1.182  loss_ce_0: 0.6048  loss_mask_0: 0.2024  loss_dice_0: 1.436  loss_ce_1: 0.6628  loss_mask_1: 0.19  loss_dice_1: 1.295  loss_ce_2: 0.6313  loss_mask_2: 0.185  loss_dice_2: 1.27  loss_ce_3: 0.5329  loss_mask_3: 0.1852  loss_dice_3: 1.221  loss_ce_4: 0.545  loss_mask_4: 0.1841  loss_dice_4: 1.226  loss_ce_5: 0.5123  loss_mask_5: 0.1805  loss_dice_5: 1.234  loss_ce_6: 0.4752  loss_mask_6: 0.183  loss_dice_6: 1.223  loss_ce_7: 0.4947  loss_mask_7: 0.1794  loss_dice_7: 1.219  loss_ce_8: 0.5133  loss_mask_8: 0.1781  loss_dice_8: 1.204    time: 1.0495  last_time: 1.0540  data_time: 0.0704  last_data_time: 0.0724   lr: 0.0001  max_mem: 32638M
[10/19 11:41:48] d2.utils.events INFO:  eta: 0:41:51  iter: 599  total_loss: 20.08  loss_ce: 0.5025  loss_mask: 0.1839  loss_dice: 1.223  loss_ce_0: 0.5743  loss_mask_0: 0.2132  loss_dice_0: 1.478  loss_ce_1: 0.6602  loss_mask_1: 0.1911  loss_dice_1: 1.343  loss_ce_2: 0.6155  loss_mask_2: 0.1889  loss_dice_2: 1.305  loss_ce_3: 0.539  loss_mask_3: 0.1848  loss_dice_3: 1.23  loss_ce_4: 0.5401  loss_mask_4: 0.1841  loss_dice_4: 1.243  loss_ce_5: 0.5354  loss_mask_5: 0.1847  loss_dice_5: 1.239  loss_ce_6: 0.5139  loss_mask_6: 0.181  loss_dice_6: 1.198  loss_ce_7: 0.4863  loss_mask_7: 0.1805  loss_dice_7: 1.231  loss_ce_8: 0.5216  loss_mask_8: 0.1824  loss_dice_8: 1.206    time: 1.0491  last_time: 1.0495  data_time: 0.0697  last_data_time: 0.0825   lr: 0.0001  max_mem: 32638M
[10/19 11:42:08] d2.utils.events INFO:  eta: 0:41:30  iter: 619  total_loss: 20.22  loss_ce: 0.5134  loss_mask: 0.1777  loss_dice: 1.231  loss_ce_0: 0.5958  loss_mask_0: 0.2035  loss_dice_0: 1.517  loss_ce_1: 0.6942  loss_mask_1: 0.1897  loss_dice_1: 1.36  loss_ce_2: 0.6284  loss_mask_2: 0.1871  loss_dice_2: 1.285  loss_ce_3: 0.5631  loss_mask_3: 0.1853  loss_dice_3: 1.207  loss_ce_4: 0.5065  loss_mask_4: 0.178  loss_dice_4: 1.249  loss_ce_5: 0.5007  loss_mask_5: 0.1785  loss_dice_5: 1.241  loss_ce_6: 0.5049  loss_mask_6: 0.1788  loss_dice_6: 1.218  loss_ce_7: 0.4953  loss_mask_7: 0.1797  loss_dice_7: 1.247  loss_ce_8: 0.5129  loss_mask_8: 0.1785  loss_dice_8: 1.238    time: 1.0490  last_time: 1.0942  data_time: 0.0675  last_data_time: 0.0763   lr: 0.0001  max_mem: 32638M
[10/19 11:42:30] d2.utils.events INFO:  eta: 0:41:10  iter: 639  total_loss: 20.27  loss_ce: 0.4814  loss_mask: 0.1836  loss_dice: 1.25  loss_ce_0: 0.5681  loss_mask_0: 0.21  loss_dice_0: 1.5  loss_ce_1: 0.6565  loss_mask_1: 0.1957  loss_dice_1: 1.387  loss_ce_2: 0.6223  loss_mask_2: 0.1859  loss_dice_2: 1.335  loss_ce_3: 0.5668  loss_mask_3: 0.1838  loss_dice_3: 1.258  loss_ce_4: 0.5226  loss_mask_4: 0.1829  loss_dice_4: 1.275  loss_ce_5: 0.5068  loss_mask_5: 0.1825  loss_dice_5: 1.293  loss_ce_6: 0.4919  loss_mask_6: 0.1818  loss_dice_6: 1.258  loss_ce_7: 0.4757  loss_mask_7: 0.1847  loss_dice_7: 1.272  loss_ce_8: 0.5009  loss_mask_8: 0.1809  loss_dice_8: 1.251    time: 1.0492  last_time: 1.1176  data_time: 0.0681  last_data_time: 0.0712   lr: 0.0001  max_mem: 32638M
[10/19 11:42:51] d2.utils.events INFO:  eta: 0:40:51  iter: 659  total_loss: 20.74  loss_ce: 0.5511  loss_mask: 0.1618  loss_dice: 1.241  loss_ce_0: 0.6307  loss_mask_0: 0.1898  loss_dice_0: 1.524  loss_ce_1: 0.7078  loss_mask_1: 0.1758  loss_dice_1: 1.45  loss_ce_2: 0.6807  loss_mask_2: 0.1674  loss_dice_2: 1.348  loss_ce_3: 0.5696  loss_mask_3: 0.1639  loss_dice_3: 1.321  loss_ce_4: 0.5453  loss_mask_4: 0.1615  loss_dice_4: 1.297  loss_ce_5: 0.5185  loss_mask_5: 0.1608  loss_dice_5: 1.324  loss_ce_6: 0.57  loss_mask_6: 0.1611  loss_dice_6: 1.264  loss_ce_7: 0.5262  loss_mask_7: 0.1622  loss_dice_7: 1.287  loss_ce_8: 0.5308  loss_mask_8: 0.1611  loss_dice_8: 1.277    time: 1.0496  last_time: 1.1369  data_time: 0.0754  last_data_time: 0.0652   lr: 0.0001  max_mem: 32638M
[10/19 11:43:12] d2.utils.events INFO:  eta: 0:40:30  iter: 679  total_loss: 20.99  loss_ce: 0.4808  loss_mask: 0.1849  loss_dice: 1.304  loss_ce_0: 0.5567  loss_mask_0: 0.2106  loss_dice_0: 1.588  loss_ce_1: 0.6025  loss_mask_1: 0.2004  loss_dice_1: 1.454  loss_ce_2: 0.611  loss_mask_2: 0.1933  loss_dice_2: 1.35  loss_ce_3: 0.5304  loss_mask_3: 0.1916  loss_dice_3: 1.352  loss_ce_4: 0.5048  loss_mask_4: 0.1937  loss_dice_4: 1.345  loss_ce_5: 0.5106  loss_mask_5: 0.1872  loss_dice_5: 1.311  loss_ce_6: 0.5129  loss_mask_6: 0.19  loss_dice_6: 1.315  loss_ce_7: 0.484  loss_mask_7: 0.1883  loss_dice_7: 1.293  loss_ce_8: 0.4832  loss_mask_8: 0.1866  loss_dice_8: 1.324    time: 1.0497  last_time: 1.0647  data_time: 0.0723  last_data_time: 0.0729   lr: 0.0001  max_mem: 32638M
[10/19 11:43:33] d2.utils.events INFO:  eta: 0:40:09  iter: 699  total_loss: 20.97  loss_ce: 0.5118  loss_mask: 0.1859  loss_dice: 1.254  loss_ce_0: 0.5552  loss_mask_0: 0.2154  loss_dice_0: 1.497  loss_ce_1: 0.6529  loss_mask_1: 0.1971  loss_dice_1: 1.386  loss_ce_2: 0.6068  loss_mask_2: 0.187  loss_dice_2: 1.326  loss_ce_3: 0.5576  loss_mask_3: 0.1899  loss_dice_3: 1.298  loss_ce_4: 0.5504  loss_mask_4: 0.1864  loss_dice_4: 1.309  loss_ce_5: 0.5317  loss_mask_5: 0.1879  loss_dice_5: 1.262  loss_ce_6: 0.513  loss_mask_6: 0.186  loss_dice_6: 1.268  loss_ce_7: 0.5382  loss_mask_7: 0.1837  loss_dice_7: 1.252  loss_ce_8: 0.4929  loss_mask_8: 0.1858  loss_dice_8: 1.233    time: 1.0495  last_time: 1.0535  data_time: 0.0712  last_data_time: 0.0864   lr: 0.0001  max_mem: 32638M
[10/19 11:43:54] d2.utils.events INFO:  eta: 0:39:48  iter: 719  total_loss: 21.13  loss_ce: 0.5646  loss_mask: 0.1795  loss_dice: 1.295  loss_ce_0: 0.6208  loss_mask_0: 0.1984  loss_dice_0: 1.546  loss_ce_1: 0.7032  loss_mask_1: 0.1823  loss_dice_1: 1.433  loss_ce_2: 0.6494  loss_mask_2: 0.1803  loss_dice_2: 1.356  loss_ce_3: 0.5801  loss_mask_3: 0.1802  loss_dice_3: 1.359  loss_ce_4: 0.5197  loss_mask_4: 0.1814  loss_dice_4: 1.331  loss_ce_5: 0.5595  loss_mask_5: 0.1816  loss_dice_5: 1.306  loss_ce_6: 0.58  loss_mask_6: 0.1788  loss_dice_6: 1.292  loss_ce_7: 0.5754  loss_mask_7: 0.181  loss_dice_7: 1.286  loss_ce_8: 0.5629  loss_mask_8: 0.1784  loss_dice_8: 1.262    time: 1.0494  last_time: 1.0134  data_time: 0.0681  last_data_time: 0.0731   lr: 0.0001  max_mem: 32638M
[10/19 11:44:15] d2.utils.events INFO:  eta: 0:39:27  iter: 739  total_loss: 20.09  loss_ce: 0.4925  loss_mask: 0.1829  loss_dice: 1.203  loss_ce_0: 0.5755  loss_mask_0: 0.2015  loss_dice_0: 1.431  loss_ce_1: 0.6701  loss_mask_1: 0.1902  loss_dice_1: 1.324  loss_ce_2: 0.6039  loss_mask_2: 0.1901  loss_dice_2: 1.262  loss_ce_3: 0.5231  loss_mask_3: 0.1835  loss_dice_3: 1.251  loss_ce_4: 0.508  loss_mask_4: 0.1845  loss_dice_4: 1.244  loss_ce_5: 0.5125  loss_mask_5: 0.1833  loss_dice_5: 1.249  loss_ce_6: 0.5046  loss_mask_6: 0.1823  loss_dice_6: 1.25  loss_ce_7: 0.503  loss_mask_7: 0.1844  loss_dice_7: 1.226  loss_ce_8: 0.5159  loss_mask_8: 0.1815  loss_dice_8: 1.203    time: 1.0494  last_time: 1.0386  data_time: 0.0703  last_data_time: 0.0726   lr: 0.0001  max_mem: 32638M
[10/19 11:44:36] d2.utils.events INFO:  eta: 0:39:06  iter: 759  total_loss: 19.69  loss_ce: 0.4926  loss_mask: 0.1639  loss_dice: 1.188  loss_ce_0: 0.5499  loss_mask_0: 0.1898  loss_dice_0: 1.45  loss_ce_1: 0.6552  loss_mask_1: 0.1727  loss_dice_1: 1.317  loss_ce_2: 0.6038  loss_mask_2: 0.1697  loss_dice_2: 1.27  loss_ce_3: 0.5463  loss_mask_3: 0.1647  loss_dice_3: 1.197  loss_ce_4: 0.5315  loss_mask_4: 0.1683  loss_dice_4: 1.205  loss_ce_5: 0.4924  loss_mask_5: 0.1672  loss_dice_5: 1.207  loss_ce_6: 0.4705  loss_mask_6: 0.1625  loss_dice_6: 1.191  loss_ce_7: 0.4886  loss_mask_7: 0.1629  loss_dice_7: 1.182  loss_ce_8: 0.4756  loss_mask_8: 0.163  loss_dice_8: 1.207    time: 1.0494  last_time: 1.0427  data_time: 0.0675  last_data_time: 0.0735   lr: 0.0001  max_mem: 32638M
[10/19 11:44:57] d2.utils.events INFO:  eta: 0:38:45  iter: 779  total_loss: 19.51  loss_ce: 0.4903  loss_mask: 0.1689  loss_dice: 1.217  loss_ce_0: 0.6149  loss_mask_0: 0.1958  loss_dice_0: 1.413  loss_ce_1: 0.6806  loss_mask_1: 0.18  loss_dice_1: 1.319  loss_ce_2: 0.592  loss_mask_2: 0.1744  loss_dice_2: 1.275  loss_ce_3: 0.5165  loss_mask_3: 0.171  loss_dice_3: 1.24  loss_ce_4: 0.5017  loss_mask_4: 0.1696  loss_dice_4: 1.224  loss_ce_5: 0.5001  loss_mask_5: 0.1694  loss_dice_5: 1.184  loss_ce_6: 0.4994  loss_mask_6: 0.1693  loss_dice_6: 1.174  loss_ce_7: 0.4612  loss_mask_7: 0.1668  loss_dice_7: 1.187  loss_ce_8: 0.4782  loss_mask_8: 0.1663  loss_dice_8: 1.2    time: 1.0496  last_time: 1.0332  data_time: 0.0708  last_data_time: 0.0906   lr: 0.0001  max_mem: 32638M
[10/19 11:45:18] d2.utils.events INFO:  eta: 0:38:24  iter: 799  total_loss: 19.75  loss_ce: 0.4993  loss_mask: 0.1774  loss_dice: 1.227  loss_ce_0: 0.5799  loss_mask_0: 0.1981  loss_dice_0: 1.449  loss_ce_1: 0.6135  loss_mask_1: 0.1916  loss_dice_1: 1.373  loss_ce_2: 0.5816  loss_mask_2: 0.1874  loss_dice_2: 1.302  loss_ce_3: 0.5259  loss_mask_3: 0.1798  loss_dice_3: 1.236  loss_ce_4: 0.4888  loss_mask_4: 0.1777  loss_dice_4: 1.241  loss_ce_5: 0.5143  loss_mask_5: 0.1771  loss_dice_5: 1.251  loss_ce_6: 0.4878  loss_mask_6: 0.1773  loss_dice_6: 1.21  loss_ce_7: 0.4472  loss_mask_7: 0.1781  loss_dice_7: 1.179  loss_ce_8: 0.4815  loss_mask_8: 0.1763  loss_dice_8: 1.202    time: 1.0494  last_time: 1.1482  data_time: 0.0672  last_data_time: 0.0490   lr: 0.0001  max_mem: 32638M
[10/19 11:45:39] d2.utils.events INFO:  eta: 0:38:01  iter: 819  total_loss: 19.57  loss_ce: 0.5042  loss_mask: 0.1701  loss_dice: 1.194  loss_ce_0: 0.5724  loss_mask_0: 0.1892  loss_dice_0: 1.434  loss_ce_1: 0.6518  loss_mask_1: 0.1788  loss_dice_1: 1.338  loss_ce_2: 0.6126  loss_mask_2: 0.1687  loss_dice_2: 1.249  loss_ce_3: 0.5348  loss_mask_3: 0.1689  loss_dice_3: 1.172  loss_ce_4: 0.5251  loss_mask_4: 0.1666  loss_dice_4: 1.222  loss_ce_5: 0.509  loss_mask_5: 0.1696  loss_dice_5: 1.233  loss_ce_6: 0.5129  loss_mask_6: 0.1711  loss_dice_6: 1.191  loss_ce_7: 0.5138  loss_mask_7: 0.171  loss_dice_7: 1.183  loss_ce_8: 0.4877  loss_mask_8: 0.169  loss_dice_8: 1.169    time: 1.0491  last_time: 1.0453  data_time: 0.0692  last_data_time: 0.0715   lr: 0.0001  max_mem: 32638M
[10/19 11:46:00] d2.utils.events INFO:  eta: 0:37:40  iter: 839  total_loss: 20.47  loss_ce: 0.5419  loss_mask: 0.1802  loss_dice: 1.254  loss_ce_0: 0.6241  loss_mask_0: 0.2108  loss_dice_0: 1.507  loss_ce_1: 0.6705  loss_mask_1: 0.1971  loss_dice_1: 1.417  loss_ce_2: 0.6504  loss_mask_2: 0.183  loss_dice_2: 1.335  loss_ce_3: 0.6128  loss_mask_3: 0.1797  loss_dice_3: 1.28  loss_ce_4: 0.5644  loss_mask_4: 0.1824  loss_dice_4: 1.288  loss_ce_5: 0.5489  loss_mask_5: 0.1746  loss_dice_5: 1.261  loss_ce_6: 0.5692  loss_mask_6: 0.1784  loss_dice_6: 1.242  loss_ce_7: 0.5137  loss_mask_7: 0.1786  loss_dice_7: 1.216  loss_ce_8: 0.5314  loss_mask_8: 0.1758  loss_dice_8: 1.206    time: 1.0492  last_time: 1.0222  data_time: 0.0715  last_data_time: 0.0616   lr: 0.0001  max_mem: 32638M
[10/19 11:46:21] d2.utils.events INFO:  eta: 0:37:19  iter: 859  total_loss: 20.04  loss_ce: 0.47  loss_mask: 0.1806  loss_dice: 1.266  loss_ce_0: 0.5785  loss_mask_0: 0.2165  loss_dice_0: 1.496  loss_ce_1: 0.6008  loss_mask_1: 0.1927  loss_dice_1: 1.375  loss_ce_2: 0.5789  loss_mask_2: 0.1817  loss_dice_2: 1.314  loss_ce_3: 0.5106  loss_mask_3: 0.1826  loss_dice_3: 1.301  loss_ce_4: 0.4933  loss_mask_4: 0.1774  loss_dice_4: 1.278  loss_ce_5: 0.4841  loss_mask_5: 0.1774  loss_dice_5: 1.289  loss_ce_6: 0.4718  loss_mask_6: 0.1816  loss_dice_6: 1.23  loss_ce_7: 0.4717  loss_mask_7: 0.1811  loss_dice_7: 1.248  loss_ce_8: 0.5084  loss_mask_8: 0.1812  loss_dice_8: 1.254    time: 1.0491  last_time: 1.0465  data_time: 0.0680  last_data_time: 0.0569   lr: 0.0001  max_mem: 32638M
[10/19 11:46:42] d2.utils.events INFO:  eta: 0:36:58  iter: 879  total_loss: 19.49  loss_ce: 0.4909  loss_mask: 0.1713  loss_dice: 1.209  loss_ce_0: 0.5626  loss_mask_0: 0.1922  loss_dice_0: 1.404  loss_ce_1: 0.5899  loss_mask_1: 0.1806  loss_dice_1: 1.294  loss_ce_2: 0.5751  loss_mask_2: 0.174  loss_dice_2: 1.288  loss_ce_3: 0.4956  loss_mask_3: 0.1741  loss_dice_3: 1.207  loss_ce_4: 0.4755  loss_mask_4: 0.17  loss_dice_4: 1.199  loss_ce_5: 0.4672  loss_mask_5: 0.1716  loss_dice_5: 1.22  loss_ce_6: 0.4911  loss_mask_6: 0.1726  loss_dice_6: 1.215  loss_ce_7: 0.4738  loss_mask_7: 0.1698  loss_dice_7: 1.201  loss_ce_8: 0.4859  loss_mask_8: 0.1729  loss_dice_8: 1.194    time: 1.0489  last_time: 1.0199  data_time: 0.0655  last_data_time: 0.0692   lr: 0.0001  max_mem: 32638M
[10/19 11:47:03] d2.utils.events INFO:  eta: 0:36:39  iter: 899  total_loss: 21.14  loss_ce: 0.5084  loss_mask: 0.1611  loss_dice: 1.291  loss_ce_0: 0.6261  loss_mask_0: 0.1866  loss_dice_0: 1.535  loss_ce_1: 0.6917  loss_mask_1: 0.1806  loss_dice_1: 1.47  loss_ce_2: 0.6038  loss_mask_2: 0.1719  loss_dice_2: 1.376  loss_ce_3: 0.5636  loss_mask_3: 0.161  loss_dice_3: 1.294  loss_ce_4: 0.5327  loss_mask_4: 0.161  loss_dice_4: 1.3  loss_ce_5: 0.5336  loss_mask_5: 0.1628  loss_dice_5: 1.316  loss_ce_6: 0.4977  loss_mask_6: 0.1629  loss_dice_6: 1.334  loss_ce_7: 0.499  loss_mask_7: 0.1614  loss_dice_7: 1.287  loss_ce_8: 0.496  loss_mask_8: 0.1616  loss_dice_8: 1.307    time: 1.0491  last_time: 1.1086  data_time: 0.0691  last_data_time: 0.0816   lr: 0.0001  max_mem: 32638M
[10/19 11:47:24] d2.utils.events INFO:  eta: 0:36:17  iter: 919  total_loss: 19.84  loss_ce: 0.4736  loss_mask: 0.1801  loss_dice: 1.234  loss_ce_0: 0.5847  loss_mask_0: 0.2026  loss_dice_0: 1.419  loss_ce_1: 0.6332  loss_mask_1: 0.1913  loss_dice_1: 1.352  loss_ce_2: 0.5771  loss_mask_2: 0.1843  loss_dice_2: 1.306  loss_ce_3: 0.5101  loss_mask_3: 0.1793  loss_dice_3: 1.236  loss_ce_4: 0.497  loss_mask_4: 0.1764  loss_dice_4: 1.243  loss_ce_5: 0.4845  loss_mask_5: 0.1782  loss_dice_5: 1.257  loss_ce_6: 0.4789  loss_mask_6: 0.1806  loss_dice_6: 1.214  loss_ce_7: 0.4656  loss_mask_7: 0.1794  loss_dice_7: 1.243  loss_ce_8: 0.4718  loss_mask_8: 0.1784  loss_dice_8: 1.211    time: 1.0490  last_time: 1.0442  data_time: 0.0666  last_data_time: 0.0590   lr: 0.0001  max_mem: 32638M
[10/19 11:47:46] d2.utils.events INFO:  eta: 0:35:57  iter: 939  total_loss: 20.39  loss_ce: 0.4989  loss_mask: 0.1713  loss_dice: 1.259  loss_ce_0: 0.6123  loss_mask_0: 0.1968  loss_dice_0: 1.529  loss_ce_1: 0.659  loss_mask_1: 0.1846  loss_dice_1: 1.42  loss_ce_2: 0.6196  loss_mask_2: 0.1761  loss_dice_2: 1.372  loss_ce_3: 0.5615  loss_mask_3: 0.1726  loss_dice_3: 1.285  loss_ce_4: 0.5071  loss_mask_4: 0.173  loss_dice_4: 1.274  loss_ce_5: 0.5018  loss_mask_5: 0.1716  loss_dice_5: 1.289  loss_ce_6: 0.5288  loss_mask_6: 0.1713  loss_dice_6: 1.234  loss_ce_7: 0.4953  loss_mask_7: 0.1731  loss_dice_7: 1.294  loss_ce_8: 0.5033  loss_mask_8: 0.1695  loss_dice_8: 1.302    time: 1.0496  last_time: 1.0554  data_time: 0.0762  last_data_time: 0.0998   lr: 0.0001  max_mem: 32638M
[10/19 11:48:07] d2.utils.events INFO:  eta: 0:35:36  iter: 959  total_loss: 20.25  loss_ce: 0.4738  loss_mask: 0.1763  loss_dice: 1.279  loss_ce_0: 0.591  loss_mask_0: 0.2056  loss_dice_0: 1.416  loss_ce_1: 0.6637  loss_mask_1: 0.1847  loss_dice_1: 1.374  loss_ce_2: 0.5994  loss_mask_2: 0.1782  loss_dice_2: 1.338  loss_ce_3: 0.5004  loss_mask_3: 0.1773  loss_dice_3: 1.301  loss_ce_4: 0.5032  loss_mask_4: 0.1723  loss_dice_4: 1.265  loss_ce_5: 0.4988  loss_mask_5: 0.1785  loss_dice_5: 1.293  loss_ce_6: 0.4442  loss_mask_6: 0.1773  loss_dice_6: 1.263  loss_ce_7: 0.4865  loss_mask_7: 0.1761  loss_dice_7: 1.247  loss_ce_8: 0.4845  loss_mask_8: 0.175  loss_dice_8: 1.254    time: 1.0497  last_time: 1.0688  data_time: 0.0755  last_data_time: 0.0665   lr: 0.0001  max_mem: 32638M
[10/19 11:48:28] d2.utils.events INFO:  eta: 0:35:16  iter: 979  total_loss: 20.11  loss_ce: 0.466  loss_mask: 0.1741  loss_dice: 1.247  loss_ce_0: 0.5407  loss_mask_0: 0.1901  loss_dice_0: 1.451  loss_ce_1: 0.6606  loss_mask_1: 0.1806  loss_dice_1: 1.409  loss_ce_2: 0.5984  loss_mask_2: 0.1685  loss_dice_2: 1.347  loss_ce_3: 0.5122  loss_mask_3: 0.1738  loss_dice_3: 1.291  loss_ce_4: 0.4867  loss_mask_4: 0.1734  loss_dice_4: 1.28  loss_ce_5: 0.4935  loss_mask_5: 0.1738  loss_dice_5: 1.253  loss_ce_6: 0.4509  loss_mask_6: 0.1762  loss_dice_6: 1.266  loss_ce_7: 0.4795  loss_mask_7: 0.1734  loss_dice_7: 1.235  loss_ce_8: 0.4679  loss_mask_8: 0.1734  loss_dice_8: 1.211    time: 1.0499  last_time: 1.0069  data_time: 0.0710  last_data_time: 0.0723   lr: 0.0001  max_mem: 32638M
[10/19 11:48:50] d2.utils.events INFO:  eta: 0:34:56  iter: 999  total_loss: 20.59  loss_ce: 0.5046  loss_mask: 0.1756  loss_dice: 1.357  loss_ce_0: 0.6118  loss_mask_0: 0.1987  loss_dice_0: 1.603  loss_ce_1: 0.683  loss_mask_1: 0.1831  loss_dice_1: 1.455  loss_ce_2: 0.6716  loss_mask_2: 0.1739  loss_dice_2: 1.378  loss_ce_3: 0.6181  loss_mask_3: 0.1766  loss_dice_3: 1.312  loss_ce_4: 0.5578  loss_mask_4: 0.1702  loss_dice_4: 1.33  loss_ce_5: 0.5622  loss_mask_5: 0.174  loss_dice_5: 1.359  loss_ce_6: 0.5335  loss_mask_6: 0.1739  loss_dice_6: 1.341  loss_ce_7: 0.5472  loss_mask_7: 0.1716  loss_dice_7: 1.287  loss_ce_8: 0.5297  loss_mask_8: 0.1714  loss_dice_8: 1.286    time: 1.0503  last_time: 1.0683  data_time: 0.0739  last_data_time: 0.0769   lr: 0.0001  max_mem: 32638M
[10/19 11:49:11] d2.utils.events INFO:  eta: 0:34:36  iter: 1019  total_loss: 20.81  loss_ce: 0.4998  loss_mask: 0.1727  loss_dice: 1.231  loss_ce_0: 0.6088  loss_mask_0: 0.2006  loss_dice_0: 1.492  loss_ce_1: 0.6551  loss_mask_1: 0.1905  loss_dice_1: 1.373  loss_ce_2: 0.6353  loss_mask_2: 0.1846  loss_dice_2: 1.336  loss_ce_3: 0.537  loss_mask_3: 0.175  loss_dice_3: 1.272  loss_ce_4: 0.5432  loss_mask_4: 0.1741  loss_dice_4: 1.281  loss_ce_5: 0.538  loss_mask_5: 0.1768  loss_dice_5: 1.255  loss_ce_6: 0.5169  loss_mask_6: 0.1764  loss_dice_6: 1.249  loss_ce_7: 0.5106  loss_mask_7: 0.1726  loss_dice_7: 1.252  loss_ce_8: 0.5085  loss_mask_8: 0.1705  loss_dice_8: 1.241    time: 1.0506  last_time: 1.0496  data_time: 0.0731  last_data_time: 0.0760   lr: 0.0001  max_mem: 32638M
[10/19 11:49:32] d2.utils.events INFO:  eta: 0:34:16  iter: 1039  total_loss: 20.37  loss_ce: 0.4668  loss_mask: 0.1664  loss_dice: 1.275  loss_ce_0: 0.5855  loss_mask_0: 0.1961  loss_dice_0: 1.499  loss_ce_1: 0.6569  loss_mask_1: 0.1814  loss_dice_1: 1.456  loss_ce_2: 0.5934  loss_mask_2: 0.1717  loss_dice_2: 1.372  loss_ce_3: 0.5164  loss_mask_3: 0.1656  loss_dice_3: 1.3  loss_ce_4: 0.5055  loss_mask_4: 0.1636  loss_dice_4: 1.316  loss_ce_5: 0.4807  loss_mask_5: 0.1646  loss_dice_5: 1.282  loss_ce_6: 0.4682  loss_mask_6: 0.1647  loss_dice_6: 1.253  loss_ce_7: 0.4681  loss_mask_7: 0.166  loss_dice_7: 1.278  loss_ce_8: 0.4941  loss_mask_8: 0.1628  loss_dice_8: 1.263    time: 1.0508  last_time: 1.0386  data_time: 0.0717  last_data_time: 0.0817   lr: 0.0001  max_mem: 32638M
[10/19 11:49:53] d2.utils.events INFO:  eta: 0:33:55  iter: 1059  total_loss: 20.28  loss_ce: 0.4574  loss_mask: 0.1617  loss_dice: 1.278  loss_ce_0: 0.5476  loss_mask_0: 0.1853  loss_dice_0: 1.504  loss_ce_1: 0.6437  loss_mask_1: 0.1709  loss_dice_1: 1.388  loss_ce_2: 0.5958  loss_mask_2: 0.1624  loss_dice_2: 1.35  loss_ce_3: 0.5251  loss_mask_3: 0.1611  loss_dice_3: 1.3  loss_ce_4: 0.5007  loss_mask_4: 0.1643  loss_dice_4: 1.328  loss_ce_5: 0.4916  loss_mask_5: 0.1631  loss_dice_5: 1.298  loss_ce_6: 0.4807  loss_mask_6: 0.1617  loss_dice_6: 1.253  loss_ce_7: 0.4902  loss_mask_7: 0.1616  loss_dice_7: 1.268  loss_ce_8: 0.4707  loss_mask_8: 0.167  loss_dice_8: 1.271    time: 1.0509  last_time: 1.0563  data_time: 0.0722  last_data_time: 0.0582   lr: 0.0001  max_mem: 32638M
[10/19 11:50:14] d2.utils.events INFO:  eta: 0:33:34  iter: 1079  total_loss: 19.47  loss_ce: 0.4889  loss_mask: 0.1751  loss_dice: 1.191  loss_ce_0: 0.5885  loss_mask_0: 0.208  loss_dice_0: 1.423  loss_ce_1: 0.6745  loss_mask_1: 0.1906  loss_dice_1: 1.331  loss_ce_2: 0.6015  loss_mask_2: 0.1784  loss_dice_2: 1.261  loss_ce_3: 0.5365  loss_mask_3: 0.1803  loss_dice_3: 1.197  loss_ce_4: 0.5085  loss_mask_4: 0.1782  loss_dice_4: 1.204  loss_ce_5: 0.489  loss_mask_5: 0.1785  loss_dice_5: 1.202  loss_ce_6: 0.4889  loss_mask_6: 0.1763  loss_dice_6: 1.178  loss_ce_7: 0.4822  loss_mask_7: 0.1769  loss_dice_7: 1.178  loss_ce_8: 0.4654  loss_mask_8: 0.1762  loss_dice_8: 1.192    time: 1.0509  last_time: 1.0418  data_time: 0.0714  last_data_time: 0.0686   lr: 0.0001  max_mem: 32638M
[10/19 11:50:36] d2.utils.events INFO:  eta: 0:33:13  iter: 1099  total_loss: 20.58  loss_ce: 0.5273  loss_mask: 0.1723  loss_dice: 1.254  loss_ce_0: 0.6194  loss_mask_0: 0.2013  loss_dice_0: 1.452  loss_ce_1: 0.6611  loss_mask_1: 0.1906  loss_dice_1: 1.38  loss_ce_2: 0.6001  loss_mask_2: 0.1807  loss_dice_2: 1.352  loss_ce_3: 0.5816  loss_mask_3: 0.1784  loss_dice_3: 1.276  loss_ce_4: 0.5244  loss_mask_4: 0.176  loss_dice_4: 1.294  loss_ce_5: 0.5205  loss_mask_5: 0.175  loss_dice_5: 1.27  loss_ce_6: 0.5168  loss_mask_6: 0.1727  loss_dice_6: 1.261  loss_ce_7: 0.5012  loss_mask_7: 0.1712  loss_dice_7: 1.291  loss_ce_8: 0.5473  loss_mask_8: 0.1723  loss_dice_8: 1.268    time: 1.0509  last_time: 1.0452  data_time: 0.0665  last_data_time: 0.0701   lr: 0.0001  max_mem: 32638M
[10/19 11:50:57] d2.utils.events INFO:  eta: 0:32:53  iter: 1119  total_loss: 19.63  loss_ce: 0.4742  loss_mask: 0.1785  loss_dice: 1.272  loss_ce_0: 0.5757  loss_mask_0: 0.2123  loss_dice_0: 1.423  loss_ce_1: 0.5992  loss_mask_1: 0.1989  loss_dice_1: 1.333  loss_ce_2: 0.5913  loss_mask_2: 0.1868  loss_dice_2: 1.291  loss_ce_3: 0.5257  loss_mask_3: 0.1838  loss_dice_3: 1.282  loss_ce_4: 0.4914  loss_mask_4: 0.1867  loss_dice_4: 1.255  loss_ce_5: 0.4772  loss_mask_5: 0.1827  loss_dice_5: 1.251  loss_ce_6: 0.4932  loss_mask_6: 0.1788  loss_dice_6: 1.226  loss_ce_7: 0.4999  loss_mask_7: 0.1803  loss_dice_7: 1.227  loss_ce_8: 0.4913  loss_mask_8: 0.1793  loss_dice_8: 1.202    time: 1.0511  last_time: 1.0279  data_time: 0.0744  last_data_time: 0.0700   lr: 0.0001  max_mem: 32638M
[10/19 11:51:18] d2.utils.events INFO:  eta: 0:32:31  iter: 1139  total_loss: 18.91  loss_ce: 0.4606  loss_mask: 0.1714  loss_dice: 1.159  loss_ce_0: 0.5477  loss_mask_0: 0.1952  loss_dice_0: 1.374  loss_ce_1: 0.6154  loss_mask_1: 0.1813  loss_dice_1: 1.325  loss_ce_2: 0.5375  loss_mask_2: 0.1754  loss_dice_2: 1.211  loss_ce_3: 0.4839  loss_mask_3: 0.1709  loss_dice_3: 1.201  loss_ce_4: 0.4612  loss_mask_4: 0.172  loss_dice_4: 1.18  loss_ce_5: 0.4431  loss_mask_5: 0.1713  loss_dice_5: 1.168  loss_ce_6: 0.449  loss_mask_6: 0.1696  loss_dice_6: 1.142  loss_ce_7: 0.4368  loss_mask_7: 0.1707  loss_dice_7: 1.157  loss_ce_8: 0.4262  loss_mask_8: 0.1718  loss_dice_8: 1.178    time: 1.0510  last_time: 1.0172  data_time: 0.0694  last_data_time: 0.0766   lr: 0.0001  max_mem: 32638M
[10/19 11:51:39] d2.utils.events INFO:  eta: 0:32:11  iter: 1159  total_loss: 21.06  loss_ce: 0.5024  loss_mask: 0.1521  loss_dice: 1.324  loss_ce_0: 0.5974  loss_mask_0: 0.1781  loss_dice_0: 1.527  loss_ce_1: 0.6554  loss_mask_1: 0.1623  loss_dice_1: 1.456  loss_ce_2: 0.6331  loss_mask_2: 0.1572  loss_dice_2: 1.39  loss_ce_3: 0.5718  loss_mask_3: 0.1549  loss_dice_3: 1.343  loss_ce_4: 0.5483  loss_mask_4: 0.1544  loss_dice_4: 1.352  loss_ce_5: 0.5285  loss_mask_5: 0.1555  loss_dice_5: 1.341  loss_ce_6: 0.5157  loss_mask_6: 0.1544  loss_dice_6: 1.333  loss_ce_7: 0.4763  loss_mask_7: 0.1537  loss_dice_7: 1.337  loss_ce_8: 0.4974  loss_mask_8: 0.1554  loss_dice_8: 1.342    time: 1.0513  last_time: 1.0606  data_time: 0.0770  last_data_time: 0.0670   lr: 0.0001  max_mem: 32638M
[10/19 11:52:00] d2.utils.events INFO:  eta: 0:31:50  iter: 1179  total_loss: 19.21  loss_ce: 0.4655  loss_mask: 0.1761  loss_dice: 1.182  loss_ce_0: 0.5661  loss_mask_0: 0.1986  loss_dice_0: 1.401  loss_ce_1: 0.6235  loss_mask_1: 0.183  loss_dice_1: 1.327  loss_ce_2: 0.5876  loss_mask_2: 0.1755  loss_dice_2: 1.245  loss_ce_3: 0.5189  loss_mask_3: 0.1773  loss_dice_3: 1.212  loss_ce_4: 0.477  loss_mask_4: 0.1783  loss_dice_4: 1.215  loss_ce_5: 0.4845  loss_mask_5: 0.1766  loss_dice_5: 1.209  loss_ce_6: 0.4868  loss_mask_6: 0.1755  loss_dice_6: 1.203  loss_ce_7: 0.4939  loss_mask_7: 0.1757  loss_dice_7: 1.202  loss_ce_8: 0.4599  loss_mask_8: 0.1766  loss_dice_8: 1.176    time: 1.0512  last_time: 1.0360  data_time: 0.0642  last_data_time: 0.0751   lr: 0.0001  max_mem: 32638M
[10/19 11:52:21] d2.utils.events INFO:  eta: 0:31:29  iter: 1199  total_loss: 19.3  loss_ce: 0.4735  loss_mask: 0.178  loss_dice: 1.247  loss_ce_0: 0.5535  loss_mask_0: 0.2147  loss_dice_0: 1.442  loss_ce_1: 0.6003  loss_mask_1: 0.1966  loss_dice_1: 1.319  loss_ce_2: 0.5983  loss_mask_2: 0.1825  loss_dice_2: 1.273  loss_ce_3: 0.5039  loss_mask_3: 0.1816  loss_dice_3: 1.268  loss_ce_4: 0.51  loss_mask_4: 0.179  loss_dice_4: 1.275  loss_ce_5: 0.4947  loss_mask_5: 0.1766  loss_dice_5: 1.237  loss_ce_6: 0.4609  loss_mask_6: 0.1789  loss_dice_6: 1.207  loss_ce_7: 0.4775  loss_mask_7: 0.1776  loss_dice_7: 1.21  loss_ce_8: 0.4719  loss_mask_8: 0.1757  loss_dice_8: 1.221    time: 1.0511  last_time: 1.0309  data_time: 0.0710  last_data_time: 0.0837   lr: 0.0001  max_mem: 32638M
[10/19 11:52:42] d2.utils.events INFO:  eta: 0:31:08  iter: 1219  total_loss: 20.4  loss_ce: 0.4395  loss_mask: 0.1642  loss_dice: 1.231  loss_ce_0: 0.5883  loss_mask_0: 0.1935  loss_dice_0: 1.464  loss_ce_1: 0.6755  loss_mask_1: 0.1809  loss_dice_1: 1.396  loss_ce_2: 0.6055  loss_mask_2: 0.1785  loss_dice_2: 1.297  loss_ce_3: 0.5496  loss_mask_3: 0.1797  loss_dice_3: 1.285  loss_ce_4: 0.5306  loss_mask_4: 0.1716  loss_dice_4: 1.253  loss_ce_5: 0.5589  loss_mask_5: 0.1677  loss_dice_5: 1.286  loss_ce_6: 0.4802  loss_mask_6: 0.1658  loss_dice_6: 1.244  loss_ce_7: 0.466  loss_mask_7: 0.1673  loss_dice_7: 1.236  loss_ce_8: 0.4948  loss_mask_8: 0.1658  loss_dice_8: 1.244    time: 1.0509  last_time: 1.1306  data_time: 0.0684  last_data_time: 0.0734   lr: 0.0001  max_mem: 32638M
[10/19 11:53:03] d2.utils.events INFO:  eta: 0:30:47  iter: 1239  total_loss: 20.67  loss_ce: 0.4653  loss_mask: 0.1687  loss_dice: 1.319  loss_ce_0: 0.6179  loss_mask_0: 0.2096  loss_dice_0: 1.484  loss_ce_1: 0.6411  loss_mask_1: 0.1868  loss_dice_1: 1.374  loss_ce_2: 0.5773  loss_mask_2: 0.1763  loss_dice_2: 1.352  loss_ce_3: 0.5428  loss_mask_3: 0.1726  loss_dice_3: 1.329  loss_ce_4: 0.5065  loss_mask_4: 0.1727  loss_dice_4: 1.333  loss_ce_5: 0.4757  loss_mask_5: 0.1728  loss_dice_5: 1.269  loss_ce_6: 0.4806  loss_mask_6: 0.1691  loss_dice_6: 1.284  loss_ce_7: 0.4486  loss_mask_7: 0.1709  loss_dice_7: 1.275  loss_ce_8: 0.4712  loss_mask_8: 0.171  loss_dice_8: 1.284    time: 1.0509  last_time: 1.1704  data_time: 0.0750  last_data_time: 0.1457   lr: 0.0001  max_mem: 32638M
[10/19 11:53:24] d2.utils.events INFO:  eta: 0:30:27  iter: 1259  total_loss: 18  loss_ce: 0.3964  loss_mask: 0.1608  loss_dice: 1.157  loss_ce_0: 0.5433  loss_mask_0: 0.194  loss_dice_0: 1.366  loss_ce_1: 0.5735  loss_mask_1: 0.1805  loss_dice_1: 1.272  loss_ce_2: 0.4971  loss_mask_2: 0.1698  loss_dice_2: 1.22  loss_ce_3: 0.45  loss_mask_3: 0.166  loss_dice_3: 1.196  loss_ce_4: 0.4274  loss_mask_4: 0.1629  loss_dice_4: 1.195  loss_ce_5: 0.4  loss_mask_5: 0.1617  loss_dice_5: 1.176  loss_ce_6: 0.3866  loss_mask_6: 0.1602  loss_dice_6: 1.205  loss_ce_7: 0.3968  loss_mask_7: 0.161  loss_dice_7: 1.149  loss_ce_8: 0.379  loss_mask_8: 0.1615  loss_dice_8: 1.168    time: 1.0510  last_time: 1.0310  data_time: 0.0705  last_data_time: 0.0515   lr: 0.0001  max_mem: 32638M
[10/19 11:53:45] d2.utils.events INFO:  eta: 0:30:05  iter: 1279  total_loss: 19.36  loss_ce: 0.4759  loss_mask: 0.1781  loss_dice: 1.211  loss_ce_0: 0.5733  loss_mask_0: 0.1955  loss_dice_0: 1.377  loss_ce_1: 0.6017  loss_mask_1: 0.1895  loss_dice_1: 1.328  loss_ce_2: 0.5358  loss_mask_2: 0.181  loss_dice_2: 1.283  loss_ce_3: 0.5118  loss_mask_3: 0.1812  loss_dice_3: 1.225  loss_ce_4: 0.5048  loss_mask_4: 0.1796  loss_dice_4: 1.213  loss_ce_5: 0.4798  loss_mask_5: 0.1773  loss_dice_5: 1.224  loss_ce_6: 0.4682  loss_mask_6: 0.1782  loss_dice_6: 1.201  loss_ce_7: 0.4467  loss_mask_7: 0.1782  loss_dice_7: 1.201  loss_ce_8: 0.4605  loss_mask_8: 0.1779  loss_dice_8: 1.219    time: 1.0510  last_time: 1.0395  data_time: 0.0689  last_data_time: 0.0731   lr: 0.0001  max_mem: 32638M
[10/19 11:54:06] d2.utils.events INFO:  eta: 0:29:44  iter: 1299  total_loss: 19.88  loss_ce: 0.4666  loss_mask: 0.1639  loss_dice: 1.211  loss_ce_0: 0.5809  loss_mask_0: 0.195  loss_dice_0: 1.398  loss_ce_1: 0.6656  loss_mask_1: 0.1807  loss_dice_1: 1.322  loss_ce_2: 0.5955  loss_mask_2: 0.1686  loss_dice_2: 1.308  loss_ce_3: 0.5271  loss_mask_3: 0.1657  loss_dice_3: 1.215  loss_ce_4: 0.5123  loss_mask_4: 0.1662  loss_dice_4: 1.255  loss_ce_5: 0.4939  loss_mask_5: 0.165  loss_dice_5: 1.257  loss_ce_6: 0.5052  loss_mask_6: 0.1643  loss_dice_6: 1.197  loss_ce_7: 0.4715  loss_mask_7: 0.1636  loss_dice_7: 1.193  loss_ce_8: 0.4784  loss_mask_8: 0.1674  loss_dice_8: 1.227    time: 1.0509  last_time: 1.0469  data_time: 0.0717  last_data_time: 0.0612   lr: 0.0001  max_mem: 32638M
[10/19 11:54:27] d2.utils.events INFO:  eta: 0:29:23  iter: 1319  total_loss: 19.67  loss_ce: 0.4922  loss_mask: 0.1603  loss_dice: 1.193  loss_ce_0: 0.5666  loss_mask_0: 0.1898  loss_dice_0: 1.424  loss_ce_1: 0.6373  loss_mask_1: 0.1717  loss_dice_1: 1.379  loss_ce_2: 0.5871  loss_mask_2: 0.1689  loss_dice_2: 1.292  loss_ce_3: 0.5316  loss_mask_3: 0.1696  loss_dice_3: 1.216  loss_ce_4: 0.4932  loss_mask_4: 0.1688  loss_dice_4: 1.242  loss_ce_5: 0.4865  loss_mask_5: 0.1677  loss_dice_5: 1.227  loss_ce_6: 0.48  loss_mask_6: 0.1642  loss_dice_6: 1.212  loss_ce_7: 0.473  loss_mask_7: 0.1628  loss_dice_7: 1.183  loss_ce_8: 0.4832  loss_mask_8: 0.1601  loss_dice_8: 1.223    time: 1.0509  last_time: 1.0333  data_time: 0.0711  last_data_time: 0.0650   lr: 0.0001  max_mem: 32638M
[10/19 11:54:48] d2.utils.events INFO:  eta: 0:29:01  iter: 1339  total_loss: 19.65  loss_ce: 0.4431  loss_mask: 0.1753  loss_dice: 1.232  loss_ce_0: 0.5295  loss_mask_0: 0.2025  loss_dice_0: 1.453  loss_ce_1: 0.6087  loss_mask_1: 0.1866  loss_dice_1: 1.364  loss_ce_2: 0.5704  loss_mask_2: 0.1879  loss_dice_2: 1.287  loss_ce_3: 0.5249  loss_mask_3: 0.1793  loss_dice_3: 1.228  loss_ce_4: 0.4964  loss_mask_4: 0.1796  loss_dice_4: 1.252  loss_ce_5: 0.4692  loss_mask_5: 0.1855  loss_dice_5: 1.243  loss_ce_6: 0.4682  loss_mask_6: 0.184  loss_dice_6: 1.181  loss_ce_7: 0.4474  loss_mask_7: 0.1799  loss_dice_7: 1.237  loss_ce_8: 0.455  loss_mask_8: 0.1745  loss_dice_8: 1.222    time: 1.0507  last_time: 1.0322  data_time: 0.0685  last_data_time: 0.0753   lr: 0.0001  max_mem: 32638M
[10/19 11:55:10] d2.utils.events INFO:  eta: 0:28:41  iter: 1359  total_loss: 19.26  loss_ce: 0.4513  loss_mask: 0.1668  loss_dice: 1.209  loss_ce_0: 0.569  loss_mask_0: 0.1998  loss_dice_0: 1.476  loss_ce_1: 0.5912  loss_mask_1: 0.1787  loss_dice_1: 1.332  loss_ce_2: 0.5853  loss_mask_2: 0.1727  loss_dice_2: 1.276  loss_ce_3: 0.5117  loss_mask_3: 0.1709  loss_dice_3: 1.249  loss_ce_4: 0.4693  loss_mask_4: 0.1684  loss_dice_4: 1.26  loss_ce_5: 0.449  loss_mask_5: 0.1689  loss_dice_5: 1.243  loss_ce_6: 0.4416  loss_mask_6: 0.167  loss_dice_6: 1.237  loss_ce_7: 0.4551  loss_mask_7: 0.1673  loss_dice_7: 1.208  loss_ce_8: 0.4511  loss_mask_8: 0.1668  loss_dice_8: 1.23    time: 1.0509  last_time: 1.2027  data_time: 0.0699  last_data_time: 0.0621   lr: 0.0001  max_mem: 32638M
[10/19 11:55:31] d2.utils.events INFO:  eta: 0:28:20  iter: 1379  total_loss: 19.76  loss_ce: 0.4433  loss_mask: 0.1724  loss_dice: 1.217  loss_ce_0: 0.5832  loss_mask_0: 0.1908  loss_dice_0: 1.468  loss_ce_1: 0.6351  loss_mask_1: 0.1837  loss_dice_1: 1.378  loss_ce_2: 0.5654  loss_mask_2: 0.1694  loss_dice_2: 1.323  loss_ce_3: 0.5202  loss_mask_3: 0.1746  loss_dice_3: 1.246  loss_ce_4: 0.4968  loss_mask_4: 0.1742  loss_dice_4: 1.27  loss_ce_5: 0.4585  loss_mask_5: 0.1725  loss_dice_5: 1.244  loss_ce_6: 0.4537  loss_mask_6: 0.1728  loss_dice_6: 1.236  loss_ce_7: 0.4614  loss_mask_7: 0.1714  loss_dice_7: 1.246  loss_ce_8: 0.4339  loss_mask_8: 0.1712  loss_dice_8: 1.242    time: 1.0511  last_time: 1.0124  data_time: 0.0728  last_data_time: 0.0574   lr: 0.0001  max_mem: 32638M
[10/19 11:55:52] d2.utils.events INFO:  eta: 0:27:58  iter: 1399  total_loss: 19.01  loss_ce: 0.4777  loss_mask: 0.1631  loss_dice: 1.174  loss_ce_0: 0.6157  loss_mask_0: 0.1965  loss_dice_0: 1.381  loss_ce_1: 0.6574  loss_mask_1: 0.1815  loss_dice_1: 1.284  loss_ce_2: 0.579  loss_mask_2: 0.1709  loss_dice_2: 1.208  loss_ce_3: 0.5094  loss_mask_3: 0.1698  loss_dice_3: 1.18  loss_ce_4: 0.4969  loss_mask_4: 0.1642  loss_dice_4: 1.183  loss_ce_5: 0.4981  loss_mask_5: 0.163  loss_dice_5: 1.191  loss_ce_6: 0.4713  loss_mask_6: 0.1654  loss_dice_6: 1.152  loss_ce_7: 0.4827  loss_mask_7: 0.163  loss_dice_7: 1.167  loss_ce_8: 0.4804  loss_mask_8: 0.1609  loss_dice_8: 1.18    time: 1.0509  last_time: 1.0072  data_time: 0.0667  last_data_time: 0.0652   lr: 0.0001  max_mem: 32638M
[10/19 11:56:13] d2.utils.events INFO:  eta: 0:27:38  iter: 1419  total_loss: 20.15  loss_ce: 0.4422  loss_mask: 0.1597  loss_dice: 1.236  loss_ce_0: 0.5902  loss_mask_0: 0.1877  loss_dice_0: 1.468  loss_ce_1: 0.6562  loss_mask_1: 0.1712  loss_dice_1: 1.386  loss_ce_2: 0.5869  loss_mask_2: 0.1669  loss_dice_2: 1.316  loss_ce_3: 0.5411  loss_mask_3: 0.1648  loss_dice_3: 1.282  loss_ce_4: 0.5033  loss_mask_4: 0.1635  loss_dice_4: 1.24  loss_ce_5: 0.4746  loss_mask_5: 0.1629  loss_dice_5: 1.283  loss_ce_6: 0.4853  loss_mask_6: 0.1585  loss_dice_6: 1.233  loss_ce_7: 0.4619  loss_mask_7: 0.1631  loss_dice_7: 1.224  loss_ce_8: 0.4622  loss_mask_8: 0.1623  loss_dice_8: 1.235    time: 1.0509  last_time: 1.0603  data_time: 0.0712  last_data_time: 0.0541   lr: 0.0001  max_mem: 32638M
[10/19 11:56:34] d2.utils.events INFO:  eta: 0:27:17  iter: 1439  total_loss: 20.18  loss_ce: 0.4441  loss_mask: 0.1679  loss_dice: 1.246  loss_ce_0: 0.5497  loss_mask_0: 0.196  loss_dice_0: 1.493  loss_ce_1: 0.6288  loss_mask_1: 0.1779  loss_dice_1: 1.386  loss_ce_2: 0.5942  loss_mask_2: 0.1723  loss_dice_2: 1.331  loss_ce_3: 0.5188  loss_mask_3: 0.1721  loss_dice_3: 1.28  loss_ce_4: 0.5147  loss_mask_4: 0.1701  loss_dice_4: 1.27  loss_ce_5: 0.5199  loss_mask_5: 0.1674  loss_dice_5: 1.283  loss_ce_6: 0.4689  loss_mask_6: 0.1668  loss_dice_6: 1.285  loss_ce_7: 0.4815  loss_mask_7: 0.1675  loss_dice_7: 1.266  loss_ce_8: 0.4786  loss_mask_8: 0.1686  loss_dice_8: 1.294    time: 1.0511  last_time: 1.0436  data_time: 0.0739  last_data_time: 0.0709   lr: 0.0001  max_mem: 32638M
[10/19 11:56:55] d2.utils.events INFO:  eta: 0:26:56  iter: 1459  total_loss: 18.81  loss_ce: 0.4277  loss_mask: 0.1687  loss_dice: 1.219  loss_ce_0: 0.5431  loss_mask_0: 0.1895  loss_dice_0: 1.392  loss_ce_1: 0.6035  loss_mask_1: 0.1781  loss_dice_1: 1.33  loss_ce_2: 0.5743  loss_mask_2: 0.1696  loss_dice_2: 1.291  loss_ce_3: 0.4992  loss_mask_3: 0.1724  loss_dice_3: 1.218  loss_ce_4: 0.4856  loss_mask_4: 0.1692  loss_dice_4: 1.245  loss_ce_5: 0.4801  loss_mask_5: 0.1717  loss_dice_5: 1.242  loss_ce_6: 0.4576  loss_mask_6: 0.1711  loss_dice_6: 1.187  loss_ce_7: 0.4413  loss_mask_7: 0.1711  loss_dice_7: 1.211  loss_ce_8: 0.4595  loss_mask_8: 0.1688  loss_dice_8: 1.176    time: 1.0509  last_time: 1.0222  data_time: 0.0702  last_data_time: 0.0744   lr: 0.0001  max_mem: 32638M
[10/19 11:57:16] d2.utils.events INFO:  eta: 0:26:34  iter: 1479  total_loss: 19.73  loss_ce: 0.4232  loss_mask: 0.1723  loss_dice: 1.23  loss_ce_0: 0.5562  loss_mask_0: 0.1917  loss_dice_0: 1.425  loss_ce_1: 0.6032  loss_mask_1: 0.182  loss_dice_1: 1.387  loss_ce_2: 0.5804  loss_mask_2: 0.171  loss_dice_2: 1.291  loss_ce_3: 0.4955  loss_mask_3: 0.1727  loss_dice_3: 1.273  loss_ce_4: 0.4994  loss_mask_4: 0.1738  loss_dice_4: 1.247  loss_ce_5: 0.5023  loss_mask_5: 0.1718  loss_dice_5: 1.246  loss_ce_6: 0.5019  loss_mask_6: 0.1721  loss_dice_6: 1.221  loss_ce_7: 0.4652  loss_mask_7: 0.1709  loss_dice_7: 1.229  loss_ce_8: 0.4718  loss_mask_8: 0.1712  loss_dice_8: 1.195    time: 1.0509  last_time: 1.0445  data_time: 0.0725  last_data_time: 0.0900   lr: 0.0001  max_mem: 32638M
[10/19 11:57:37] d2.utils.events INFO:  eta: 0:26:13  iter: 1499  total_loss: 19.68  loss_ce: 0.4469  loss_mask: 0.1577  loss_dice: 1.28  loss_ce_0: 0.5637  loss_mask_0: 0.1905  loss_dice_0: 1.418  loss_ce_1: 0.6245  loss_mask_1: 0.1718  loss_dice_1: 1.381  loss_ce_2: 0.5761  loss_mask_2: 0.163  loss_dice_2: 1.352  loss_ce_3: 0.4911  loss_mask_3: 0.1664  loss_dice_3: 1.279  loss_ce_4: 0.4774  loss_mask_4: 0.1649  loss_dice_4: 1.285  loss_ce_5: 0.4904  loss_mask_5: 0.1618  loss_dice_5: 1.264  loss_ce_6: 0.461  loss_mask_6: 0.1589  loss_dice_6: 1.27  loss_ce_7: 0.4571  loss_mask_7: 0.1585  loss_dice_7: 1.255  loss_ce_8: 0.4475  loss_mask_8: 0.1589  loss_dice_8: 1.238    time: 1.0510  last_time: 1.0115  data_time: 0.0698  last_data_time: 0.0691   lr: 0.0001  max_mem: 32638M
[10/19 11:57:58] d2.utils.events INFO:  eta: 0:25:52  iter: 1519  total_loss: 18.72  loss_ce: 0.4204  loss_mask: 0.1804  loss_dice: 1.169  loss_ce_0: 0.5629  loss_mask_0: 0.2124  loss_dice_0: 1.317  loss_ce_1: 0.5793  loss_mask_1: 0.1996  loss_dice_1: 1.266  loss_ce_2: 0.5674  loss_mask_2: 0.1892  loss_dice_2: 1.212  loss_ce_3: 0.5005  loss_mask_3: 0.1828  loss_dice_3: 1.163  loss_ce_4: 0.4971  loss_mask_4: 0.1808  loss_dice_4: 1.167  loss_ce_5: 0.4557  loss_mask_5: 0.179  loss_dice_5: 1.135  loss_ce_6: 0.4506  loss_mask_6: 0.1781  loss_dice_6: 1.131  loss_ce_7: 0.4456  loss_mask_7: 0.1809  loss_dice_7: 1.151  loss_ce_8: 0.4239  loss_mask_8: 0.179  loss_dice_8: 1.148    time: 1.0507  last_time: 1.0144  data_time: 0.0640  last_data_time: 0.0676   lr: 0.0001  max_mem: 32638M
[10/19 11:58:19] d2.utils.events INFO:  eta: 0:25:30  iter: 1539  total_loss: 18.84  loss_ce: 0.3999  loss_mask: 0.1696  loss_dice: 1.155  loss_ce_0: 0.4973  loss_mask_0: 0.1977  loss_dice_0: 1.357  loss_ce_1: 0.5862  loss_mask_1: 0.1825  loss_dice_1: 1.242  loss_ce_2: 0.5362  loss_mask_2: 0.1732  loss_dice_2: 1.224  loss_ce_3: 0.4559  loss_mask_3: 0.1707  loss_dice_3: 1.14  loss_ce_4: 0.4215  loss_mask_4: 0.173  loss_dice_4: 1.162  loss_ce_5: 0.4083  loss_mask_5: 0.1733  loss_dice_5: 1.163  loss_ce_6: 0.4247  loss_mask_6: 0.1725  loss_dice_6: 1.145  loss_ce_7: 0.4025  loss_mask_7: 0.1723  loss_dice_7: 1.152  loss_ce_8: 0.4215  loss_mask_8: 0.1725  loss_dice_8: 1.167    time: 1.0506  last_time: 1.0421  data_time: 0.0667  last_data_time: 0.0675   lr: 0.0001  max_mem: 32638M
[10/19 11:58:40] d2.utils.events INFO:  eta: 0:25:09  iter: 1559  total_loss: 19.4  loss_ce: 0.4366  loss_mask: 0.1658  loss_dice: 1.233  loss_ce_0: 0.5536  loss_mask_0: 0.1878  loss_dice_0: 1.477  loss_ce_1: 0.6135  loss_mask_1: 0.1766  loss_dice_1: 1.369  loss_ce_2: 0.5788  loss_mask_2: 0.171  loss_dice_2: 1.305  loss_ce_3: 0.4979  loss_mask_3: 0.1672  loss_dice_3: 1.25  loss_ce_4: 0.4845  loss_mask_4: 0.1669  loss_dice_4: 1.272  loss_ce_5: 0.466  loss_mask_5: 0.1638  loss_dice_5: 1.27  loss_ce_6: 0.4287  loss_mask_6: 0.1626  loss_dice_6: 1.214  loss_ce_7: 0.4333  loss_mask_7: 0.1626  loss_dice_7: 1.245  loss_ce_8: 0.4369  loss_mask_8: 0.1623  loss_dice_8: 1.238    time: 1.0506  last_time: 1.0296  data_time: 0.0721  last_data_time: 0.0708   lr: 0.0001  max_mem: 32638M
[10/19 11:59:01] d2.utils.events INFO:  eta: 0:24:48  iter: 1579  total_loss: 19.31  loss_ce: 0.4553  loss_mask: 0.1588  loss_dice: 1.225  loss_ce_0: 0.5583  loss_mask_0: 0.1768  loss_dice_0: 1.402  loss_ce_1: 0.6084  loss_mask_1: 0.1655  loss_dice_1: 1.345  loss_ce_2: 0.5815  loss_mask_2: 0.1585  loss_dice_2: 1.263  loss_ce_3: 0.517  loss_mask_3: 0.1595  loss_dice_3: 1.23  loss_ce_4: 0.496  loss_mask_4: 0.1585  loss_dice_4: 1.237  loss_ce_5: 0.4876  loss_mask_5: 0.1582  loss_dice_5: 1.245  loss_ce_6: 0.47  loss_mask_6: 0.1595  loss_dice_6: 1.253  loss_ce_7: 0.479  loss_mask_7: 0.159  loss_dice_7: 1.249  loss_ce_8: 0.4618  loss_mask_8: 0.1572  loss_dice_8: 1.219    time: 1.0506  last_time: 1.0783  data_time: 0.0728  last_data_time: 0.0703   lr: 0.0001  max_mem: 32638M
[10/19 11:59:22] d2.utils.events INFO:  eta: 0:24:27  iter: 1599  total_loss: 19.18  loss_ce: 0.4376  loss_mask: 0.1755  loss_dice: 1.194  loss_ce_0: 0.5495  loss_mask_0: 0.2057  loss_dice_0: 1.429  loss_ce_1: 0.6111  loss_mask_1: 0.1903  loss_dice_1: 1.302  loss_ce_2: 0.5597  loss_mask_2: 0.1809  loss_dice_2: 1.215  loss_ce_3: 0.4813  loss_mask_3: 0.182  loss_dice_3: 1.238  loss_ce_4: 0.4749  loss_mask_4: 0.1801  loss_dice_4: 1.255  loss_ce_5: 0.4647  loss_mask_5: 0.1771  loss_dice_5: 1.207  loss_ce_6: 0.4463  loss_mask_6: 0.1763  loss_dice_6: 1.174  loss_ce_7: 0.4482  loss_mask_7: 0.1764  loss_dice_7: 1.168  loss_ce_8: 0.4397  loss_mask_8: 0.1762  loss_dice_8: 1.172    time: 1.0505  last_time: 1.1100  data_time: 0.0697  last_data_time: 0.0728   lr: 0.0001  max_mem: 32638M
[10/19 11:59:43] d2.utils.events INFO:  eta: 0:24:06  iter: 1619  total_loss: 20.13  loss_ce: 0.4694  loss_mask: 0.1777  loss_dice: 1.262  loss_ce_0: 0.591  loss_mask_0: 0.2045  loss_dice_0: 1.536  loss_ce_1: 0.6564  loss_mask_1: 0.1879  loss_dice_1: 1.415  loss_ce_2: 0.5944  loss_mask_2: 0.1816  loss_dice_2: 1.318  loss_ce_3: 0.5161  loss_mask_3: 0.179  loss_dice_3: 1.259  loss_ce_4: 0.5105  loss_mask_4: 0.1767  loss_dice_4: 1.266  loss_ce_5: 0.4386  loss_mask_5: 0.1748  loss_dice_5: 1.28  loss_ce_6: 0.4827  loss_mask_6: 0.1779  loss_dice_6: 1.249  loss_ce_7: 0.4356  loss_mask_7: 0.1764  loss_dice_7: 1.255  loss_ce_8: 0.4515  loss_mask_8: 0.1745  loss_dice_8: 1.261    time: 1.0507  last_time: 1.0705  data_time: 0.0714  last_data_time: 0.0958   lr: 0.0001  max_mem: 32952M
[10/19 12:00:04] d2.utils.events INFO:  eta: 0:23:45  iter: 1639  total_loss: 19.72  loss_ce: 0.4081  loss_mask: 0.1507  loss_dice: 1.225  loss_ce_0: 0.557  loss_mask_0: 0.1714  loss_dice_0: 1.44  loss_ce_1: 0.5656  loss_mask_1: 0.1584  loss_dice_1: 1.352  loss_ce_2: 0.5482  loss_mask_2: 0.1563  loss_dice_2: 1.291  loss_ce_3: 0.4705  loss_mask_3: 0.151  loss_dice_3: 1.266  loss_ce_4: 0.4634  loss_mask_4: 0.1511  loss_dice_4: 1.275  loss_ce_5: 0.4523  loss_mask_5: 0.1503  loss_dice_5: 1.256  loss_ce_6: 0.4311  loss_mask_6: 0.1508  loss_dice_6: 1.24  loss_ce_7: 0.4072  loss_mask_7: 0.1531  loss_dice_7: 1.236  loss_ce_8: 0.4043  loss_mask_8: 0.1503  loss_dice_8: 1.246    time: 1.0508  last_time: 1.0341  data_time: 0.0711  last_data_time: 0.0803   lr: 0.0001  max_mem: 32952M
[10/19 12:00:26] d2.utils.events INFO:  eta: 0:23:24  iter: 1659  total_loss: 19.33  loss_ce: 0.4609  loss_mask: 0.1652  loss_dice: 1.199  loss_ce_0: 0.5629  loss_mask_0: 0.1842  loss_dice_0: 1.413  loss_ce_1: 0.6211  loss_mask_1: 0.1755  loss_dice_1: 1.3  loss_ce_2: 0.5844  loss_mask_2: 0.1683  loss_dice_2: 1.25  loss_ce_3: 0.5302  loss_mask_3: 0.1658  loss_dice_3: 1.208  loss_ce_4: 0.4985  loss_mask_4: 0.1685  loss_dice_4: 1.195  loss_ce_5: 0.4874  loss_mask_5: 0.1665  loss_dice_5: 1.221  loss_ce_6: 0.4704  loss_mask_6: 0.1683  loss_dice_6: 1.211  loss_ce_7: 0.469  loss_mask_7: 0.1651  loss_dice_7: 1.184  loss_ce_8: 0.4602  loss_mask_8: 0.1647  loss_dice_8: 1.228    time: 1.0510  last_time: 1.0182  data_time: 0.0710  last_data_time: 0.0609   lr: 0.0001  max_mem: 32952M
[10/19 12:00:47] d2.utils.events INFO:  eta: 0:23:02  iter: 1679  total_loss: 19.05  loss_ce: 0.4519  loss_mask: 0.1653  loss_dice: 1.17  loss_ce_0: 0.5961  loss_mask_0: 0.1932  loss_dice_0: 1.349  loss_ce_1: 0.6192  loss_mask_1: 0.1796  loss_dice_1: 1.32  loss_ce_2: 0.6245  loss_mask_2: 0.1751  loss_dice_2: 1.254  loss_ce_3: 0.5224  loss_mask_3: 0.1683  loss_dice_3: 1.166  loss_ce_4: 0.4941  loss_mask_4: 0.1697  loss_dice_4: 1.2  loss_ce_5: 0.4931  loss_mask_5: 0.1677  loss_dice_5: 1.21  loss_ce_6: 0.4514  loss_mask_6: 0.1663  loss_dice_6: 1.147  loss_ce_7: 0.4751  loss_mask_7: 0.1669  loss_dice_7: 1.149  loss_ce_8: 0.4405  loss_mask_8: 0.1672  loss_dice_8: 1.181    time: 1.0509  last_time: 1.0406  data_time: 0.0698  last_data_time: 0.0730   lr: 0.0001  max_mem: 32952M
[10/19 12:01:08] d2.utils.events INFO:  eta: 0:22:42  iter: 1699  total_loss: 19.54  loss_ce: 0.4609  loss_mask: 0.165  loss_dice: 1.193  loss_ce_0: 0.5801  loss_mask_0: 0.184  loss_dice_0: 1.427  loss_ce_1: 0.653  loss_mask_1: 0.1746  loss_dice_1: 1.302  loss_ce_2: 0.6049  loss_mask_2: 0.169  loss_dice_2: 1.277  loss_ce_3: 0.5146  loss_mask_3: 0.1645  loss_dice_3: 1.222  loss_ce_4: 0.5163  loss_mask_4: 0.168  loss_dice_4: 1.198  loss_ce_5: 0.5045  loss_mask_5: 0.1667  loss_dice_5: 1.199  loss_ce_6: 0.4613  loss_mask_6: 0.1651  loss_dice_6: 1.187  loss_ce_7: 0.4844  loss_mask_7: 0.1645  loss_dice_7: 1.222  loss_ce_8: 0.4455  loss_mask_8: 0.163  loss_dice_8: 1.207    time: 1.0510  last_time: 1.0136  data_time: 0.0730  last_data_time: 0.0603   lr: 0.0001  max_mem: 32952M
[10/19 12:01:29] d2.utils.events INFO:  eta: 0:22:20  iter: 1719  total_loss: 19.25  loss_ce: 0.4222  loss_mask: 0.1845  loss_dice: 1.204  loss_ce_0: 0.5306  loss_mask_0: 0.2191  loss_dice_0: 1.401  loss_ce_1: 0.5881  loss_mask_1: 0.2011  loss_dice_1: 1.339  loss_ce_2: 0.5803  loss_mask_2: 0.1891  loss_dice_2: 1.282  loss_ce_3: 0.4894  loss_mask_3: 0.1912  loss_dice_3: 1.215  loss_ce_4: 0.4716  loss_mask_4: 0.1884  loss_dice_4: 1.214  loss_ce_5: 0.4683  loss_mask_5: 0.1848  loss_dice_5: 1.233  loss_ce_6: 0.4422  loss_mask_6: 0.186  loss_dice_6: 1.236  loss_ce_7: 0.4376  loss_mask_7: 0.1837  loss_dice_7: 1.219  loss_ce_8: 0.4364  loss_mask_8: 0.187  loss_dice_8: 1.215    time: 1.0508  last_time: 1.0426  data_time: 0.0653  last_data_time: 0.0518   lr: 0.0001  max_mem: 32952M
[10/19 12:01:50] d2.utils.events INFO:  eta: 0:22:00  iter: 1739  total_loss: 19.09  loss_ce: 0.432  loss_mask: 0.1657  loss_dice: 1.155  loss_ce_0: 0.55  loss_mask_0: 0.1942  loss_dice_0: 1.367  loss_ce_1: 0.6195  loss_mask_1: 0.1819  loss_dice_1: 1.276  loss_ce_2: 0.5604  loss_mask_2: 0.1735  loss_dice_2: 1.215  loss_ce_3: 0.5421  loss_mask_3: 0.1685  loss_dice_3: 1.16  loss_ce_4: 0.4813  loss_mask_4: 0.1671  loss_dice_4: 1.147  loss_ce_5: 0.4619  loss_mask_5: 0.168  loss_dice_5: 1.129  loss_ce_6: 0.466  loss_mask_6: 0.1656  loss_dice_6: 1.134  loss_ce_7: 0.436  loss_mask_7: 0.1657  loss_dice_7: 1.122  loss_ce_8: 0.4569  loss_mask_8: 0.1657  loss_dice_8: 1.131    time: 1.0508  last_time: 1.0075  data_time: 0.0669  last_data_time: 0.0684   lr: 0.0001  max_mem: 32952M
[10/19 12:02:11] d2.utils.events INFO:  eta: 0:21:39  iter: 1759  total_loss: 18.69  loss_ce: 0.4616  loss_mask: 0.1577  loss_dice: 1.197  loss_ce_0: 0.5484  loss_mask_0: 0.1787  loss_dice_0: 1.377  loss_ce_1: 0.6183  loss_mask_1: 0.1759  loss_dice_1: 1.336  loss_ce_2: 0.5526  loss_mask_2: 0.1644  loss_dice_2: 1.257  loss_ce_3: 0.5091  loss_mask_3: 0.1651  loss_dice_3: 1.212  loss_ce_4: 0.4804  loss_mask_4: 0.1623  loss_dice_4: 1.231  loss_ce_5: 0.4536  loss_mask_5: 0.1669  loss_dice_5: 1.188  loss_ce_6: 0.456  loss_mask_6: 0.1608  loss_dice_6: 1.184  loss_ce_7: 0.4332  loss_mask_7: 0.161  loss_dice_7: 1.195  loss_ce_8: 0.4673  loss_mask_8: 0.1583  loss_dice_8: 1.192    time: 1.0509  last_time: 1.0276  data_time: 0.0737  last_data_time: 0.0810   lr: 0.0001  max_mem: 32952M
[10/19 12:02:32] d2.utils.events INFO:  eta: 0:21:18  iter: 1779  total_loss: 20.13  loss_ce: 0.4489  loss_mask: 0.1591  loss_dice: 1.298  loss_ce_0: 0.5789  loss_mask_0: 0.1925  loss_dice_0: 1.521  loss_ce_1: 0.5672  loss_mask_1: 0.1797  loss_dice_1: 1.429  loss_ce_2: 0.5854  loss_mask_2: 0.1678  loss_dice_2: 1.371  loss_ce_3: 0.5344  loss_mask_3: 0.1638  loss_dice_3: 1.328  loss_ce_4: 0.4975  loss_mask_4: 0.1615  loss_dice_4: 1.322  loss_ce_5: 0.4948  loss_mask_5: 0.161  loss_dice_5: 1.292  loss_ce_6: 0.4775  loss_mask_6: 0.1594  loss_dice_6: 1.265  loss_ce_7: 0.4522  loss_mask_7: 0.1611  loss_dice_7: 1.291  loss_ce_8: 0.4677  loss_mask_8: 0.1606  loss_dice_8: 1.278    time: 1.0510  last_time: 1.0979  data_time: 0.0685  last_data_time: 0.0449   lr: 0.0001  max_mem: 32952M
[10/19 12:02:54] d2.utils.events INFO:  eta: 0:20:58  iter: 1799  total_loss: 20.25  loss_ce: 0.4332  loss_mask: 0.1504  loss_dice: 1.284  loss_ce_0: 0.5891  loss_mask_0: 0.1729  loss_dice_0: 1.484  loss_ce_1: 0.6366  loss_mask_1: 0.1637  loss_dice_1: 1.351  loss_ce_2: 0.586  loss_mask_2: 0.1585  loss_dice_2: 1.37  loss_ce_3: 0.516  loss_mask_3: 0.1552  loss_dice_3: 1.295  loss_ce_4: 0.48  loss_mask_4: 0.1574  loss_dice_4: 1.307  loss_ce_5: 0.4654  loss_mask_5: 0.1545  loss_dice_5: 1.28  loss_ce_6: 0.4533  loss_mask_6: 0.1529  loss_dice_6: 1.279  loss_ce_7: 0.4458  loss_mask_7: 0.1526  loss_dice_7: 1.296  loss_ce_8: 0.4598  loss_mask_8: 0.1523  loss_dice_8: 1.26    time: 1.0513  last_time: 1.0918  data_time: 0.0777  last_data_time: 0.0795   lr: 0.0001  max_mem: 32952M
[10/19 12:03:15] d2.utils.events INFO:  eta: 0:20:38  iter: 1819  total_loss: 19.86  loss_ce: 0.4341  loss_mask: 0.1599  loss_dice: 1.267  loss_ce_0: 0.559  loss_mask_0: 0.1876  loss_dice_0: 1.487  loss_ce_1: 0.6002  loss_mask_1: 0.1721  loss_dice_1: 1.41  loss_ce_2: 0.571  loss_mask_2: 0.1676  loss_dice_2: 1.335  loss_ce_3: 0.4944  loss_mask_3: 0.165  loss_dice_3: 1.282  loss_ce_4: 0.4863  loss_mask_4: 0.1647  loss_dice_4: 1.285  loss_ce_5: 0.4935  loss_mask_5: 0.1645  loss_dice_5: 1.249  loss_ce_6: 0.4477  loss_mask_6: 0.1622  loss_dice_6: 1.262  loss_ce_7: 0.4467  loss_mask_7: 0.1603  loss_dice_7: 1.251  loss_ce_8: 0.43  loss_mask_8: 0.1616  loss_dice_8: 1.27    time: 1.0513  last_time: 1.0788  data_time: 0.0709  last_data_time: 0.0933   lr: 0.0001  max_mem: 32952M
[10/19 12:03:36] d2.utils.events INFO:  eta: 0:20:16  iter: 1839  total_loss: 18.19  loss_ce: 0.4331  loss_mask: 0.1641  loss_dice: 1.132  loss_ce_0: 0.551  loss_mask_0: 0.1901  loss_dice_0: 1.347  loss_ce_1: 0.6291  loss_mask_1: 0.1793  loss_dice_1: 1.285  loss_ce_2: 0.557  loss_mask_2: 0.1688  loss_dice_2: 1.228  loss_ce_3: 0.465  loss_mask_3: 0.1651  loss_dice_3: 1.163  loss_ce_4: 0.439  loss_mask_4: 0.1634  loss_dice_4: 1.171  loss_ce_5: 0.455  loss_mask_5: 0.1619  loss_dice_5: 1.139  loss_ce_6: 0.4276  loss_mask_6: 0.1629  loss_dice_6: 1.149  loss_ce_7: 0.4516  loss_mask_7: 0.1643  loss_dice_7: 1.173  loss_ce_8: 0.4205  loss_mask_8: 0.1669  loss_dice_8: 1.152    time: 1.0513  last_time: 1.0474  data_time: 0.0666  last_data_time: 0.0671   lr: 0.0001  max_mem: 32952M
[10/19 12:03:57] d2.utils.events INFO:  eta: 0:19:55  iter: 1859  total_loss: 18.56  loss_ce: 0.4537  loss_mask: 0.1674  loss_dice: 1.162  loss_ce_0: 0.5909  loss_mask_0: 0.1909  loss_dice_0: 1.341  loss_ce_1: 0.6038  loss_mask_1: 0.1765  loss_dice_1: 1.339  loss_ce_2: 0.581  loss_mask_2: 0.1681  loss_dice_2: 1.239  loss_ce_3: 0.4619  loss_mask_3: 0.1703  loss_dice_3: 1.215  loss_ce_4: 0.4794  loss_mask_4: 0.1704  loss_dice_4: 1.195  loss_ce_5: 0.4604  loss_mask_5: 0.1701  loss_dice_5: 1.186  loss_ce_6: 0.4408  loss_mask_6: 0.1691  loss_dice_6: 1.149  loss_ce_7: 0.4478  loss_mask_7: 0.1694  loss_dice_7: 1.208  loss_ce_8: 0.4583  loss_mask_8: 0.1674  loss_dice_8: 1.159    time: 1.0512  last_time: 1.1026  data_time: 0.0681  last_data_time: 0.0818   lr: 0.0001  max_mem: 32952M
[10/19 12:04:18] d2.utils.events INFO:  eta: 0:19:35  iter: 1879  total_loss: 18.38  loss_ce: 0.3939  loss_mask: 0.1662  loss_dice: 1.17  loss_ce_0: 0.5295  loss_mask_0: 0.1924  loss_dice_0: 1.336  loss_ce_1: 0.5485  loss_mask_1: 0.1741  loss_dice_1: 1.31  loss_ce_2: 0.5248  loss_mask_2: 0.1655  loss_dice_2: 1.253  loss_ce_3: 0.4379  loss_mask_3: 0.1673  loss_dice_3: 1.189  loss_ce_4: 0.4552  loss_mask_4: 0.1658  loss_dice_4: 1.207  loss_ce_5: 0.4104  loss_mask_5: 0.1684  loss_dice_5: 1.199  loss_ce_6: 0.3725  loss_mask_6: 0.1659  loss_dice_6: 1.173  loss_ce_7: 0.4016  loss_mask_7: 0.1638  loss_dice_7: 1.177  loss_ce_8: 0.3922  loss_mask_8: 0.1644  loss_dice_8: 1.153    time: 1.0512  last_time: 1.0542  data_time: 0.0718  last_data_time: 0.0595   lr: 0.0001  max_mem: 32952M
[10/19 12:04:40] d2.utils.events INFO:  eta: 0:19:14  iter: 1899  total_loss: 19.63  loss_ce: 0.4307  loss_mask: 0.1482  loss_dice: 1.241  loss_ce_0: 0.5655  loss_mask_0: 0.1721  loss_dice_0: 1.467  loss_ce_1: 0.62  loss_mask_1: 0.1614  loss_dice_1: 1.375  loss_ce_2: 0.5935  loss_mask_2: 0.1517  loss_dice_2: 1.33  loss_ce_3: 0.4844  loss_mask_3: 0.1509  loss_dice_3: 1.246  loss_ce_4: 0.4843  loss_mask_4: 0.1516  loss_dice_4: 1.248  loss_ce_5: 0.4711  loss_mask_5: 0.1491  loss_dice_5: 1.247  loss_ce_6: 0.4517  loss_mask_6: 0.1509  loss_dice_6: 1.232  loss_ce_7: 0.4434  loss_mask_7: 0.1518  loss_dice_7: 1.243  loss_ce_8: 0.4262  loss_mask_8: 0.1504  loss_dice_8: 1.232    time: 1.0515  last_time: 1.0873  data_time: 0.0756  last_data_time: 0.0724   lr: 0.0001  max_mem: 32952M
[10/19 12:05:01] d2.utils.events INFO:  eta: 0:18:53  iter: 1919  total_loss: 18.38  loss_ce: 0.3968  loss_mask: 0.1607  loss_dice: 1.144  loss_ce_0: 0.5373  loss_mask_0: 0.183  loss_dice_0: 1.381  loss_ce_1: 0.5758  loss_mask_1: 0.1679  loss_dice_1: 1.261  loss_ce_2: 0.535  loss_mask_2: 0.1654  loss_dice_2: 1.266  loss_ce_3: 0.4807  loss_mask_3: 0.1653  loss_dice_3: 1.19  loss_ce_4: 0.4296  loss_mask_4: 0.1646  loss_dice_4: 1.187  loss_ce_5: 0.4418  loss_mask_5: 0.1602  loss_dice_5: 1.184  loss_ce_6: 0.4353  loss_mask_6: 0.1614  loss_dice_6: 1.167  loss_ce_7: 0.3921  loss_mask_7: 0.1615  loss_dice_7: 1.173  loss_ce_8: 0.3966  loss_mask_8: 0.1621  loss_dice_8: 1.136    time: 1.0515  last_time: 0.9964  data_time: 0.0720  last_data_time: 0.0561   lr: 0.0001  max_mem: 32952M
[10/19 12:05:22] d2.utils.events INFO:  eta: 0:18:32  iter: 1939  total_loss: 18.64  loss_ce: 0.4298  loss_mask: 0.1645  loss_dice: 1.155  loss_ce_0: 0.5219  loss_mask_0: 0.191  loss_dice_0: 1.364  loss_ce_1: 0.6178  loss_mask_1: 0.1842  loss_dice_1: 1.292  loss_ce_2: 0.5574  loss_mask_2: 0.1728  loss_dice_2: 1.254  loss_ce_3: 0.4777  loss_mask_3: 0.1696  loss_dice_3: 1.18  loss_ce_4: 0.4811  loss_mask_4: 0.1671  loss_dice_4: 1.18  loss_ce_5: 0.456  loss_mask_5: 0.1624  loss_dice_5: 1.187  loss_ce_6: 0.4282  loss_mask_6: 0.1643  loss_dice_6: 1.178  loss_ce_7: 0.4191  loss_mask_7: 0.164  loss_dice_7: 1.169  loss_ce_8: 0.4111  loss_mask_8: 0.1639  loss_dice_8: 1.188    time: 1.0515  last_time: 1.0718  data_time: 0.0664  last_data_time: 0.0689   lr: 0.0001  max_mem: 32952M
[10/19 12:05:43] d2.utils.events INFO:  eta: 0:18:11  iter: 1959  total_loss: 19.35  loss_ce: 0.4283  loss_mask: 0.1591  loss_dice: 1.194  loss_ce_0: 0.5341  loss_mask_0: 0.1916  loss_dice_0: 1.429  loss_ce_1: 0.5875  loss_mask_1: 0.1739  loss_dice_1: 1.349  loss_ce_2: 0.548  loss_mask_2: 0.1665  loss_dice_2: 1.263  loss_ce_3: 0.5263  loss_mask_3: 0.1629  loss_dice_3: 1.251  loss_ce_4: 0.454  loss_mask_4: 0.1609  loss_dice_4: 1.222  loss_ce_5: 0.4686  loss_mask_5: 0.157  loss_dice_5: 1.211  loss_ce_6: 0.4509  loss_mask_6: 0.1588  loss_dice_6: 1.201  loss_ce_7: 0.4512  loss_mask_7: 0.1576  loss_dice_7: 1.187  loss_ce_8: 0.4524  loss_mask_8: 0.1597  loss_dice_8: 1.184    time: 1.0515  last_time: 1.1359  data_time: 0.0667  last_data_time: 0.0570   lr: 0.0001  max_mem: 32952M
[10/19 12:06:04] d2.utils.events INFO:  eta: 0:17:50  iter: 1979  total_loss: 18.76  loss_ce: 0.436  loss_mask: 0.1574  loss_dice: 1.141  loss_ce_0: 0.5317  loss_mask_0: 0.1842  loss_dice_0: 1.385  loss_ce_1: 0.6177  loss_mask_1: 0.18  loss_dice_1: 1.269  loss_ce_2: 0.5605  loss_mask_2: 0.1668  loss_dice_2: 1.243  loss_ce_3: 0.5097  loss_mask_3: 0.1658  loss_dice_3: 1.164  loss_ce_4: 0.4513  loss_mask_4: 0.1613  loss_dice_4: 1.171  loss_ce_5: 0.4558  loss_mask_5: 0.1577  loss_dice_5: 1.146  loss_ce_6: 0.4582  loss_mask_6: 0.158  loss_dice_6: 1.146  loss_ce_7: 0.4332  loss_mask_7: 0.1553  loss_dice_7: 1.139  loss_ce_8: 0.4699  loss_mask_8: 0.1566  loss_dice_8: 1.146    time: 1.0515  last_time: 1.0561  data_time: 0.0668  last_data_time: 0.0774   lr: 0.0001  max_mem: 32952M
[10/19 12:06:25] d2.utils.events INFO:  eta: 0:17:28  iter: 1999  total_loss: 19.82  loss_ce: 0.4427  loss_mask: 0.1716  loss_dice: 1.228  loss_ce_0: 0.531  loss_mask_0: 0.1976  loss_dice_0: 1.437  loss_ce_1: 0.6186  loss_mask_1: 0.1809  loss_dice_1: 1.351  loss_ce_2: 0.5577  loss_mask_2: 0.1754  loss_dice_2: 1.315  loss_ce_3: 0.5056  loss_mask_3: 0.1737  loss_dice_3: 1.257  loss_ce_4: 0.5065  loss_mask_4: 0.1714  loss_dice_4: 1.235  loss_ce_5: 0.4504  loss_mask_5: 0.1702  loss_dice_5: 1.265  loss_ce_6: 0.4412  loss_mask_6: 0.1692  loss_dice_6: 1.221  loss_ce_7: 0.4289  loss_mask_7: 0.1692  loss_dice_7: 1.242  loss_ce_8: 0.4636  loss_mask_8: 0.1703  loss_dice_8: 1.252    time: 1.0514  last_time: 1.0225  data_time: 0.0675  last_data_time: 0.0556   lr: 0.0001  max_mem: 32952M
[10/19 12:06:46] d2.utils.events INFO:  eta: 0:17:06  iter: 2019  total_loss: 18.05  loss_ce: 0.4313  loss_mask: 0.1642  loss_dice: 1.108  loss_ce_0: 0.5016  loss_mask_0: 0.195  loss_dice_0: 1.289  loss_ce_1: 0.5769  loss_mask_1: 0.1759  loss_dice_1: 1.269  loss_ce_2: 0.5359  loss_mask_2: 0.1692  loss_dice_2: 1.179  loss_ce_3: 0.4913  loss_mask_3: 0.1632  loss_dice_3: 1.132  loss_ce_4: 0.4302  loss_mask_4: 0.1654  loss_dice_4: 1.145  loss_ce_5: 0.4375  loss_mask_5: 0.1669  loss_dice_5: 1.13  loss_ce_6: 0.3987  loss_mask_6: 0.1634  loss_dice_6: 1.122  loss_ce_7: 0.3992  loss_mask_7: 0.165  loss_dice_7: 1.134  loss_ce_8: 0.4099  loss_mask_8: 0.1642  loss_dice_8: 1.095    time: 1.0514  last_time: 1.0634  data_time: 0.0679  last_data_time: 0.0663   lr: 0.0001  max_mem: 32952M
[10/19 12:07:07] d2.utils.events INFO:  eta: 0:16:45  iter: 2039  total_loss: 18.99  loss_ce: 0.4313  loss_mask: 0.1716  loss_dice: 1.197  loss_ce_0: 0.5603  loss_mask_0: 0.1994  loss_dice_0: 1.431  loss_ce_1: 0.581  loss_mask_1: 0.1933  loss_dice_1: 1.356  loss_ce_2: 0.5602  loss_mask_2: 0.1736  loss_dice_2: 1.25  loss_ce_3: 0.4813  loss_mask_3: 0.1756  loss_dice_3: 1.199  loss_ce_4: 0.4586  loss_mask_4: 0.1714  loss_dice_4: 1.209  loss_ce_5: 0.4282  loss_mask_5: 0.1726  loss_dice_5: 1.177  loss_ce_6: 0.4232  loss_mask_6: 0.1737  loss_dice_6: 1.175  loss_ce_7: 0.4343  loss_mask_7: 0.1721  loss_dice_7: 1.216  loss_ce_8: 0.4194  loss_mask_8: 0.1711  loss_dice_8: 1.209    time: 1.0514  last_time: 1.0024  data_time: 0.0718  last_data_time: 0.0647   lr: 0.0001  max_mem: 32952M
[10/19 12:07:28] d2.utils.events INFO:  eta: 0:16:24  iter: 2059  total_loss: 19.28  loss_ce: 0.4167  loss_mask: 0.1494  loss_dice: 1.202  loss_ce_0: 0.5474  loss_mask_0: 0.1724  loss_dice_0: 1.412  loss_ce_1: 0.5843  loss_mask_1: 0.1607  loss_dice_1: 1.351  loss_ce_2: 0.5531  loss_mask_2: 0.1639  loss_dice_2: 1.291  loss_ce_3: 0.472  loss_mask_3: 0.1556  loss_dice_3: 1.244  loss_ce_4: 0.45  loss_mask_4: 0.154  loss_dice_4: 1.239  loss_ce_5: 0.4592  loss_mask_5: 0.1538  loss_dice_5: 1.24  loss_ce_6: 0.4236  loss_mask_6: 0.1524  loss_dice_6: 1.23  loss_ce_7: 0.4205  loss_mask_7: 0.1515  loss_dice_7: 1.22  loss_ce_8: 0.4282  loss_mask_8: 0.1502  loss_dice_8: 1.211    time: 1.0513  last_time: 1.0341  data_time: 0.0676  last_data_time: 0.0555   lr: 0.0001  max_mem: 32952M
[10/19 12:07:49] d2.utils.events INFO:  eta: 0:16:03  iter: 2079  total_loss: 19.28  loss_ce: 0.4551  loss_mask: 0.1589  loss_dice: 1.173  loss_ce_0: 0.5708  loss_mask_0: 0.183  loss_dice_0: 1.354  loss_ce_1: 0.6464  loss_mask_1: 0.1713  loss_dice_1: 1.33  loss_ce_2: 0.6009  loss_mask_2: 0.1709  loss_dice_2: 1.267  loss_ce_3: 0.5062  loss_mask_3: 0.1685  loss_dice_3: 1.222  loss_ce_4: 0.4656  loss_mask_4: 0.1674  loss_dice_4: 1.207  loss_ce_5: 0.4841  loss_mask_5: 0.166  loss_dice_5: 1.218  loss_ce_6: 0.4684  loss_mask_6: 0.1618  loss_dice_6: 1.204  loss_ce_7: 0.4473  loss_mask_7: 0.1615  loss_dice_7: 1.197  loss_ce_8: 0.4609  loss_mask_8: 0.1584  loss_dice_8: 1.199    time: 1.0513  last_time: 1.0214  data_time: 0.0699  last_data_time: 0.0554   lr: 0.0001  max_mem: 32952M
[10/19 12:08:10] d2.utils.events INFO:  eta: 0:15:42  iter: 2099  total_loss: 19.19  loss_ce: 0.4164  loss_mask: 0.1654  loss_dice: 1.218  loss_ce_0: 0.5496  loss_mask_0: 0.1811  loss_dice_0: 1.467  loss_ce_1: 0.5852  loss_mask_1: 0.1714  loss_dice_1: 1.364  loss_ce_2: 0.5469  loss_mask_2: 0.1628  loss_dice_2: 1.283  loss_ce_3: 0.4953  loss_mask_3: 0.1683  loss_dice_3: 1.241  loss_ce_4: 0.4499  loss_mask_4: 0.166  loss_dice_4: 1.224  loss_ce_5: 0.4266  loss_mask_5: 0.1643  loss_dice_5: 1.263  loss_ce_6: 0.4542  loss_mask_6: 0.1647  loss_dice_6: 1.2  loss_ce_7: 0.4292  loss_mask_7: 0.1646  loss_dice_7: 1.205  loss_ce_8: 0.4148  loss_mask_8: 0.1647  loss_dice_8: 1.216    time: 1.0513  last_time: 1.0132  data_time: 0.0694  last_data_time: 0.0748   lr: 0.0001  max_mem: 32952M
[10/19 12:08:31] d2.utils.events INFO:  eta: 0:15:21  iter: 2119  total_loss: 18.9  loss_ce: 0.4334  loss_mask: 0.1663  loss_dice: 1.185  loss_ce_0: 0.5349  loss_mask_0: 0.1948  loss_dice_0: 1.364  loss_ce_1: 0.6333  loss_mask_1: 0.1786  loss_dice_1: 1.293  loss_ce_2: 0.5431  loss_mask_2: 0.1699  loss_dice_2: 1.253  loss_ce_3: 0.4684  loss_mask_3: 0.1715  loss_dice_3: 1.204  loss_ce_4: 0.4796  loss_mask_4: 0.1671  loss_dice_4: 1.217  loss_ce_5: 0.4306  loss_mask_5: 0.1664  loss_dice_5: 1.213  loss_ce_6: 0.4251  loss_mask_6: 0.1649  loss_dice_6: 1.179  loss_ce_7: 0.4241  loss_mask_7: 0.1658  loss_dice_7: 1.164  loss_ce_8: 0.4327  loss_mask_8: 0.1656  loss_dice_8: 1.182    time: 1.0512  last_time: 1.0480  data_time: 0.0678  last_data_time: 0.0546   lr: 0.0001  max_mem: 32952M
[10/19 12:08:52] d2.utils.events INFO:  eta: 0:15:00  iter: 2139  total_loss: 19.04  loss_ce: 0.4322  loss_mask: 0.1539  loss_dice: 1.206  loss_ce_0: 0.5631  loss_mask_0: 0.1815  loss_dice_0: 1.378  loss_ce_1: 0.6145  loss_mask_1: 0.1727  loss_dice_1: 1.293  loss_ce_2: 0.5553  loss_mask_2: 0.1527  loss_dice_2: 1.238  loss_ce_3: 0.5054  loss_mask_3: 0.1528  loss_dice_3: 1.186  loss_ce_4: 0.4567  loss_mask_4: 0.1571  loss_dice_4: 1.199  loss_ce_5: 0.4597  loss_mask_5: 0.1553  loss_dice_5: 1.2  loss_ce_6: 0.4451  loss_mask_6: 0.1531  loss_dice_6: 1.184  loss_ce_7: 0.4488  loss_mask_7: 0.1557  loss_dice_7: 1.196  loss_ce_8: 0.4073  loss_mask_8: 0.1539  loss_dice_8: 1.224    time: 1.0512  last_time: 1.0068  data_time: 0.0734  last_data_time: 0.0777   lr: 0.0001  max_mem: 32952M
[10/19 12:09:13] d2.utils.events INFO:  eta: 0:14:39  iter: 2159  total_loss: 19.63  loss_ce: 0.4805  loss_mask: 0.1591  loss_dice: 1.229  loss_ce_0: 0.5664  loss_mask_0: 0.1878  loss_dice_0: 1.463  loss_ce_1: 0.6402  loss_mask_1: 0.1749  loss_dice_1: 1.377  loss_ce_2: 0.5932  loss_mask_2: 0.1635  loss_dice_2: 1.321  loss_ce_3: 0.5314  loss_mask_3: 0.1634  loss_dice_3: 1.263  loss_ce_4: 0.4983  loss_mask_4: 0.162  loss_dice_4: 1.245  loss_ce_5: 0.4973  loss_mask_5: 0.1599  loss_dice_5: 1.256  loss_ce_6: 0.4781  loss_mask_6: 0.1607  loss_dice_6: 1.252  loss_ce_7: 0.4928  loss_mask_7: 0.1576  loss_dice_7: 1.248  loss_ce_8: 0.4777  loss_mask_8: 0.1589  loss_dice_8: 1.277    time: 1.0512  last_time: 1.1065  data_time: 0.0634  last_data_time: 0.0586   lr: 0.0001  max_mem: 32952M
[10/19 12:09:34] d2.utils.events INFO:  eta: 0:14:18  iter: 2179  total_loss: 19.96  loss_ce: 0.4886  loss_mask: 0.1645  loss_dice: 1.227  loss_ce_0: 0.5683  loss_mask_0: 0.1996  loss_dice_0: 1.464  loss_ce_1: 0.624  loss_mask_1: 0.1836  loss_dice_1: 1.339  loss_ce_2: 0.5951  loss_mask_2: 0.171  loss_dice_2: 1.291  loss_ce_3: 0.5265  loss_mask_3: 0.168  loss_dice_3: 1.264  loss_ce_4: 0.5229  loss_mask_4: 0.1672  loss_dice_4: 1.237  loss_ce_5: 0.4893  loss_mask_5: 0.166  loss_dice_5: 1.216  loss_ce_6: 0.4734  loss_mask_6: 0.166  loss_dice_6: 1.201  loss_ce_7: 0.4793  loss_mask_7: 0.1658  loss_dice_7: 1.215  loss_ce_8: 0.4866  loss_mask_8: 0.1637  loss_dice_8: 1.198    time: 1.0512  last_time: 1.0498  data_time: 0.0602  last_data_time: 0.0594   lr: 0.0001  max_mem: 32952M
[10/19 12:09:56] d2.utils.events INFO:  eta: 0:13:57  iter: 2199  total_loss: 19.73  loss_ce: 0.4594  loss_mask: 0.1574  loss_dice: 1.255  loss_ce_0: 0.5893  loss_mask_0: 0.1809  loss_dice_0: 1.436  loss_ce_1: 0.6306  loss_mask_1: 0.1607  loss_dice_1: 1.371  loss_ce_2: 0.5707  loss_mask_2: 0.157  loss_dice_2: 1.314  loss_ce_3: 0.5012  loss_mask_3: 0.1611  loss_dice_3: 1.26  loss_ce_4: 0.4989  loss_mask_4: 0.1526  loss_dice_4: 1.245  loss_ce_5: 0.4867  loss_mask_5: 0.1517  loss_dice_5: 1.236  loss_ce_6: 0.4621  loss_mask_6: 0.1574  loss_dice_6: 1.219  loss_ce_7: 0.4605  loss_mask_7: 0.1534  loss_dice_7: 1.189  loss_ce_8: 0.4742  loss_mask_8: 0.1577  loss_dice_8: 1.237    time: 1.0513  last_time: 1.0652  data_time: 0.0622  last_data_time: 0.1135   lr: 0.0001  max_mem: 32952M
[10/19 12:10:16] d2.utils.events INFO:  eta: 0:13:37  iter: 2219  total_loss: 19.18  loss_ce: 0.4415  loss_mask: 0.1654  loss_dice: 1.234  loss_ce_0: 0.5786  loss_mask_0: 0.1944  loss_dice_0: 1.363  loss_ce_1: 0.586  loss_mask_1: 0.1748  loss_dice_1: 1.317  loss_ce_2: 0.5716  loss_mask_2: 0.1665  loss_dice_2: 1.279  loss_ce_3: 0.5057  loss_mask_3: 0.1656  loss_dice_3: 1.217  loss_ce_4: 0.4597  loss_mask_4: 0.1673  loss_dice_4: 1.229  loss_ce_5: 0.468  loss_mask_5: 0.1641  loss_dice_5: 1.218  loss_ce_6: 0.4512  loss_mask_6: 0.1658  loss_dice_6: 1.174  loss_ce_7: 0.4427  loss_mask_7: 0.165  loss_dice_7: 1.189  loss_ce_8: 0.429  loss_mask_8: 0.1645  loss_dice_8: 1.166    time: 1.0512  last_time: 1.0553  data_time: 0.0582  last_data_time: 0.0575   lr: 0.0001  max_mem: 32952M
[10/19 12:10:37] d2.utils.events INFO:  eta: 0:13:16  iter: 2239  total_loss: 19.41  loss_ce: 0.4472  loss_mask: 0.159  loss_dice: 1.23  loss_ce_0: 0.553  loss_mask_0: 0.1809  loss_dice_0: 1.455  loss_ce_1: 0.5992  loss_mask_1: 0.1708  loss_dice_1: 1.39  loss_ce_2: 0.5538  loss_mask_2: 0.1637  loss_dice_2: 1.361  loss_ce_3: 0.4758  loss_mask_3: 0.1616  loss_dice_3: 1.294  loss_ce_4: 0.4701  loss_mask_4: 0.1611  loss_dice_4: 1.323  loss_ce_5: 0.4418  loss_mask_5: 0.1601  loss_dice_5: 1.271  loss_ce_6: 0.4463  loss_mask_6: 0.16  loss_dice_6: 1.238  loss_ce_7: 0.4442  loss_mask_7: 0.1619  loss_dice_7: 1.274  loss_ce_8: 0.4444  loss_mask_8: 0.1588  loss_dice_8: 1.258    time: 1.0511  last_time: 0.9855  data_time: 0.0591  last_data_time: 0.0472   lr: 0.0001  max_mem: 32952M
[10/19 12:10:58] d2.utils.events INFO:  eta: 0:12:54  iter: 2259  total_loss: 19.74  loss_ce: 0.4047  loss_mask: 0.1657  loss_dice: 1.258  loss_ce_0: 0.5491  loss_mask_0: 0.1877  loss_dice_0: 1.425  loss_ce_1: 0.6095  loss_mask_1: 0.1806  loss_dice_1: 1.38  loss_ce_2: 0.5792  loss_mask_2: 0.1715  loss_dice_2: 1.311  loss_ce_3: 0.501  loss_mask_3: 0.1695  loss_dice_3: 1.271  loss_ce_4: 0.4702  loss_mask_4: 0.1723  loss_dice_4: 1.253  loss_ce_5: 0.4564  loss_mask_5: 0.1711  loss_dice_5: 1.263  loss_ce_6: 0.4395  loss_mask_6: 0.1674  loss_dice_6: 1.239  loss_ce_7: 0.4282  loss_mask_7: 0.1672  loss_dice_7: 1.232  loss_ce_8: 0.426  loss_mask_8: 0.1669  loss_dice_8: 1.257    time: 1.0509  last_time: 1.0374  data_time: 0.0542  last_data_time: 0.0515   lr: 0.0001  max_mem: 32952M
[10/19 12:11:19] d2.utils.events INFO:  eta: 0:12:33  iter: 2279  total_loss: 19.46  loss_ce: 0.4635  loss_mask: 0.151  loss_dice: 1.202  loss_ce_0: 0.5481  loss_mask_0: 0.1848  loss_dice_0: 1.494  loss_ce_1: 0.6143  loss_mask_1: 0.1677  loss_dice_1: 1.35  loss_ce_2: 0.616  loss_mask_2: 0.155  loss_dice_2: 1.326  loss_ce_3: 0.5223  loss_mask_3: 0.1569  loss_dice_3: 1.242  loss_ce_4: 0.493  loss_mask_4: 0.1557  loss_dice_4: 1.247  loss_ce_5: 0.4755  loss_mask_5: 0.1505  loss_dice_5: 1.249  loss_ce_6: 0.4465  loss_mask_6: 0.1509  loss_dice_6: 1.212  loss_ce_7: 0.4348  loss_mask_7: 0.1526  loss_dice_7: 1.21  loss_ce_8: 0.4587  loss_mask_8: 0.1506  loss_dice_8: 1.215    time: 1.0510  last_time: 1.0373  data_time: 0.0670  last_data_time: 0.0637   lr: 0.0001  max_mem: 32952M
[10/19 12:11:40] d2.utils.events INFO:  eta: 0:12:12  iter: 2299  total_loss: 18.49  loss_ce: 0.3972  loss_mask: 0.1609  loss_dice: 1.163  loss_ce_0: 0.5275  loss_mask_0: 0.1835  loss_dice_0: 1.365  loss_ce_1: 0.5883  loss_mask_1: 0.1695  loss_dice_1: 1.278  loss_ce_2: 0.5341  loss_mask_2: 0.1643  loss_dice_2: 1.229  loss_ce_3: 0.4568  loss_mask_3: 0.164  loss_dice_3: 1.159  loss_ce_4: 0.4281  loss_mask_4: 0.1607  loss_dice_4: 1.167  loss_ce_5: 0.4158  loss_mask_5: 0.1585  loss_dice_5: 1.167  loss_ce_6: 0.3987  loss_mask_6: 0.1624  loss_dice_6: 1.125  loss_ce_7: 0.4431  loss_mask_7: 0.1626  loss_dice_7: 1.143  loss_ce_8: 0.4198  loss_mask_8: 0.1591  loss_dice_8: 1.15    time: 1.0508  last_time: 0.9893  data_time: 0.0552  last_data_time: 0.0573   lr: 0.0001  max_mem: 32952M
[10/19 12:12:01] d2.utils.events INFO:  eta: 0:11:51  iter: 2319  total_loss: 17.89  loss_ce: 0.4091  loss_mask: 0.1512  loss_dice: 1.174  loss_ce_0: 0.519  loss_mask_0: 0.171  loss_dice_0: 1.353  loss_ce_1: 0.5811  loss_mask_1: 0.1607  loss_dice_1: 1.274  loss_ce_2: 0.522  loss_mask_2: 0.1548  loss_dice_2: 1.183  loss_ce_3: 0.4588  loss_mask_3: 0.151  loss_dice_3: 1.171  loss_ce_4: 0.4433  loss_mask_4: 0.1518  loss_dice_4: 1.162  loss_ce_5: 0.432  loss_mask_5: 0.1511  loss_dice_5: 1.146  loss_ce_6: 0.4163  loss_mask_6: 0.1519  loss_dice_6: 1.16  loss_ce_7: 0.39  loss_mask_7: 0.152  loss_dice_7: 1.164  loss_ce_8: 0.416  loss_mask_8: 0.1503  loss_dice_8: 1.147    time: 1.0506  last_time: 1.0333  data_time: 0.0552  last_data_time: 0.0501   lr: 0.0001  max_mem: 32952M
[10/19 12:12:21] d2.utils.events INFO:  eta: 0:11:30  iter: 2339  total_loss: 18.52  loss_ce: 0.4129  loss_mask: 0.1597  loss_dice: 1.201  loss_ce_0: 0.5308  loss_mask_0: 0.1819  loss_dice_0: 1.394  loss_ce_1: 0.5575  loss_mask_1: 0.1744  loss_dice_1: 1.311  loss_ce_2: 0.5328  loss_mask_2: 0.1669  loss_dice_2: 1.258  loss_ce_3: 0.4734  loss_mask_3: 0.162  loss_dice_3: 1.187  loss_ce_4: 0.4545  loss_mask_4: 0.1616  loss_dice_4: 1.205  loss_ce_5: 0.4475  loss_mask_5: 0.1589  loss_dice_5: 1.192  loss_ce_6: 0.4218  loss_mask_6: 0.1583  loss_dice_6: 1.192  loss_ce_7: 0.4003  loss_mask_7: 0.1608  loss_dice_7: 1.174  loss_ce_8: 0.4337  loss_mask_8: 0.1608  loss_dice_8: 1.178    time: 1.0504  last_time: 1.0531  data_time: 0.0551  last_data_time: 0.0576   lr: 0.0001  max_mem: 32952M
[10/19 12:12:42] d2.utils.events INFO:  eta: 0:11:09  iter: 2359  total_loss: 18.25  loss_ce: 0.4201  loss_mask: 0.1602  loss_dice: 1.116  loss_ce_0: 0.5196  loss_mask_0: 0.1878  loss_dice_0: 1.347  loss_ce_1: 0.6026  loss_mask_1: 0.1723  loss_dice_1: 1.273  loss_ce_2: 0.5038  loss_mask_2: 0.166  loss_dice_2: 1.203  loss_ce_3: 0.4609  loss_mask_3: 0.1656  loss_dice_3: 1.162  loss_ce_4: 0.4696  loss_mask_4: 0.1619  loss_dice_4: 1.154  loss_ce_5: 0.4157  loss_mask_5: 0.1616  loss_dice_5: 1.152  loss_ce_6: 0.432  loss_mask_6: 0.1605  loss_dice_6: 1.106  loss_ce_7: 0.4254  loss_mask_7: 0.1621  loss_dice_7: 1.145  loss_ce_8: 0.4214  loss_mask_8: 0.162  loss_dice_8: 1.111    time: 1.0503  last_time: 1.0325  data_time: 0.0551  last_data_time: 0.0633   lr: 0.0001  max_mem: 32952M
[10/19 12:13:03] d2.utils.events INFO:  eta: 0:10:47  iter: 2379  total_loss: 18.08  loss_ce: 0.3821  loss_mask: 0.1591  loss_dice: 1.123  loss_ce_0: 0.4999  loss_mask_0: 0.1874  loss_dice_0: 1.309  loss_ce_1: 0.5556  loss_mask_1: 0.1768  loss_dice_1: 1.293  loss_ce_2: 0.5183  loss_mask_2: 0.1674  loss_dice_2: 1.217  loss_ce_3: 0.4699  loss_mask_3: 0.1678  loss_dice_3: 1.175  loss_ce_4: 0.4265  loss_mask_4: 0.167  loss_dice_4: 1.168  loss_ce_5: 0.4166  loss_mask_5: 0.1605  loss_dice_5: 1.153  loss_ce_6: 0.4009  loss_mask_6: 0.1608  loss_dice_6: 1.151  loss_ce_7: 0.3815  loss_mask_7: 0.1601  loss_dice_7: 1.108  loss_ce_8: 0.3839  loss_mask_8: 0.1602  loss_dice_8: 1.131    time: 1.0501  last_time: 1.0349  data_time: 0.0538  last_data_time: 0.0625   lr: 0.0001  max_mem: 32952M
[10/19 12:13:24] d2.utils.events INFO:  eta: 0:10:26  iter: 2399  total_loss: 18.83  loss_ce: 0.3843  loss_mask: 0.1572  loss_dice: 1.186  loss_ce_0: 0.5074  loss_mask_0: 0.1872  loss_dice_0: 1.408  loss_ce_1: 0.5796  loss_mask_1: 0.168  loss_dice_1: 1.341  loss_ce_2: 0.5159  loss_mask_2: 0.1587  loss_dice_2: 1.289  loss_ce_3: 0.4532  loss_mask_3: 0.161  loss_dice_3: 1.283  loss_ce_4: 0.4264  loss_mask_4: 0.1585  loss_dice_4: 1.225  loss_ce_5: 0.4219  loss_mask_5: 0.1605  loss_dice_5: 1.256  loss_ce_6: 0.4267  loss_mask_6: 0.1596  loss_dice_6: 1.194  loss_ce_7: 0.3917  loss_mask_7: 0.156  loss_dice_7: 1.222  loss_ce_8: 0.4363  loss_mask_8: 0.1555  loss_dice_8: 1.206    time: 1.0501  last_time: 1.0390  data_time: 0.0590  last_data_time: 0.0527   lr: 0.0001  max_mem: 32952M
[10/19 12:13:44] d2.utils.events INFO:  eta: 0:10:05  iter: 2419  total_loss: 18.77  loss_ce: 0.4095  loss_mask: 0.1591  loss_dice: 1.171  loss_ce_0: 0.506  loss_mask_0: 0.1822  loss_dice_0: 1.396  loss_ce_1: 0.6015  loss_mask_1: 0.1696  loss_dice_1: 1.289  loss_ce_2: 0.5412  loss_mask_2: 0.1602  loss_dice_2: 1.241  loss_ce_3: 0.4834  loss_mask_3: 0.1616  loss_dice_3: 1.204  loss_ce_4: 0.414  loss_mask_4: 0.159  loss_dice_4: 1.181  loss_ce_5: 0.4079  loss_mask_5: 0.1594  loss_dice_5: 1.218  loss_ce_6: 0.3934  loss_mask_6: 0.16  loss_dice_6: 1.204  loss_ce_7: 0.3935  loss_mask_7: 0.1578  loss_dice_7: 1.184  loss_ce_8: 0.4009  loss_mask_8: 0.1567  loss_dice_8: 1.206    time: 1.0500  last_time: 1.0876  data_time: 0.0572  last_data_time: 0.0615   lr: 0.0001  max_mem: 32952M
[10/19 12:14:05] d2.utils.events INFO:  eta: 0:09:44  iter: 2439  total_loss: 19.75  loss_ce: 0.441  loss_mask: 0.1506  loss_dice: 1.272  loss_ce_0: 0.5582  loss_mask_0: 0.1723  loss_dice_0: 1.455  loss_ce_1: 0.5866  loss_mask_1: 0.1613  loss_dice_1: 1.384  loss_ce_2: 0.5853  loss_mask_2: 0.1536  loss_dice_2: 1.344  loss_ce_3: 0.4849  loss_mask_3: 0.1567  loss_dice_3: 1.269  loss_ce_4: 0.4741  loss_mask_4: 0.1536  loss_dice_4: 1.298  loss_ce_5: 0.463  loss_mask_5: 0.1532  loss_dice_5: 1.274  loss_ce_6: 0.4304  loss_mask_6: 0.1524  loss_dice_6: 1.229  loss_ce_7: 0.4327  loss_mask_7: 0.151  loss_dice_7: 1.269  loss_ce_8: 0.464  loss_mask_8: 0.15  loss_dice_8: 1.271    time: 1.0499  last_time: 1.0615  data_time: 0.0579  last_data_time: 0.0548   lr: 0.0001  max_mem: 32952M
[10/19 12:14:26] d2.utils.events INFO:  eta: 0:09:23  iter: 2459  total_loss: 18.21  loss_ce: 0.4287  loss_mask: 0.1617  loss_dice: 1.116  loss_ce_0: 0.4802  loss_mask_0: 0.1856  loss_dice_0: 1.326  loss_ce_1: 0.5431  loss_mask_1: 0.1692  loss_dice_1: 1.27  loss_ce_2: 0.5198  loss_mask_2: 0.166  loss_dice_2: 1.17  loss_ce_3: 0.4421  loss_mask_3: 0.1656  loss_dice_3: 1.14  loss_ce_4: 0.4214  loss_mask_4: 0.1632  loss_dice_4: 1.191  loss_ce_5: 0.4118  loss_mask_5: 0.1622  loss_dice_5: 1.148  loss_ce_6: 0.3797  loss_mask_6: 0.1602  loss_dice_6: 1.105  loss_ce_7: 0.3858  loss_mask_7: 0.1612  loss_dice_7: 1.188  loss_ce_8: 0.3721  loss_mask_8: 0.1622  loss_dice_8: 1.147    time: 1.0497  last_time: 1.0345  data_time: 0.0545  last_data_time: 0.0514   lr: 0.0001  max_mem: 32952M
[10/19 12:14:46] d2.utils.events INFO:  eta: 0:09:01  iter: 2479  total_loss: 18.55  loss_ce: 0.445  loss_mask: 0.1672  loss_dice: 1.196  loss_ce_0: 0.5582  loss_mask_0: 0.1938  loss_dice_0: 1.376  loss_ce_1: 0.6053  loss_mask_1: 0.1863  loss_dice_1: 1.341  loss_ce_2: 0.5503  loss_mask_2: 0.1711  loss_dice_2: 1.275  loss_ce_3: 0.4906  loss_mask_3: 0.1709  loss_dice_3: 1.231  loss_ce_4: 0.4563  loss_mask_4: 0.1665  loss_dice_4: 1.196  loss_ce_5: 0.4614  loss_mask_5: 0.1663  loss_dice_5: 1.184  loss_ce_6: 0.4359  loss_mask_6: 0.1689  loss_dice_6: 1.192  loss_ce_7: 0.4239  loss_mask_7: 0.1678  loss_dice_7: 1.182  loss_ce_8: 0.4401  loss_mask_8: 0.1654  loss_dice_8: 1.216    time: 1.0495  last_time: 1.0166  data_time: 0.0547  last_data_time: 0.0564   lr: 0.0001  max_mem: 32952M
[10/19 12:15:07] d2.utils.events INFO:  eta: 0:08:40  iter: 2499  total_loss: 18.67  loss_ce: 0.467  loss_mask: 0.161  loss_dice: 1.154  loss_ce_0: 0.5697  loss_mask_0: 0.1869  loss_dice_0: 1.368  loss_ce_1: 0.6064  loss_mask_1: 0.1691  loss_dice_1: 1.277  loss_ce_2: 0.5292  loss_mask_2: 0.1644  loss_dice_2: 1.234  loss_ce_3: 0.4837  loss_mask_3: 0.159  loss_dice_3: 1.186  loss_ce_4: 0.4579  loss_mask_4: 0.1603  loss_dice_4: 1.172  loss_ce_5: 0.4646  loss_mask_5: 0.1586  loss_dice_5: 1.194  loss_ce_6: 0.4655  loss_mask_6: 0.1594  loss_dice_6: 1.181  loss_ce_7: 0.4611  loss_mask_7: 0.1586  loss_dice_7: 1.182  loss_ce_8: 0.4456  loss_mask_8: 0.1592  loss_dice_8: 1.136    time: 1.0493  last_time: 1.0017  data_time: 0.0563  last_data_time: 0.0505   lr: 0.0001  max_mem: 32952M
[10/19 12:15:28] d2.utils.events INFO:  eta: 0:08:19  iter: 2519  total_loss: 19.35  loss_ce: 0.4294  loss_mask: 0.1497  loss_dice: 1.256  loss_ce_0: 0.545  loss_mask_0: 0.1697  loss_dice_0: 1.485  loss_ce_1: 0.6415  loss_mask_1: 0.1573  loss_dice_1: 1.411  loss_ce_2: 0.5719  loss_mask_2: 0.1517  loss_dice_2: 1.366  loss_ce_3: 0.4964  loss_mask_3: 0.1526  loss_dice_3: 1.31  loss_ce_4: 0.4828  loss_mask_4: 0.1504  loss_dice_4: 1.255  loss_ce_5: 0.4706  loss_mask_5: 0.1486  loss_dice_5: 1.259  loss_ce_6: 0.4318  loss_mask_6: 0.1521  loss_dice_6: 1.258  loss_ce_7: 0.4385  loss_mask_7: 0.1475  loss_dice_7: 1.254  loss_ce_8: 0.4297  loss_mask_8: 0.1485  loss_dice_8: 1.273    time: 1.0494  last_time: 1.0791  data_time: 0.0597  last_data_time: 0.0836   lr: 0.0001  max_mem: 32952M
[10/19 12:15:49] d2.utils.events INFO:  eta: 0:07:59  iter: 2539  total_loss: 19.46  loss_ce: 0.4484  loss_mask: 0.1579  loss_dice: 1.25  loss_ce_0: 0.5534  loss_mask_0: 0.1781  loss_dice_0: 1.41  loss_ce_1: 0.5828  loss_mask_1: 0.1664  loss_dice_1: 1.353  loss_ce_2: 0.5541  loss_mask_2: 0.1579  loss_dice_2: 1.311  loss_ce_3: 0.5298  loss_mask_3: 0.1585  loss_dice_3: 1.254  loss_ce_4: 0.4801  loss_mask_4: 0.157  loss_dice_4: 1.254  loss_ce_5: 0.4625  loss_mask_5: 0.1567  loss_dice_5: 1.25  loss_ce_6: 0.4534  loss_mask_6: 0.157  loss_dice_6: 1.22  loss_ce_7: 0.4437  loss_mask_7: 0.155  loss_dice_7: 1.244  loss_ce_8: 0.4338  loss_mask_8: 0.1563  loss_dice_8: 1.224    time: 1.0494  last_time: 1.1162  data_time: 0.0598  last_data_time: 0.0774   lr: 0.0001  max_mem: 32952M
[10/19 12:16:10] d2.utils.events INFO:  eta: 0:07:38  iter: 2559  total_loss: 18.32  loss_ce: 0.4073  loss_mask: 0.1564  loss_dice: 1.15  loss_ce_0: 0.5032  loss_mask_0: 0.1866  loss_dice_0: 1.355  loss_ce_1: 0.5797  loss_mask_1: 0.1772  loss_dice_1: 1.276  loss_ce_2: 0.5081  loss_mask_2: 0.1628  loss_dice_2: 1.213  loss_ce_3: 0.4777  loss_mask_3: 0.1617  loss_dice_3: 1.195  loss_ce_4: 0.4415  loss_mask_4: 0.1641  loss_dice_4: 1.183  loss_ce_5: 0.4275  loss_mask_5: 0.1601  loss_dice_5: 1.163  loss_ce_6: 0.4375  loss_mask_6: 0.1609  loss_dice_6: 1.162  loss_ce_7: 0.4138  loss_mask_7: 0.1602  loss_dice_7: 1.151  loss_ce_8: 0.4192  loss_mask_8: 0.1613  loss_dice_8: 1.162    time: 1.0493  last_time: 0.9772  data_time: 0.0587  last_data_time: 0.0614   lr: 0.0001  max_mem: 32952M
[10/19 12:16:31] d2.utils.events INFO:  eta: 0:07:17  iter: 2579  total_loss: 18.41  loss_ce: 0.4532  loss_mask: 0.1568  loss_dice: 1.135  loss_ce_0: 0.5929  loss_mask_0: 0.1862  loss_dice_0: 1.348  loss_ce_1: 0.6617  loss_mask_1: 0.1647  loss_dice_1: 1.282  loss_ce_2: 0.5942  loss_mask_2: 0.1621  loss_dice_2: 1.212  loss_ce_3: 0.5308  loss_mask_3: 0.161  loss_dice_3: 1.166  loss_ce_4: 0.4831  loss_mask_4: 0.1597  loss_dice_4: 1.121  loss_ce_5: 0.4696  loss_mask_5: 0.1575  loss_dice_5: 1.132  loss_ce_6: 0.4556  loss_mask_6: 0.1579  loss_dice_6: 1.114  loss_ce_7: 0.4516  loss_mask_7: 0.1562  loss_dice_7: 1.086  loss_ce_8: 0.4926  loss_mask_8: 0.1587  loss_dice_8: 1.095    time: 1.0493  last_time: 1.0368  data_time: 0.0584  last_data_time: 0.0644   lr: 0.0001  max_mem: 32952M
[10/19 12:16:52] d2.utils.events INFO:  eta: 0:06:56  iter: 2599  total_loss: 18.27  loss_ce: 0.4325  loss_mask: 0.1612  loss_dice: 1.145  loss_ce_0: 0.5149  loss_mask_0: 0.1909  loss_dice_0: 1.338  loss_ce_1: 0.5581  loss_mask_1: 0.1681  loss_dice_1: 1.258  loss_ce_2: 0.513  loss_mask_2: 0.1639  loss_dice_2: 1.213  loss_ce_3: 0.4937  loss_mask_3: 0.1609  loss_dice_3: 1.216  loss_ce_4: 0.4437  loss_mask_4: 0.1622  loss_dice_4: 1.15  loss_ce_5: 0.447  loss_mask_5: 0.1616  loss_dice_5: 1.111  loss_ce_6: 0.4072  loss_mask_6: 0.162  loss_dice_6: 1.112  loss_ce_7: 0.4153  loss_mask_7: 0.1626  loss_dice_7: 1.128  loss_ce_8: 0.4238  loss_mask_8: 0.1609  loss_dice_8: 1.16    time: 1.0492  last_time: 1.0080  data_time: 0.0582  last_data_time: 0.0589   lr: 0.0001  max_mem: 32952M
[10/19 12:17:12] d2.utils.events INFO:  eta: 0:06:35  iter: 2619  total_loss: 18.4  loss_ce: 0.3824  loss_mask: 0.1606  loss_dice: 1.161  loss_ce_0: 0.4852  loss_mask_0: 0.1841  loss_dice_0: 1.356  loss_ce_1: 0.5549  loss_mask_1: 0.173  loss_dice_1: 1.276  loss_ce_2: 0.5187  loss_mask_2: 0.166  loss_dice_2: 1.262  loss_ce_3: 0.4383  loss_mask_3: 0.1646  loss_dice_3: 1.194  loss_ce_4: 0.4417  loss_mask_4: 0.1634  loss_dice_4: 1.199  loss_ce_5: 0.426  loss_mask_5: 0.1612  loss_dice_5: 1.216  loss_ce_6: 0.4134  loss_mask_6: 0.1607  loss_dice_6: 1.121  loss_ce_7: 0.4052  loss_mask_7: 0.1596  loss_dice_7: 1.134  loss_ce_8: 0.4304  loss_mask_8: 0.1591  loss_dice_8: 1.14    time: 1.0489  last_time: 1.0574  data_time: 0.0537  last_data_time: 0.0558   lr: 0.0001  max_mem: 32952M
[10/19 12:17:33] d2.utils.events INFO:  eta: 0:06:14  iter: 2639  total_loss: 18.67  loss_ce: 0.4067  loss_mask: 0.1563  loss_dice: 1.186  loss_ce_0: 0.536  loss_mask_0: 0.1852  loss_dice_0: 1.413  loss_ce_1: 0.5603  loss_mask_1: 0.1685  loss_dice_1: 1.346  loss_ce_2: 0.5476  loss_mask_2: 0.1628  loss_dice_2: 1.24  loss_ce_3: 0.4566  loss_mask_3: 0.1617  loss_dice_3: 1.218  loss_ce_4: 0.451  loss_mask_4: 0.1597  loss_dice_4: 1.207  loss_ce_5: 0.4393  loss_mask_5: 0.1563  loss_dice_5: 1.154  loss_ce_6: 0.4202  loss_mask_6: 0.1561  loss_dice_6: 1.199  loss_ce_7: 0.4426  loss_mask_7: 0.1586  loss_dice_7: 1.2  loss_ce_8: 0.4281  loss_mask_8: 0.1577  loss_dice_8: 1.206    time: 1.0488  last_time: 1.0272  data_time: 0.0567  last_data_time: 0.0571   lr: 0.0001  max_mem: 32952M
[10/19 12:17:54] d2.utils.events INFO:  eta: 0:05:53  iter: 2659  total_loss: 19.24  loss_ce: 0.41  loss_mask: 0.1471  loss_dice: 1.246  loss_ce_0: 0.543  loss_mask_0: 0.166  loss_dice_0: 1.486  loss_ce_1: 0.5673  loss_mask_1: 0.1627  loss_dice_1: 1.375  loss_ce_2: 0.5401  loss_mask_2: 0.155  loss_dice_2: 1.321  loss_ce_3: 0.494  loss_mask_3: 0.1513  loss_dice_3: 1.269  loss_ce_4: 0.4734  loss_mask_4: 0.1483  loss_dice_4: 1.282  loss_ce_5: 0.4426  loss_mask_5: 0.1489  loss_dice_5: 1.246  loss_ce_6: 0.4257  loss_mask_6: 0.1479  loss_dice_6: 1.24  loss_ce_7: 0.4347  loss_mask_7: 0.1473  loss_dice_7: 1.235  loss_ce_8: 0.445  loss_mask_8: 0.1492  loss_dice_8: 1.252    time: 1.0488  last_time: 0.9986  data_time: 0.0572  last_data_time: 0.0501   lr: 0.0001  max_mem: 32952M
[10/19 12:18:15] d2.utils.events INFO:  eta: 0:05:32  iter: 2679  total_loss: 17.89  loss_ce: 0.4132  loss_mask: 0.1547  loss_dice: 1.122  loss_ce_0: 0.4875  loss_mask_0: 0.1833  loss_dice_0: 1.31  loss_ce_1: 0.5283  loss_mask_1: 0.1709  loss_dice_1: 1.236  loss_ce_2: 0.4834  loss_mask_2: 0.1666  loss_dice_2: 1.218  loss_ce_3: 0.4351  loss_mask_3: 0.1665  loss_dice_3: 1.153  loss_ce_4: 0.4139  loss_mask_4: 0.1614  loss_dice_4: 1.128  loss_ce_5: 0.4146  loss_mask_5: 0.1607  loss_dice_5: 1.145  loss_ce_6: 0.4108  loss_mask_6: 0.1568  loss_dice_6: 1.115  loss_ce_7: 0.3979  loss_mask_7: 0.1575  loss_dice_7: 1.117  loss_ce_8: 0.4257  loss_mask_8: 0.1575  loss_dice_8: 1.089    time: 1.0487  last_time: 1.0212  data_time: 0.0532  last_data_time: 0.0526   lr: 0.0001  max_mem: 32952M
[10/19 12:18:36] d2.utils.events INFO:  eta: 0:05:12  iter: 2699  total_loss: 18.67  loss_ce: 0.3871  loss_mask: 0.1498  loss_dice: 1.184  loss_ce_0: 0.567  loss_mask_0: 0.1631  loss_dice_0: 1.392  loss_ce_1: 0.522  loss_mask_1: 0.1522  loss_dice_1: 1.289  loss_ce_2: 0.5091  loss_mask_2: 0.1481  loss_dice_2: 1.249  loss_ce_3: 0.4416  loss_mask_3: 0.1506  loss_dice_3: 1.176  loss_ce_4: 0.4411  loss_mask_4: 0.1514  loss_dice_4: 1.218  loss_ce_5: 0.3965  loss_mask_5: 0.1473  loss_dice_5: 1.226  loss_ce_6: 0.4034  loss_mask_6: 0.147  loss_dice_6: 1.2  loss_ce_7: 0.3965  loss_mask_7: 0.1473  loss_dice_7: 1.205  loss_ce_8: 0.3883  loss_mask_8: 0.1457  loss_dice_8: 1.188    time: 1.0487  last_time: 1.0388  data_time: 0.0568  last_data_time: 0.0564   lr: 0.0001  max_mem: 32952M
[10/19 12:18:56] d2.utils.events INFO:  eta: 0:04:51  iter: 2719  total_loss: 17.65  loss_ce: 0.3806  loss_mask: 0.1577  loss_dice: 1.136  loss_ce_0: 0.5457  loss_mask_0: 0.1874  loss_dice_0: 1.333  loss_ce_1: 0.5137  loss_mask_1: 0.1708  loss_dice_1: 1.286  loss_ce_2: 0.4908  loss_mask_2: 0.161  loss_dice_2: 1.208  loss_ce_3: 0.4405  loss_mask_3: 0.162  loss_dice_3: 1.147  loss_ce_4: 0.3836  loss_mask_4: 0.1621  loss_dice_4: 1.167  loss_ce_5: 0.3843  loss_mask_5: 0.1591  loss_dice_5: 1.155  loss_ce_6: 0.3891  loss_mask_6: 0.1573  loss_dice_6: 1.131  loss_ce_7: 0.3732  loss_mask_7: 0.1578  loss_dice_7: 1.146  loss_ce_8: 0.3733  loss_mask_8: 0.1582  loss_dice_8: 1.133    time: 1.0485  last_time: 1.0407  data_time: 0.0551  last_data_time: 0.0577   lr: 0.0001  max_mem: 32952M
[10/19 12:19:17] d2.utils.events INFO:  eta: 0:04:30  iter: 2739  total_loss: 18.74  loss_ce: 0.3956  loss_mask: 0.1611  loss_dice: 1.185  loss_ce_0: 0.5006  loss_mask_0: 0.1879  loss_dice_0: 1.368  loss_ce_1: 0.5227  loss_mask_1: 0.1716  loss_dice_1: 1.337  loss_ce_2: 0.4854  loss_mask_2: 0.1623  loss_dice_2: 1.237  loss_ce_3: 0.4418  loss_mask_3: 0.1624  loss_dice_3: 1.186  loss_ce_4: 0.4748  loss_mask_4: 0.1634  loss_dice_4: 1.154  loss_ce_5: 0.4024  loss_mask_5: 0.1617  loss_dice_5: 1.167  loss_ce_6: 0.4107  loss_mask_6: 0.1606  loss_dice_6: 1.158  loss_ce_7: 0.3884  loss_mask_7: 0.1611  loss_dice_7: 1.182  loss_ce_8: 0.401  loss_mask_8: 0.1613  loss_dice_8: 1.204    time: 1.0484  last_time: 1.0125  data_time: 0.0561  last_data_time: 0.0649   lr: 0.0001  max_mem: 32952M
[10/19 12:19:38] d2.utils.events INFO:  eta: 0:04:09  iter: 2759  total_loss: 18.74  loss_ce: 0.3908  loss_mask: 0.1466  loss_dice: 1.199  loss_ce_0: 0.5184  loss_mask_0: 0.1632  loss_dice_0: 1.36  loss_ce_1: 0.5411  loss_mask_1: 0.1589  loss_dice_1: 1.274  loss_ce_2: 0.4938  loss_mask_2: 0.1516  loss_dice_2: 1.247  loss_ce_3: 0.4673  loss_mask_3: 0.1497  loss_dice_3: 1.204  loss_ce_4: 0.4477  loss_mask_4: 0.148  loss_dice_4: 1.184  loss_ce_5: 0.408  loss_mask_5: 0.1474  loss_dice_5: 1.186  loss_ce_6: 0.4079  loss_mask_6: 0.1493  loss_dice_6: 1.178  loss_ce_7: 0.4111  loss_mask_7: 0.1483  loss_dice_7: 1.199  loss_ce_8: 0.4113  loss_mask_8: 0.1487  loss_dice_8: 1.175    time: 1.0484  last_time: 0.9896  data_time: 0.0570  last_data_time: 0.0557   lr: 0.0001  max_mem: 32952M
[10/19 12:19:59] d2.utils.events INFO:  eta: 0:03:48  iter: 2779  total_loss: 18.95  loss_ce: 0.4279  loss_mask: 0.1506  loss_dice: 1.178  loss_ce_0: 0.5695  loss_mask_0: 0.1668  loss_dice_0: 1.366  loss_ce_1: 0.5907  loss_mask_1: 0.1542  loss_dice_1: 1.314  loss_ce_2: 0.5367  loss_mask_2: 0.1487  loss_dice_2: 1.26  loss_ce_3: 0.4997  loss_mask_3: 0.1501  loss_dice_3: 1.217  loss_ce_4: 0.4632  loss_mask_4: 0.1496  loss_dice_4: 1.19  loss_ce_5: 0.452  loss_mask_5: 0.1469  loss_dice_5: 1.213  loss_ce_6: 0.459  loss_mask_6: 0.1481  loss_dice_6: 1.186  loss_ce_7: 0.4354  loss_mask_7: 0.1492  loss_dice_7: 1.212  loss_ce_8: 0.4175  loss_mask_8: 0.1488  loss_dice_8: 1.192    time: 1.0484  last_time: 1.0919  data_time: 0.0607  last_data_time: 0.0863   lr: 0.0001  max_mem: 32952M
[10/19 12:20:20] d2.utils.events INFO:  eta: 0:03:27  iter: 2799  total_loss: 18.36  loss_ce: 0.4181  loss_mask: 0.1517  loss_dice: 1.167  loss_ce_0: 0.5296  loss_mask_0: 0.1826  loss_dice_0: 1.375  loss_ce_1: 0.5524  loss_mask_1: 0.1657  loss_dice_1: 1.298  loss_ce_2: 0.5134  loss_mask_2: 0.1597  loss_dice_2: 1.252  loss_ce_3: 0.4689  loss_mask_3: 0.159  loss_dice_3: 1.178  loss_ce_4: 0.4469  loss_mask_4: 0.1576  loss_dice_4: 1.168  loss_ce_5: 0.4265  loss_mask_5: 0.1558  loss_dice_5: 1.198  loss_ce_6: 0.4203  loss_mask_6: 0.1518  loss_dice_6: 1.17  loss_ce_7: 0.4258  loss_mask_7: 0.1519  loss_dice_7: 1.155  loss_ce_8: 0.4032  loss_mask_8: 0.153  loss_dice_8: 1.174    time: 1.0482  last_time: 0.9970  data_time: 0.0534  last_data_time: 0.0460   lr: 0.0001  max_mem: 32952M
[10/19 14:02:14] detectron2 INFO: Rank of current process: 0. World size: 2
[10/19 14:02:16] detectron2 INFO: Environment info:
-------------------------------  -------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.8.19 | packaged by conda-forge | (default, Mar 20 2024, 12:47:35) [GCC 12.3.0]
numpy                            1.24.4
detectron2                       0.6 @/home/ids/gbrison/segmentation/segmentation/detectron2/detectron2
Compiler                         GCC 11.4
CUDA compiler                    CUDA 12.5
detectron2 arch flags            8.9
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.0+cu121 @/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0,1                          NVIDIA L40S (arch=8.9)
Driver version                   555.42.06
CUDA_HOME                        /usr/local/cuda
Pillow                           9.4.0
torchvision                      0.19.0+cu121 @/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torchvision
torchvision arch flags           5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.10.0
-------------------------------  -------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX512
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.0, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/19 14:02:16] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco/panoptic-segmentation/fcclip/r50_exp_3000_a_decoder_pruning_i_15_f_0_5.yaml', dist_url='tcp://127.0.0.1:51163', eval_only=False, machine_rank=0, num_gpus=2, num_machines=1, opts=[], resume=False)
[10/19 14:02:16] detectron2 INFO: Contents of args.config_file=configs/coco/panoptic-segmentation/fcclip/r50_exp_3000_a_decoder_pruning_i_15_f_0_5.yaml:
_BASE_: ./fcclip_convnext_large_eval_ade20k_r50.yaml

INPUT:
  MIN_SIZE_TEST: 1024
  MAX_SIZE_TEST: 2560

MODEL:
  SEM_SEG_HEAD:
    NUM_CLASSES: 19
  WEIGHTS: /tsi/hi-paris/GB/segmentation/models/fcclip_cocopan_r50.pth
OUTPUT_DIR: /tsi/hi-paris/GB/segmentation/results/In_vocab/r50_008_3000_19_a_decoder_pruning_i_15_f_0_5
DATASETS:
  TRAIN: ("openvocab_cityscapes_fine_panoptic_train",)
  TEST: ("openvocab_cityscapes_fine_panoptic_val",)
SOLVER:
  IMS_PER_BATCH: 8
  MAX_ITER: 3000
TEST:
  EVAL_PERIOD: 3000


[10/19 14:02:16] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - openvocab_cityscapes_fine_panoptic_val
  TRAIN:
  - openvocab_cityscapes_fine_panoptic_train
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: false
  CROP:
    ENABLED: false
    MINIMUM_INST_AREA: 1
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  DATASET_MAPPER_NAME: coco_panoptic_lsj
  FORMAT: RGB
  IMAGE_SIZE: 1024
  MASK_FORMAT: polygon
  MAX_SCALE: 2.0
  MAX_SIZE_TEST: 2560
  MAX_SIZE_TRAIN: 1333
  MIN_SCALE: 0.1
  MIN_SIZE_TEST: 1024
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: -1
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 0
    NAME: CLIP
  DEVICE: cuda
  FC_CLIP:
    CLIP_MODEL_NAME: RN50
    CLIP_PRETRAINED_WEIGHTS: openai
    EMBED_DIM: 1024
    ENSEMBLE_ON_VALID_MASK: true
    GEOMETRIC_ENSEMBLE_ALPHA: 0.4
    GEOMETRIC_ENSEMBLE_BETA: 0.8
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_FORMER:
    CLASS_WEIGHT: 2.0
    DEC_LAYERS: 10
    DEEP_SUPERVISION: true
    DICE_WEIGHT: 5.0
    DIM_FEEDFORWARD: 2048
    DROPOUT: 0.0
    ENC_LAYERS: 0
    ENFORCE_INPUT_PROJ: false
    HIDDEN_DIM: 256
    IMPORTANCE_SAMPLE_RATIO: 0.75
    MASK_WEIGHT: 5.0
    NHEADS: 8
    NO_OBJECT_WEIGHT: 0.1
    NUM_OBJECT_QUERIES: 250
    OVERSAMPLE_RATIO: 3.0
    PRE_NORM: false
    SIZE_DIVISIBILITY: 32
    TEST:
      INSTANCE_ON: true
      OBJECT_MASK_THRESHOLD: 0.0
      OVERLAP_THRESHOLD: 0.8
      PANOPTIC_ON: true
      SEMANTIC_ON: true
      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: false
    TRAIN_NUM_POINTS: 12544
    TRANSFORMER_DECODER_NAME: MultiScaleMaskedTransformerDecoder
    TRANSFORMER_IN_FEATURE: multi_scale_pixel_decoder
  MASK_ON: false
  META_ARCHITECTURE: FCCLIP
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 122.7709383
  - 116.7460125
  - 104.09373615
  PIXEL_STD:
  - 68.5005327
  - 66.6321579
  - 70.32316305
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 1
    - 1
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: basic
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 256
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES:
    - res3
    - res4
    - res5
    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8
    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    MASK_DIM: 256
    NAME: FCCLIPHead
    NORM: GN
    NUM_CLASSES: 19
    PIXEL_DECODER_NAME: MSDeformAttnPixelDecoder
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    TRANSFORMER_ENC_LAYERS: 6
    USE_DEPTHWISE_SEPARABLE_CONV: false
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    USE_CHECKPOINT: false
    WINDOW_SIZE: 7
  WEIGHTS: /tsi/hi-paris/GB/segmentation/models/fcclip_cocopan_r50.pth
OUTPUT_DIR: /tsi/hi-paris/GB/segmentation/results/In_vocab/r50_008_3000_19_a_decoder_pruning_i_15_f_0_5
SEED: -1
SOLVER:
  AMP:
    ENABLED: true
  BACKBONE_MULTIPLIER: 0.1
  BASE_LR: 0.0001
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 5000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.01
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 8
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 3000
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 327778
  - 355092
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 3000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[10/19 14:02:16] detectron2 INFO: Full config saved to /tsi/hi-paris/GB/segmentation/results/In_vocab/r50_008_3000_19_a_decoder_pruning_i_15_f_0_5/config.yaml
[10/19 14:02:16] d2.utils.env INFO: Using a generated random seed 18245771
[10/19 14:02:21] d2.engine.defaults INFO: Model:
FCCLIP(
  (backbone): CLIP(
    (clip_model): CLIP(
      (visual): ModifiedResNet(
        (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
        (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        (layer1): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
            (downsample): Sequential(
              (-1): AvgPool2d(kernel_size=1, stride=1, padding=0)
              (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
        )
        (layer2): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
            (downsample): Sequential(
              (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (3): Bottleneck(
            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
        )
        (layer3): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
            (downsample): Sequential(
              (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
              (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (3): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (4): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (5): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
        )
        (layer4): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
            (downsample): Sequential(
              (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
              (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
        )
        (attnpool): AttentionPool2d(
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (c_proj): Linear(in_features=2048, out_features=1024, bias=True)
        )
      )
      (transformer): Transformer(
        (resblocks): ModuleList(
          (0-11): 12 x ResidualAttentionBlock(
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ls_2): Identity()
          )
        )
      )
      (token_embedding): Embedding(49408, 512)
      (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
  )
  (sem_seg_head): FCCLIPHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0-8): 9 x SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0-8): 9 x CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0-8): 9 x FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(250, 256)
      (query_embed): Embedding(250, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0-2): 3 x Sequential()
      )
      (mask_embed): MLP(
        (layers): ModuleList(
          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (mask_pooling): MaskPooling()
      (_mask_pooling_proj): Sequential(
        (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
      )
      (class_embed): MLP(
        (layers): ModuleList(
          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=1024, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 19
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
  (conv1): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv2): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU()
  (mask_pooling): MaskPooling()
  (decoder_adapter): DecoderAdapter(
    (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
    (conv2): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
    (norm1): LayerNorm((128, 1, 1), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((256, 1, 1), eps=1e-05, elementwise_affine=True)
    (relu): ReLU()
  )
  (void_embedding): Embedding(1, 1024)
)
[10/19 14:02:21] fcclip.data.dataset_mappers.mask_former_semantic_dataset_mapper INFO: [MaskFormerPanopticDatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(800,), max_size=1333, sample_style='choice'), RandomFlip()]
[10/19 14:02:21] fcclip.data.datasets.register_cityscapes_panoptic INFO: 18 cities found in 'datasets/cityscapes/leftImg8bit/train'.
[10/19 14:02:21] d2.data.build INFO: Using training sampler TrainingSampler
[10/19 14:02:21] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/19 14:02:21] d2.data.common INFO: Serializing 2975 elements to byte tensors and concatenating them all ...
[10/19 14:02:21] d2.data.common INFO: Serialized dataset takes 4.12 MiB
[10/19 14:02:21] d2.data.build INFO: Making batched data loader with batch_size=4
[10/19 14:02:22] d2.solver.build WARNING: SOLVER.STEPS contains values larger than SOLVER.MAX_ITER. These values will be ignored.
[10/19 14:02:22] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /tsi/hi-paris/GB/segmentation/models/fcclip_cocopan_r50.pth ...
[10/19 14:02:22] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /tsi/hi-paris/GB/segmentation/models/fcclip_cocopan_r50.pth ...
[10/19 14:02:23] fvcore.common.checkpoint WARNING: Skip loading parameter 'criterion.empty_weight' to the model due to incompatible shapes: (134,) in the checkpoint but (20,) in the model! You might want to double check if this is expected.
[10/19 14:02:23] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.clip_model.ln_final.{bias, weight}[0m
[34mbackbone.clip_model.token_embedding.weight[0m
[34mbackbone.clip_model.transformer.resblocks.0.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.attnpool.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.attnpool.k_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.attnpool.positional_embedding[0m
[34mbackbone.clip_model.visual.attnpool.q_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.attnpool.v_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.conv1.weight[0m
[34mbackbone.clip_model.visual.conv2.weight[0m
[34mbackbone.clip_model.visual.conv3.weight[0m
[34mbackbone.clip_model.visual.layer1.0.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.0.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.0.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.0.conv1.weight[0m
[34mbackbone.clip_model.visual.layer1.0.conv2.weight[0m
[34mbackbone.clip_model.visual.layer1.0.conv3.weight[0m
[34mbackbone.clip_model.visual.layer1.0.downsample.0.weight[0m
[34mbackbone.clip_model.visual.layer1.0.downsample.1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.1.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.1.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.1.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.1.conv1.weight[0m
[34mbackbone.clip_model.visual.layer1.1.conv2.weight[0m
[34mbackbone.clip_model.visual.layer1.1.conv3.weight[0m
[34mbackbone.clip_model.visual.layer1.2.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.2.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.2.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.2.conv1.weight[0m
[34mbackbone.clip_model.visual.layer1.2.conv2.weight[0m
[34mbackbone.clip_model.visual.layer1.2.conv3.weight[0m
[34mbackbone.clip_model.visual.layer2.0.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.0.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.0.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.0.conv1.weight[0m
[34mbackbone.clip_model.visual.layer2.0.conv2.weight[0m
[34mbackbone.clip_model.visual.layer2.0.conv3.weight[0m
[34mbackbone.clip_model.visual.layer2.0.downsample.0.weight[0m
[34mbackbone.clip_model.visual.layer2.0.downsample.1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.1.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.1.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.1.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.1.conv1.weight[0m
[34mbackbone.clip_model.visual.layer2.1.conv2.weight[0m
[34mbackbone.clip_model.visual.layer2.1.conv3.weight[0m
[34mbackbone.clip_model.visual.layer2.2.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.2.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.2.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.2.conv1.weight[0m
[34mbackbone.clip_model.visual.layer2.2.conv2.weight[0m
[34mbackbone.clip_model.visual.layer2.2.conv3.weight[0m
[34mbackbone.clip_model.visual.layer2.3.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.3.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.3.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.3.conv1.weight[0m
[34mbackbone.clip_model.visual.layer2.3.conv2.weight[0m
[34mbackbone.clip_model.visual.layer2.3.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.0.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.0.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.0.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.0.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.0.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.0.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.0.downsample.0.weight[0m
[34mbackbone.clip_model.visual.layer3.0.downsample.1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.1.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.1.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.1.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.1.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.1.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.1.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.2.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.2.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.2.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.2.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.2.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.2.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.3.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.3.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.3.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.3.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.3.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.3.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.4.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.4.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.4.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.4.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.4.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.4.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.5.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.5.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.5.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.5.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.5.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.5.conv3.weight[0m
[34mbackbone.clip_model.visual.layer4.0.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.0.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.0.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.0.conv1.weight[0m
[34mbackbone.clip_model.visual.layer4.0.conv2.weight[0m
[34mbackbone.clip_model.visual.layer4.0.conv3.weight[0m
[34mbackbone.clip_model.visual.layer4.0.downsample.0.weight[0m
[34mbackbone.clip_model.visual.layer4.0.downsample.1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.1.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.1.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.1.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.1.conv1.weight[0m
[34mbackbone.clip_model.visual.layer4.1.conv2.weight[0m
[34mbackbone.clip_model.visual.layer4.1.conv3.weight[0m
[34mbackbone.clip_model.visual.layer4.2.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.2.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.2.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.2.conv1.weight[0m
[34mbackbone.clip_model.visual.layer4.2.conv2.weight[0m
[34mbackbone.clip_model.visual.layer4.2.conv3.weight[0m
[34mbackbone.clip_model.{logit_scale, positional_embedding, text_projection}[0m
[34mbn1.{bias, running_mean, running_var, weight}[0m
[34mbn2.{bias, running_mean, running_var, weight}[0m
[34mconv1.{bias, weight}[0m
[34mconv2.{bias, weight}[0m
[34mcriterion.empty_weight[0m
[34mdecoder_adapter.conv1.{bias, weight}[0m
[34mdecoder_adapter.conv2.{bias, weight}[0m
[34mdecoder_adapter.norm1.{bias, weight}[0m
[34mdecoder_adapter.norm2.{bias, weight}[0m
[10/19 14:02:23] d2.engine.train_loop INFO: Starting training from iteration 0
[10/19 14:02:32] d2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/home/ids/gbrison/segmentation/segmentation/detectron2/detectron2/engine/train_loop.py", line 155, in train
    self.run_step()
  File "/home/ids/gbrison/segmentation/segmentation/detectron2/detectron2/engine/defaults.py", line 498, in run_step
    self._trainer.run_step()
  File "/home/ids/gbrison/segmentation/segmentation/detectron2/detectron2/engine/train_loop.py", line 494, in run_step
    loss_dict = self.model(data)
  File "/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1632, in forward
    inputs, kwargs = self._pre_forward(*inputs, **kwargs)
  File "/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1523, in _pre_forward
    if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameter indices which did not receive grad for rank 0: 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316
 In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[10/19 14:02:32] d2.engine.hooks INFO: Total training time: 0:00:00 (0:00:00 on hooks)
[10/19 14:02:32] d2.utils.events INFO:  iter: 1  total_loss: 50.67  loss_ce: 3.131  loss_mask: 0.3281  loss_dice: 1.534  loss_ce_0: 3.247  loss_mask_0: 0.3337  loss_dice_0: 1.901  loss_ce_1: 3.343  loss_mask_1: 0.3212  loss_dice_1: 1.756  loss_ce_2: 3.074  loss_mask_2: 0.3293  loss_dice_2: 1.692  loss_ce_3: 2.926  loss_mask_3: 0.3327  loss_dice_3: 1.66  loss_ce_4: 3.089  loss_mask_4: 0.3311  loss_dice_4: 1.597  loss_ce_5: 3.057  loss_mask_5: 0.327  loss_dice_5: 1.596  loss_ce_6: 3.071  loss_mask_6: 0.3277  loss_dice_6: 1.561  loss_ce_7: 3.009  loss_mask_7: 0.3338  loss_dice_7: 1.528  loss_ce_8: 3.135  loss_mask_8: 0.3288  loss_dice_8: 1.466    data_time: 6.0101  last_data_time: 6.0101   lr: 0.0001  max_mem: 15504M
[10/19 14:05:43] detectron2 INFO: Rank of current process: 0. World size: 1
[10/19 14:05:44] detectron2 INFO: Environment info:
-------------------------------  -------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.8.19 | packaged by conda-forge | (default, Mar 20 2024, 12:47:35) [GCC 12.3.0]
numpy                            1.24.4
detectron2                       0.6 @/home/ids/gbrison/segmentation/segmentation/detectron2/detectron2
Compiler                         GCC 11.4
CUDA compiler                    CUDA 12.5
detectron2 arch flags            8.9
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.0+cu121 @/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0,1                          NVIDIA L40S (arch=8.9)
Driver version                   555.42.06
CUDA_HOME                        /usr/local/cuda
Pillow                           9.4.0
torchvision                      0.19.0+cu121 @/home/ids/gbrison/segmentation/miniconda3/envs/fc/lib/python3.8/site-packages/torchvision
torchvision arch flags           5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.10.0
-------------------------------  -------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX512
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.0, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/19 14:05:44] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco/panoptic-segmentation/fcclip/r50_exp_3000_a_decoder_pruning_i_15_f_0_5.yaml', dist_url='tcp://127.0.0.1:51163', eval_only=False, machine_rank=0, num_gpus=1, num_machines=1, opts=[], resume=False)
[10/19 14:05:44] detectron2 INFO: Contents of args.config_file=configs/coco/panoptic-segmentation/fcclip/r50_exp_3000_a_decoder_pruning_i_15_f_0_5.yaml:
_BASE_: ./fcclip_convnext_large_eval_ade20k_r50.yaml

INPUT:
  MIN_SIZE_TEST: 1024
  MAX_SIZE_TEST: 2560

MODEL:
  SEM_SEG_HEAD:
    NUM_CLASSES: 19
  WEIGHTS: /tsi/hi-paris/GB/segmentation/models/fcclip_cocopan_r50.pth
OUTPUT_DIR: /tsi/hi-paris/GB/segmentation/results/In_vocab/r50_008_3000_19_a_decoder_pruning_i_15_f_0_5
DATASETS:
  TRAIN: ("openvocab_cityscapes_fine_panoptic_train",)
  TEST: ("openvocab_cityscapes_fine_panoptic_val",)
SOLVER:
  IMS_PER_BATCH: 8
  MAX_ITER: 3000
TEST:
  EVAL_PERIOD: 3000


[10/19 14:05:44] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - openvocab_cityscapes_fine_panoptic_val
  TRAIN:
  - openvocab_cityscapes_fine_panoptic_train
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: false
  CROP:
    ENABLED: false
    MINIMUM_INST_AREA: 1
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  DATASET_MAPPER_NAME: coco_panoptic_lsj
  FORMAT: RGB
  IMAGE_SIZE: 1024
  MASK_FORMAT: polygon
  MAX_SCALE: 2.0
  MAX_SIZE_TEST: 2560
  MAX_SIZE_TRAIN: 1333
  MIN_SCALE: 0.1
  MIN_SIZE_TEST: 1024
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: -1
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 0
    NAME: CLIP
  DEVICE: cuda
  FC_CLIP:
    CLIP_MODEL_NAME: RN50
    CLIP_PRETRAINED_WEIGHTS: openai
    EMBED_DIM: 1024
    ENSEMBLE_ON_VALID_MASK: true
    GEOMETRIC_ENSEMBLE_ALPHA: 0.4
    GEOMETRIC_ENSEMBLE_BETA: 0.8
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_FORMER:
    CLASS_WEIGHT: 2.0
    DEC_LAYERS: 10
    DEEP_SUPERVISION: true
    DICE_WEIGHT: 5.0
    DIM_FEEDFORWARD: 2048
    DROPOUT: 0.0
    ENC_LAYERS: 0
    ENFORCE_INPUT_PROJ: false
    HIDDEN_DIM: 256
    IMPORTANCE_SAMPLE_RATIO: 0.75
    MASK_WEIGHT: 5.0
    NHEADS: 8
    NO_OBJECT_WEIGHT: 0.1
    NUM_OBJECT_QUERIES: 250
    OVERSAMPLE_RATIO: 3.0
    PRE_NORM: false
    SIZE_DIVISIBILITY: 32
    TEST:
      INSTANCE_ON: true
      OBJECT_MASK_THRESHOLD: 0.0
      OVERLAP_THRESHOLD: 0.8
      PANOPTIC_ON: true
      SEMANTIC_ON: true
      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: false
    TRAIN_NUM_POINTS: 12544
    TRANSFORMER_DECODER_NAME: MultiScaleMaskedTransformerDecoder
    TRANSFORMER_IN_FEATURE: multi_scale_pixel_decoder
  MASK_ON: false
  META_ARCHITECTURE: FCCLIP
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 122.7709383
  - 116.7460125
  - 104.09373615
  PIXEL_STD:
  - 68.5005327
  - 66.6321579
  - 70.32316305
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 1
    - 1
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: basic
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 256
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES:
    - res3
    - res4
    - res5
    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8
    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    MASK_DIM: 256
    NAME: FCCLIPHead
    NORM: GN
    NUM_CLASSES: 19
    PIXEL_DECODER_NAME: MSDeformAttnPixelDecoder
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    TRANSFORMER_ENC_LAYERS: 6
    USE_DEPTHWISE_SEPARABLE_CONV: false
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    USE_CHECKPOINT: false
    WINDOW_SIZE: 7
  WEIGHTS: /tsi/hi-paris/GB/segmentation/models/fcclip_cocopan_r50.pth
OUTPUT_DIR: /tsi/hi-paris/GB/segmentation/results/In_vocab/r50_008_3000_19_a_decoder_pruning_i_15_f_0_5
SEED: -1
SOLVER:
  AMP:
    ENABLED: true
  BACKBONE_MULTIPLIER: 0.1
  BASE_LR: 0.0001
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 5000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.01
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 8
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 3000
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 327778
  - 355092
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 3000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[10/19 14:05:44] detectron2 INFO: Full config saved to /tsi/hi-paris/GB/segmentation/results/In_vocab/r50_008_3000_19_a_decoder_pruning_i_15_f_0_5/config.yaml
[10/19 14:05:44] d2.utils.env INFO: Using a generated random seed 47023044
[10/19 14:05:49] d2.engine.defaults INFO: Model:
FCCLIP(
  (backbone): CLIP(
    (clip_model): CLIP(
      (visual): ModifiedResNet(
        (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): ReLU(inplace=True)
        (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
        (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
        (layer1): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
            (downsample): Sequential(
              (-1): AvgPool2d(kernel_size=1, stride=1, padding=0)
              (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
        )
        (layer2): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
            (downsample): Sequential(
              (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (3): Bottleneck(
            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
        )
        (layer3): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
            (downsample): Sequential(
              (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
              (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (3): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (4): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (5): Bottleneck(
            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
        )
        (layer4): Sequential(
          (0): Bottleneck(
            (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
            (downsample): Sequential(
              (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
              (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): Bottleneck(
            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
          (2): Bottleneck(
            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act1): ReLU(inplace=True)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act2): ReLU(inplace=True)
            (avgpool): Identity()
            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act3): ReLU(inplace=True)
          )
        )
        (attnpool): AttentionPool2d(
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (c_proj): Linear(in_features=2048, out_features=1024, bias=True)
        )
      )
      (transformer): Transformer(
        (resblocks): ModuleList(
          (0-11): 12 x ResidualAttentionBlock(
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ls_2): Identity()
          )
        )
      )
      (token_embedding): Embedding(49408, 512)
      (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
  )
  (sem_seg_head): FCCLIPHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0-8): 9 x SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0-8): 9 x CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0-8): 9 x FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(250, 256)
      (query_embed): Embedding(250, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0-2): 3 x Sequential()
      )
      (mask_embed): MLP(
        (layers): ModuleList(
          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (mask_pooling): MaskPooling()
      (_mask_pooling_proj): Sequential(
        (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
      )
      (class_embed): MLP(
        (layers): ModuleList(
          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=1024, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 19
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
  (conv1): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv2): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU()
  (mask_pooling): MaskPooling()
  (decoder_adapter): DecoderAdapter(
    (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
    (conv2): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
    (norm1): LayerNorm((128, 1, 1), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((256, 1, 1), eps=1e-05, elementwise_affine=True)
    (relu): ReLU()
  )
  (void_embedding): Embedding(1, 1024)
)
[10/19 14:05:49] fcclip.data.dataset_mappers.mask_former_semantic_dataset_mapper INFO: [MaskFormerPanopticDatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(800,), max_size=1333, sample_style='choice'), RandomFlip()]
[10/19 14:05:49] fcclip.data.datasets.register_cityscapes_panoptic INFO: 18 cities found in 'datasets/cityscapes/leftImg8bit/train'.
[10/19 14:05:49] d2.data.build INFO: Using training sampler TrainingSampler
[10/19 14:05:49] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/19 14:05:49] d2.data.common INFO: Serializing 2975 elements to byte tensors and concatenating them all ...
[10/19 14:05:49] d2.data.common INFO: Serialized dataset takes 4.12 MiB
[10/19 14:05:49] d2.data.build INFO: Making batched data loader with batch_size=8
[10/19 14:05:49] d2.solver.build WARNING: SOLVER.STEPS contains values larger than SOLVER.MAX_ITER. These values will be ignored.
[10/19 14:05:49] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /tsi/hi-paris/GB/segmentation/models/fcclip_cocopan_r50.pth ...
[10/19 14:05:49] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /tsi/hi-paris/GB/segmentation/models/fcclip_cocopan_r50.pth ...
[10/19 14:05:51] fvcore.common.checkpoint WARNING: Skip loading parameter 'criterion.empty_weight' to the model due to incompatible shapes: (134,) in the checkpoint but (20,) in the model! You might want to double check if this is expected.
[10/19 14:05:51] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.clip_model.ln_final.{bias, weight}[0m
[34mbackbone.clip_model.token_embedding.weight[0m
[34mbackbone.clip_model.transformer.resblocks.0.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.0.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.1.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.10.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.11.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.2.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.3.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.4.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.5.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.6.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.7.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.8.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.attn.out_proj.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.attn.{in_proj_bias, in_proj_weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.ln_1.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.ln_2.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.mlp.c_fc.{bias, weight}[0m
[34mbackbone.clip_model.transformer.resblocks.9.mlp.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.attnpool.c_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.attnpool.k_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.attnpool.positional_embedding[0m
[34mbackbone.clip_model.visual.attnpool.q_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.attnpool.v_proj.{bias, weight}[0m
[34mbackbone.clip_model.visual.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.conv1.weight[0m
[34mbackbone.clip_model.visual.conv2.weight[0m
[34mbackbone.clip_model.visual.conv3.weight[0m
[34mbackbone.clip_model.visual.layer1.0.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.0.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.0.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.0.conv1.weight[0m
[34mbackbone.clip_model.visual.layer1.0.conv2.weight[0m
[34mbackbone.clip_model.visual.layer1.0.conv3.weight[0m
[34mbackbone.clip_model.visual.layer1.0.downsample.0.weight[0m
[34mbackbone.clip_model.visual.layer1.0.downsample.1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.1.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.1.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.1.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.1.conv1.weight[0m
[34mbackbone.clip_model.visual.layer1.1.conv2.weight[0m
[34mbackbone.clip_model.visual.layer1.1.conv3.weight[0m
[34mbackbone.clip_model.visual.layer1.2.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.2.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.2.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer1.2.conv1.weight[0m
[34mbackbone.clip_model.visual.layer1.2.conv2.weight[0m
[34mbackbone.clip_model.visual.layer1.2.conv3.weight[0m
[34mbackbone.clip_model.visual.layer2.0.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.0.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.0.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.0.conv1.weight[0m
[34mbackbone.clip_model.visual.layer2.0.conv2.weight[0m
[34mbackbone.clip_model.visual.layer2.0.conv3.weight[0m
[34mbackbone.clip_model.visual.layer2.0.downsample.0.weight[0m
[34mbackbone.clip_model.visual.layer2.0.downsample.1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.1.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.1.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.1.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.1.conv1.weight[0m
[34mbackbone.clip_model.visual.layer2.1.conv2.weight[0m
[34mbackbone.clip_model.visual.layer2.1.conv3.weight[0m
[34mbackbone.clip_model.visual.layer2.2.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.2.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.2.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.2.conv1.weight[0m
[34mbackbone.clip_model.visual.layer2.2.conv2.weight[0m
[34mbackbone.clip_model.visual.layer2.2.conv3.weight[0m
[34mbackbone.clip_model.visual.layer2.3.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.3.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.3.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer2.3.conv1.weight[0m
[34mbackbone.clip_model.visual.layer2.3.conv2.weight[0m
[34mbackbone.clip_model.visual.layer2.3.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.0.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.0.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.0.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.0.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.0.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.0.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.0.downsample.0.weight[0m
[34mbackbone.clip_model.visual.layer3.0.downsample.1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.1.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.1.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.1.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.1.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.1.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.1.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.2.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.2.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.2.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.2.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.2.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.2.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.3.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.3.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.3.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.3.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.3.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.3.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.4.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.4.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.4.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.4.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.4.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.4.conv3.weight[0m
[34mbackbone.clip_model.visual.layer3.5.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.5.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.5.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer3.5.conv1.weight[0m
[34mbackbone.clip_model.visual.layer3.5.conv2.weight[0m
[34mbackbone.clip_model.visual.layer3.5.conv3.weight[0m
[34mbackbone.clip_model.visual.layer4.0.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.0.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.0.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.0.conv1.weight[0m
[34mbackbone.clip_model.visual.layer4.0.conv2.weight[0m
[34mbackbone.clip_model.visual.layer4.0.conv3.weight[0m
[34mbackbone.clip_model.visual.layer4.0.downsample.0.weight[0m
[34mbackbone.clip_model.visual.layer4.0.downsample.1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.1.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.1.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.1.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.1.conv1.weight[0m
[34mbackbone.clip_model.visual.layer4.1.conv2.weight[0m
[34mbackbone.clip_model.visual.layer4.1.conv3.weight[0m
[34mbackbone.clip_model.visual.layer4.2.bn1.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.2.bn2.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.2.bn3.{bias, num_batches_tracked, running_mean, running_var, weight}[0m
[34mbackbone.clip_model.visual.layer4.2.conv1.weight[0m
[34mbackbone.clip_model.visual.layer4.2.conv2.weight[0m
[34mbackbone.clip_model.visual.layer4.2.conv3.weight[0m
[34mbackbone.clip_model.{logit_scale, positional_embedding, text_projection}[0m
[34mbn1.{bias, running_mean, running_var, weight}[0m
[34mbn2.{bias, running_mean, running_var, weight}[0m
[34mconv1.{bias, weight}[0m
[34mconv2.{bias, weight}[0m
[34mcriterion.empty_weight[0m
[34mdecoder_adapter.conv1.{bias, weight}[0m
[34mdecoder_adapter.conv2.{bias, weight}[0m
[34mdecoder_adapter.norm1.{bias, weight}[0m
[34mdecoder_adapter.norm2.{bias, weight}[0m
[10/19 14:05:51] d2.engine.train_loop INFO: Starting training from iteration 0
[10/19 14:06:16] d2.utils.events INFO:  eta: 0:53:12  iter: 19  total_loss: 30.87  loss_ce: 1.124  loss_mask: 0.2846  loss_dice: 1.514  loss_ce_0: 1.329  loss_mask_0: 0.3019  loss_dice_0: 1.835  loss_ce_1: 1.357  loss_mask_1: 0.2962  loss_dice_1: 1.694  loss_ce_2: 1.266  loss_mask_2: 0.2835  loss_dice_2: 1.606  loss_ce_3: 1.219  loss_mask_3: 0.2781  loss_dice_3: 1.572  loss_ce_4: 1.18  loss_mask_4: 0.2828  loss_dice_4: 1.54  loss_ce_5: 1.139  loss_mask_5: 0.2842  loss_dice_5: 1.552  loss_ce_6: 1.14  loss_mask_6: 0.2905  loss_dice_6: 1.519  loss_ce_7: 1.124  loss_mask_7: 0.2856  loss_dice_7: 1.507  loss_ce_8: 1.113  loss_mask_8: 0.2801  loss_dice_8: 1.506    time: 1.0838  last_time: 1.0949  data_time: 0.1253  last_data_time: 0.0739   lr: 0.0001  max_mem: 31545M
[10/19 14:06:38] d2.utils.events INFO:  eta: 0:53:14  iter: 39  total_loss: 25.43  loss_ce: 0.7509  loss_mask: 0.2312  loss_dice: 1.391  loss_ce_0: 0.8406  loss_mask_0: 0.2712  loss_dice_0: 1.682  loss_ce_1: 0.9261  loss_mask_1: 0.2487  loss_dice_1: 1.55  loss_ce_2: 0.8328  loss_mask_2: 0.2339  loss_dice_2: 1.468  loss_ce_3: 0.7876  loss_mask_3: 0.2287  loss_dice_3: 1.438  loss_ce_4: 0.7866  loss_mask_4: 0.2315  loss_dice_4: 1.419  loss_ce_5: 0.7832  loss_mask_5: 0.2302  loss_dice_5: 1.417  loss_ce_6: 0.7746  loss_mask_6: 0.2278  loss_dice_6: 1.419  loss_ce_7: 0.7611  loss_mask_7: 0.2278  loss_dice_7: 1.403  loss_ce_8: 0.7736  loss_mask_8: 0.2306  loss_dice_8: 1.363    time: 1.0829  last_time: 1.0514  data_time: 0.0751  last_data_time: 0.0740   lr: 0.0001  max_mem: 31655M
[10/19 14:07:00] d2.utils.events INFO:  eta: 0:53:11  iter: 59  total_loss: 24.25  loss_ce: 0.6541  loss_mask: 0.2306  loss_dice: 1.382  loss_ce_0: 0.7239  loss_mask_0: 0.2518  loss_dice_0: 1.68  loss_ce_1: 0.822  loss_mask_1: 0.2407  loss_dice_1: 1.527  loss_ce_2: 0.7488  loss_mask_2: 0.2368  loss_dice_2: 1.473  loss_ce_3: 0.7081  loss_mask_3: 0.2274  loss_dice_3: 1.421  loss_ce_4: 0.6865  loss_mask_4: 0.2342  loss_dice_4: 1.429  loss_ce_5: 0.6945  loss_mask_5: 0.2287  loss_dice_5: 1.413  loss_ce_6: 0.6682  loss_mask_6: 0.2324  loss_dice_6: 1.338  loss_ce_7: 0.6697  loss_mask_7: 0.2307  loss_dice_7: 1.398  loss_ce_8: 0.6607  loss_mask_8: 0.232  loss_dice_8: 1.377    time: 1.0885  last_time: 1.0603  data_time: 0.0791  last_data_time: 0.0672   lr: 0.0001  max_mem: 31892M
[10/19 14:07:23] d2.utils.events INFO:  eta: 0:53:07  iter: 79  total_loss: 24.14  loss_ce: 0.718  loss_mask: 0.1955  loss_dice: 1.367  loss_ce_0: 0.7425  loss_mask_0: 0.2304  loss_dice_0: 1.697  loss_ce_1: 0.8766  loss_mask_1: 0.2131  loss_dice_1: 1.534  loss_ce_2: 0.7994  loss_mask_2: 0.2049  loss_dice_2: 1.49  loss_ce_3: 0.7578  loss_mask_3: 0.2035  loss_dice_3: 1.401  loss_ce_4: 0.7218  loss_mask_4: 0.2028  loss_dice_4: 1.425  loss_ce_5: 0.7146  loss_mask_5: 0.2095  loss_dice_5: 1.424  loss_ce_6: 0.7165  loss_mask_6: 0.2011  loss_dice_6: 1.384  loss_ce_7: 0.7093  loss_mask_7: 0.2003  loss_dice_7: 1.402  loss_ce_8: 0.7009  loss_mask_8: 0.1961  loss_dice_8: 1.402    time: 1.0958  last_time: 1.1942  data_time: 0.0785  last_data_time: 0.0964   lr: 0.0001  max_mem: 31892M
[10/19 14:07:45] d2.utils.events INFO:  eta: 0:52:53  iter: 99  total_loss: 22.24  loss_ce: 0.5964  loss_mask: 0.204  loss_dice: 1.311  loss_ce_0: 0.6448  loss_mask_0: 0.2246  loss_dice_0: 1.646  loss_ce_1: 0.8009  loss_mask_1: 0.2131  loss_dice_1: 1.468  loss_ce_2: 0.6681  loss_mask_2: 0.2039  loss_dice_2: 1.422  loss_ce_3: 0.6568  loss_mask_3: 0.2058  loss_dice_3: 1.35  loss_ce_4: 0.6005  loss_mask_4: 0.2064  loss_dice_4: 1.333  loss_ce_5: 0.5852  loss_mask_5: 0.2066  loss_dice_5: 1.354  loss_ce_6: 0.5792  loss_mask_6: 0.2007  loss_dice_6: 1.314  loss_ce_7: 0.574  loss_mask_7: 0.2023  loss_dice_7: 1.302  loss_ce_8: 0.5862  loss_mask_8: 0.2038  loss_dice_8: 1.337    time: 1.0977  last_time: 1.1360  data_time: 0.0744  last_data_time: 0.1174   lr: 0.0001  max_mem: 31892M
[10/19 14:08:07] d2.utils.events INFO:  eta: 0:52:32  iter: 119  total_loss: 22.65  loss_ce: 0.6318  loss_mask: 0.2134  loss_dice: 1.314  loss_ce_0: 0.6702  loss_mask_0: 0.2425  loss_dice_0: 1.608  loss_ce_1: 0.7831  loss_mask_1: 0.2233  loss_dice_1: 1.504  loss_ce_2: 0.7207  loss_mask_2: 0.2135  loss_dice_2: 1.447  loss_ce_3: 0.6706  loss_mask_3: 0.2146  loss_dice_3: 1.375  loss_ce_4: 0.6424  loss_mask_4: 0.2126  loss_dice_4: 1.372  loss_ce_5: 0.6192  loss_mask_5: 0.2126  loss_dice_5: 1.398  loss_ce_6: 0.6491  loss_mask_6: 0.214  loss_dice_6: 1.325  loss_ce_7: 0.6115  loss_mask_7: 0.2141  loss_dice_7: 1.328  loss_ce_8: 0.6179  loss_mask_8: 0.2138  loss_dice_8: 1.318    time: 1.0986  last_time: 1.1048  data_time: 0.0769  last_data_time: 0.0638   lr: 0.0001  max_mem: 31892M
[10/19 14:08:29] d2.utils.events INFO:  eta: 0:52:13  iter: 139  total_loss: 22.46  loss_ce: 0.5875  loss_mask: 0.2063  loss_dice: 1.4  loss_ce_0: 0.6836  loss_mask_0: 0.2383  loss_dice_0: 1.553  loss_ce_1: 0.7681  loss_mask_1: 0.2167  loss_dice_1: 1.478  loss_ce_2: 0.6828  loss_mask_2: 0.2092  loss_dice_2: 1.398  loss_ce_3: 0.6793  loss_mask_3: 0.2086  loss_dice_3: 1.336  loss_ce_4: 0.6257  loss_mask_4: 0.2078  loss_dice_4: 1.347  loss_ce_5: 0.614  loss_mask_5: 0.2072  loss_dice_5: 1.357  loss_ce_6: 0.6021  loss_mask_6: 0.2082  loss_dice_6: 1.311  loss_ce_7: 0.5879  loss_mask_7: 0.2095  loss_dice_7: 1.332  loss_ce_8: 0.5962  loss_mask_8: 0.2075  loss_dice_8: 1.352    time: 1.1017  last_time: 1.1543  data_time: 0.0775  last_data_time: 0.1003   lr: 0.0001  max_mem: 31892M
[10/19 14:08:52] d2.utils.events INFO:  eta: 0:52:10  iter: 159  total_loss: 22.25  loss_ce: 0.5913  loss_mask: 0.1955  loss_dice: 1.339  loss_ce_0: 0.6673  loss_mask_0: 0.2191  loss_dice_0: 1.541  loss_ce_1: 0.799  loss_mask_1: 0.204  loss_dice_1: 1.47  loss_ce_2: 0.6967  loss_mask_2: 0.1959  loss_dice_2: 1.377  loss_ce_3: 0.6388  loss_mask_3: 0.1958  loss_dice_3: 1.356  loss_ce_4: 0.5927  loss_mask_4: 0.1907  loss_dice_4: 1.333  loss_ce_5: 0.6326  loss_mask_5: 0.1904  loss_dice_5: 1.324  loss_ce_6: 0.6138  loss_mask_6: 0.1907  loss_dice_6: 1.304  loss_ce_7: 0.5936  loss_mask_7: 0.1921  loss_dice_7: 1.295  loss_ce_8: 0.5761  loss_mask_8: 0.1925  loss_dice_8: 1.337    time: 1.1039  last_time: 1.1146  data_time: 0.0761  last_data_time: 0.0794   lr: 0.0001  max_mem: 31892M
[10/19 14:09:14] d2.utils.events INFO:  eta: 0:51:44  iter: 179  total_loss: 21  loss_ce: 0.5547  loss_mask: 0.2123  loss_dice: 1.251  loss_ce_0: 0.5993  loss_mask_0: 0.2276  loss_dice_0: 1.49  loss_ce_1: 0.7142  loss_mask_1: 0.2141  loss_dice_1: 1.356  loss_ce_2: 0.642  loss_mask_2: 0.208  loss_dice_2: 1.32  loss_ce_3: 0.6185  loss_mask_3: 0.2068  loss_dice_3: 1.266  loss_ce_4: 0.5727  loss_mask_4: 0.2069  loss_dice_4: 1.312  loss_ce_5: 0.5734  loss_mask_5: 0.2094  loss_dice_5: 1.241  loss_ce_6: 0.5655  loss_mask_6: 0.2099  loss_dice_6: 1.234  loss_ce_7: 0.5811  loss_mask_7: 0.2103  loss_dice_7: 1.284  loss_ce_8: 0.5616  loss_mask_8: 0.2104  loss_dice_8: 1.249    time: 1.1034  last_time: 1.0705  data_time: 0.0741  last_data_time: 0.0649   lr: 0.0001  max_mem: 31892M
[10/19 14:09:36] d2.utils.events INFO:  eta: 0:51:22  iter: 199  total_loss: 21.67  loss_ce: 0.5714  loss_mask: 0.1978  loss_dice: 1.311  loss_ce_0: 0.6555  loss_mask_0: 0.2484  loss_dice_0: 1.533  loss_ce_1: 0.7282  loss_mask_1: 0.2221  loss_dice_1: 1.441  loss_ce_2: 0.6657  loss_mask_2: 0.2108  loss_dice_2: 1.407  loss_ce_3: 0.6385  loss_mask_3: 0.1988  loss_dice_3: 1.323  loss_ce_4: 0.5661  loss_mask_4: 0.2013  loss_dice_4: 1.332  loss_ce_5: 0.6017  loss_mask_5: 0.1937  loss_dice_5: 1.308  loss_ce_6: 0.5749  loss_mask_6: 0.1933  loss_dice_6: 1.334  loss_ce_7: 0.5861  loss_mask_7: 0.1927  loss_dice_7: 1.308  loss_ce_8: 0.5448  loss_mask_8: 0.1956  loss_dice_8: 1.311    time: 1.1036  last_time: 1.1727  data_time: 0.0779  last_data_time: 0.0648   lr: 0.0001  max_mem: 32630M
[10/19 14:09:59] d2.utils.events INFO:  eta: 0:51:05  iter: 219  total_loss: 23.28  loss_ce: 0.6271  loss_mask: 0.1906  loss_dice: 1.33  loss_ce_0: 0.6669  loss_mask_0: 0.2164  loss_dice_0: 1.678  loss_ce_1: 0.7814  loss_mask_1: 0.2078  loss_dice_1: 1.574  loss_ce_2: 0.7373  loss_mask_2: 0.1982  loss_dice_2: 1.458  loss_ce_3: 0.6777  loss_mask_3: 0.1976  loss_dice_3: 1.41  loss_ce_4: 0.628  loss_mask_4: 0.1943  loss_dice_4: 1.456  loss_ce_5: 0.6457  loss_mask_5: 0.191  loss_dice_5: 1.391  loss_ce_6: 0.6382  loss_mask_6: 0.1879  loss_dice_6: 1.361  loss_ce_7: 0.609  loss_mask_7: 0.1891  loss_dice_7: 1.385  loss_ce_8: 0.6102  loss_mask_8: 0.1905  loss_dice_8: 1.394    time: 1.1058  last_time: 1.1477  data_time: 0.0854  last_data_time: 0.1012   lr: 0.0001  max_mem: 32630M
[10/19 14:10:21] d2.utils.events INFO:  eta: 0:50:42  iter: 239  total_loss: 22.4  loss_ce: 0.6146  loss_mask: 0.2037  loss_dice: 1.334  loss_ce_0: 0.6745  loss_mask_0: 0.2304  loss_dice_0: 1.594  loss_ce_1: 0.7581  loss_mask_1: 0.2146  loss_dice_1: 1.525  loss_ce_2: 0.6903  loss_mask_2: 0.2071  loss_dice_2: 1.428  loss_ce_3: 0.6352  loss_mask_3: 0.2041  loss_dice_3: 1.386  loss_ce_4: 0.6257  loss_mask_4: 0.2065  loss_dice_4: 1.353  loss_ce_5: 0.6195  loss_mask_5: 0.2051  loss_dice_5: 1.353  loss_ce_6: 0.5926  loss_mask_6: 0.2047  loss_dice_6: 1.346  loss_ce_7: 0.5811  loss_mask_7: 0.2054  loss_dice_7: 1.337  loss_ce_8: 0.5875  loss_mask_8: 0.2054  loss_dice_8: 1.34    time: 1.1050  last_time: 1.1396  data_time: 0.0818  last_data_time: 0.0893   lr: 0.0001  max_mem: 32630M
[10/19 14:10:43] d2.utils.events INFO:  eta: 0:50:19  iter: 259  total_loss: 21.32  loss_ce: 0.5519  loss_mask: 0.1978  loss_dice: 1.303  loss_ce_0: 0.6499  loss_mask_0: 0.2297  loss_dice_0: 1.554  loss_ce_1: 0.7266  loss_mask_1: 0.2083  loss_dice_1: 1.436  loss_ce_2: 0.6248  loss_mask_2: 0.2038  loss_dice_2: 1.367  loss_ce_3: 0.6039  loss_mask_3: 0.2016  loss_dice_3: 1.296  loss_ce_4: 0.5887  loss_mask_4: 0.1985  loss_dice_4: 1.286  loss_ce_5: 0.5619  loss_mask_5: 0.1967  loss_dice_5: 1.324  loss_ce_6: 0.5581  loss_mask_6: 0.1977  loss_dice_6: 1.296  loss_ce_7: 0.5526  loss_mask_7: 0.1979  loss_dice_7: 1.271  loss_ce_8: 0.5636  loss_mask_8: 0.199  loss_dice_8: 1.255    time: 1.1049  last_time: 1.1035  data_time: 0.0786  last_data_time: 0.0999   lr: 0.0001  max_mem: 32630M
[10/19 14:11:05] d2.utils.events INFO:  eta: 0:49:56  iter: 279  total_loss: 20.42  loss_ce: 0.5474  loss_mask: 0.1859  loss_dice: 1.201  loss_ce_0: 0.6244  loss_mask_0: 0.2164  loss_dice_0: 1.467  loss_ce_1: 0.7384  loss_mask_1: 0.2026  loss_dice_1: 1.36  loss_ce_2: 0.6297  loss_mask_2: 0.1872  loss_dice_2: 1.296  loss_ce_3: 0.5821  loss_mask_3: 0.1893  loss_dice_3: 1.216  loss_ce_4: 0.572  loss_mask_4: 0.19  loss_dice_4: 1.228  loss_ce_5: 0.6002  loss_mask_5: 0.1884  loss_dice_5: 1.244  loss_ce_6: 0.5587  loss_mask_6: 0.1882  loss_dice_6: 1.2  loss_ce_7: 0.5507  loss_mask_7: 0.1894  loss_dice_7: 1.226  loss_ce_8: 0.5548  loss_mask_8: 0.1864  loss_dice_8: 1.229    time: 1.1040  last_time: 1.1227  data_time: 0.0785  last_data_time: 0.0881   lr: 0.0001  max_mem: 32630M
[10/19 14:11:27] d2.utils.events INFO:  eta: 0:49:34  iter: 299  total_loss: 20.61  loss_ce: 0.5777  loss_mask: 0.178  loss_dice: 1.254  loss_ce_0: 0.6131  loss_mask_0: 0.2065  loss_dice_0: 1.526  loss_ce_1: 0.7009  loss_mask_1: 0.1925  loss_dice_1: 1.364  loss_ce_2: 0.6571  loss_mask_2: 0.1819  loss_dice_2: 1.401  loss_ce_3: 0.5979  loss_mask_3: 0.1792  loss_dice_3: 1.293  loss_ce_4: 0.59  loss_mask_4: 0.1794  loss_dice_4: 1.285  loss_ce_5: 0.588  loss_mask_5: 0.1785  loss_dice_5: 1.228  loss_ce_6: 0.5638  loss_mask_6: 0.1782  loss_dice_6: 1.265  loss_ce_7: 0.5614  loss_mask_7: 0.1804  loss_dice_7: 1.283  loss_ce_8: 0.5562  loss_mask_8: 0.1778  loss_dice_8: 1.229    time: 1.1049  last_time: 1.0904  data_time: 0.0847  last_data_time: 0.0967   lr: 0.0001  max_mem: 32630M
[10/19 14:11:49] d2.utils.events INFO:  eta: 0:49:13  iter: 319  total_loss: 20.92  loss_ce: 0.5456  loss_mask: 0.1876  loss_dice: 1.254  loss_ce_0: 0.645  loss_mask_0: 0.2156  loss_dice_0: 1.559  loss_ce_1: 0.7217  loss_mask_1: 0.2026  loss_dice_1: 1.458  loss_ce_2: 0.637  loss_mask_2: 0.1941  loss_dice_2: 1.387  loss_ce_3: 0.581  loss_mask_3: 0.1851  loss_dice_3: 1.303  loss_ce_4: 0.5615  loss_mask_4: 0.1918  loss_dice_4: 1.334  loss_ce_5: 0.5575  loss_mask_5: 0.1891  loss_dice_5: 1.321  loss_ce_6: 0.5856  loss_mask_6: 0.1852  loss_dice_6: 1.286  loss_ce_7: 0.5638  loss_mask_7: 0.1886  loss_dice_7: 1.294  loss_ce_8: 0.5692  loss_mask_8: 0.188  loss_dice_8: 1.285    time: 1.1053  last_time: 1.0823  data_time: 0.0804  last_data_time: 0.0684   lr: 0.0001  max_mem: 32630M
[10/19 14:12:11] d2.utils.events INFO:  eta: 0:48:49  iter: 339  total_loss: 22.03  loss_ce: 0.5423  loss_mask: 0.2014  loss_dice: 1.282  loss_ce_0: 0.6372  loss_mask_0: 0.2274  loss_dice_0: 1.561  loss_ce_1: 0.6972  loss_mask_1: 0.2135  loss_dice_1: 1.465  loss_ce_2: 0.634  loss_mask_2: 0.2056  loss_dice_2: 1.364  loss_ce_3: 0.5845  loss_mask_3: 0.2002  loss_dice_3: 1.331  loss_ce_4: 0.5767  loss_mask_4: 0.202  loss_dice_4: 1.34  loss_ce_5: 0.5985  loss_mask_5: 0.2021  loss_dice_5: 1.332  loss_ce_6: 0.5529  loss_mask_6: 0.2009  loss_dice_6: 1.312  loss_ce_7: 0.5585  loss_mask_7: 0.2006  loss_dice_7: 1.272  loss_ce_8: 0.5575  loss_mask_8: 0.1998  loss_dice_8: 1.302    time: 1.1046  last_time: 1.1013  data_time: 0.0785  last_data_time: 0.0884   lr: 0.0001  max_mem: 32630M
[10/19 14:12:33] d2.utils.events INFO:  eta: 0:48:24  iter: 359  total_loss: 21.81  loss_ce: 0.5952  loss_mask: 0.186  loss_dice: 1.264  loss_ce_0: 0.6593  loss_mask_0: 0.2104  loss_dice_0: 1.524  loss_ce_1: 0.7178  loss_mask_1: 0.2028  loss_dice_1: 1.412  loss_ce_2: 0.6491  loss_mask_2: 0.1902  loss_dice_2: 1.364  loss_ce_3: 0.5801  loss_mask_3: 0.1916  loss_dice_3: 1.341  loss_ce_4: 0.5977  loss_mask_4: 0.1904  loss_dice_4: 1.315  loss_ce_5: 0.5698  loss_mask_5: 0.1891  loss_dice_5: 1.313  loss_ce_6: 0.5932  loss_mask_6: 0.1918  loss_dice_6: 1.267  loss_ce_7: 0.5784  loss_mask_7: 0.19  loss_dice_7: 1.292  loss_ce_8: 0.57  loss_mask_8: 0.1882  loss_dice_8: 1.29    time: 1.1034  last_time: 1.1558  data_time: 0.0745  last_data_time: 0.0822   lr: 0.0001  max_mem: 32630M
[10/19 14:12:55] d2.utils.events INFO:  eta: 0:48:02  iter: 379  total_loss: 21.09  loss_ce: 0.51  loss_mask: 0.1871  loss_dice: 1.248  loss_ce_0: 0.6032  loss_mask_0: 0.2188  loss_dice_0: 1.506  loss_ce_1: 0.682  loss_mask_1: 0.1999  loss_dice_1: 1.432  loss_ce_2: 0.6264  loss_mask_2: 0.1918  loss_dice_2: 1.344  loss_ce_3: 0.5714  loss_mask_3: 0.1857  loss_dice_3: 1.272  loss_ce_4: 0.562  loss_mask_4: 0.1881  loss_dice_4: 1.282  loss_ce_5: 0.5414  loss_mask_5: 0.1858  loss_dice_5: 1.3  loss_ce_6: 0.5278  loss_mask_6: 0.1908  loss_dice_6: 1.282  loss_ce_7: 0.5243  loss_mask_7: 0.1883  loss_dice_7: 1.285  loss_ce_8: 0.5348  loss_mask_8: 0.1868  loss_dice_8: 1.264    time: 1.1035  last_time: 1.1713  data_time: 0.0842  last_data_time: 0.0958   lr: 0.0001  max_mem: 32630M
[10/19 14:13:18] d2.utils.events INFO:  eta: 0:47:43  iter: 399  total_loss: 20.97  loss_ce: 0.518  loss_mask: 0.177  loss_dice: 1.273  loss_ce_0: 0.6239  loss_mask_0: 0.2083  loss_dice_0: 1.486  loss_ce_1: 0.6684  loss_mask_1: 0.1888  loss_dice_1: 1.416  loss_ce_2: 0.5977  loss_mask_2: 0.1832  loss_dice_2: 1.372  loss_ce_3: 0.5681  loss_mask_3: 0.1816  loss_dice_3: 1.3  loss_ce_4: 0.5646  loss_mask_4: 0.1802  loss_dice_4: 1.267  loss_ce_5: 0.5496  loss_mask_5: 0.1793  loss_dice_5: 1.275  loss_ce_6: 0.5453  loss_mask_6: 0.1774  loss_dice_6: 1.282  loss_ce_7: 0.5499  loss_mask_7: 0.1769  loss_dice_7: 1.215  loss_ce_8: 0.4991  loss_mask_8: 0.1802  loss_dice_8: 1.267    time: 1.1044  last_time: 1.0851  data_time: 0.0849  last_data_time: 0.0626   lr: 0.0001  max_mem: 32630M
[10/19 14:13:40] d2.utils.events INFO:  eta: 0:47:22  iter: 419  total_loss: 20.99  loss_ce: 0.5259  loss_mask: 0.1872  loss_dice: 1.247  loss_ce_0: 0.6536  loss_mask_0: 0.2201  loss_dice_0: 1.52  loss_ce_1: 0.6738  loss_mask_1: 0.2029  loss_dice_1: 1.464  loss_ce_2: 0.621  loss_mask_2: 0.1937  loss_dice_2: 1.418  loss_ce_3: 0.5578  loss_mask_3: 0.1945  loss_dice_3: 1.372  loss_ce_4: 0.5395  loss_mask_4: 0.193  loss_dice_4: 1.326  loss_ce_5: 0.522  loss_mask_5: 0.1863  loss_dice_5: 1.316  loss_ce_6: 0.5266  loss_mask_6: 0.1857  loss_dice_6: 1.276  loss_ce_7: 0.5032  loss_mask_7: 0.1829  loss_dice_7: 1.316  loss_ce_8: 0.5435  loss_mask_8: 0.1836  loss_dice_8: 1.29    time: 1.1049  last_time: 1.1630  data_time: 0.0810  last_data_time: 0.1110   lr: 0.0001  max_mem: 32630M
[10/19 14:14:02] d2.utils.events INFO:  eta: 0:46:59  iter: 439  total_loss: 20.34  loss_ce: 0.4893  loss_mask: 0.1842  loss_dice: 1.226  loss_ce_0: 0.5727  loss_mask_0: 0.2124  loss_dice_0: 1.453  loss_ce_1: 0.6583  loss_mask_1: 0.1968  loss_dice_1: 1.363  loss_ce_2: 0.6137  loss_mask_2: 0.1901  loss_dice_2: 1.286  loss_ce_3: 0.5525  loss_mask_3: 0.1891  loss_dice_3: 1.25  loss_ce_4: 0.5341  loss_mask_4: 0.1862  loss_dice_4: 1.242  loss_ce_5: 0.5295  loss_mask_5: 0.1861  loss_dice_5: 1.226  loss_ce_6: 0.5168  loss_mask_6: 0.1881  loss_dice_6: 1.224  loss_ce_7: 0.4838  loss_mask_7: 0.1856  loss_dice_7: 1.188  loss_ce_8: 0.4762  loss_mask_8: 0.1827  loss_dice_8: 1.217    time: 1.1040  last_time: 1.0328  data_time: 0.0758  last_data_time: 0.0743   lr: 0.0001  max_mem: 32630M
[10/19 14:14:24] d2.utils.events INFO:  eta: 0:46:37  iter: 459  total_loss: 20.62  loss_ce: 0.5122  loss_mask: 0.1809  loss_dice: 1.237  loss_ce_0: 0.5601  loss_mask_0: 0.2168  loss_dice_0: 1.526  loss_ce_1: 0.6705  loss_mask_1: 0.2009  loss_dice_1: 1.404  loss_ce_2: 0.58  loss_mask_2: 0.1952  loss_dice_2: 1.335  loss_ce_3: 0.5175  loss_mask_3: 0.1881  loss_dice_3: 1.271  loss_ce_4: 0.5413  loss_mask_4: 0.187  loss_dice_4: 1.307  loss_ce_5: 0.506  loss_mask_5: 0.1825  loss_dice_5: 1.277  loss_ce_6: 0.4762  loss_mask_6: 0.1841  loss_dice_6: 1.245  loss_ce_7: 0.4698  loss_mask_7: 0.1839  loss_dice_7: 1.236  loss_ce_8: 0.4816  loss_mask_8: 0.1806  loss_dice_8: 1.248    time: 1.1044  last_time: 1.0865  data_time: 0.0895  last_data_time: 0.0709   lr: 0.0001  max_mem: 32630M
[10/19 14:14:46] d2.utils.events INFO:  eta: 0:46:12  iter: 479  total_loss: 20.56  loss_ce: 0.5382  loss_mask: 0.1851  loss_dice: 1.244  loss_ce_0: 0.5868  loss_mask_0: 0.2142  loss_dice_0: 1.483  loss_ce_1: 0.6683  loss_mask_1: 0.1978  loss_dice_1: 1.4  loss_ce_2: 0.6119  loss_mask_2: 0.1905  loss_dice_2: 1.325  loss_ce_3: 0.5867  loss_mask_3: 0.1918  loss_dice_3: 1.266  loss_ce_4: 0.5365  loss_mask_4: 0.1917  loss_dice_4: 1.277  loss_ce_5: 0.5376  loss_mask_5: 0.1855  loss_dice_5: 1.276  loss_ce_6: 0.5096  loss_mask_6: 0.1882  loss_dice_6: 1.254  loss_ce_7: 0.5267  loss_mask_7: 0.1886  loss_dice_7: 1.247  loss_ce_8: 0.515  loss_mask_8: 0.1855  loss_dice_8: 1.269    time: 1.1037  last_time: 1.1434  data_time: 0.0729  last_data_time: 0.0861   lr: 0.0001  max_mem: 32630M
[10/19 14:15:08] d2.utils.events INFO:  eta: 0:45:53  iter: 499  total_loss: 21.35  loss_ce: 0.5443  loss_mask: 0.1786  loss_dice: 1.286  loss_ce_0: 0.6292  loss_mask_0: 0.2067  loss_dice_0: 1.554  loss_ce_1: 0.7072  loss_mask_1: 0.1954  loss_dice_1: 1.465  loss_ce_2: 0.6391  loss_mask_2: 0.1859  loss_dice_2: 1.387  loss_ce_3: 0.5972  loss_mask_3: 0.1844  loss_dice_3: 1.326  loss_ce_4: 0.5889  loss_mask_4: 0.1832  loss_dice_4: 1.302  loss_ce_5: 0.5662  loss_mask_5: 0.1835  loss_dice_5: 1.306  loss_ce_6: 0.5517  loss_mask_6: 0.1814  loss_dice_6: 1.247  loss_ce_7: 0.5462  loss_mask_7: 0.1798  loss_dice_7: 1.291  loss_ce_8: 0.5714  loss_mask_8: 0.1808  loss_dice_8: 1.259    time: 1.1045  last_time: 1.1205  data_time: 0.0847  last_data_time: 0.0847   lr: 0.0001  max_mem: 32630M
[10/19 14:15:31] d2.utils.events INFO:  eta: 0:45:31  iter: 519  total_loss: 20.7  loss_ce: 0.5228  loss_mask: 0.1822  loss_dice: 1.243  loss_ce_0: 0.5987  loss_mask_0: 0.2203  loss_dice_0: 1.47  loss_ce_1: 0.7002  loss_mask_1: 0.2041  loss_dice_1: 1.374  loss_ce_2: 0.6363  loss_mask_2: 0.1916  loss_dice_2: 1.337  loss_ce_3: 0.5862  loss_mask_3: 0.1907  loss_dice_3: 1.312  loss_ce_4: 0.5491  loss_mask_4: 0.1877  loss_dice_4: 1.288  loss_ce_5: 0.5546  loss_mask_5: 0.1869  loss_dice_5: 1.299  loss_ce_6: 0.5373  loss_mask_6: 0.1847  loss_dice_6: 1.306  loss_ce_7: 0.5487  loss_mask_7: 0.184  loss_dice_7: 1.287  loss_ce_8: 0.5153  loss_mask_8: 0.1817  loss_dice_8: 1.249    time: 1.1050  last_time: 1.2309  data_time: 0.0877  last_data_time: 0.0945   lr: 0.0001  max_mem: 32630M
[10/19 14:15:53] d2.utils.events INFO:  eta: 0:45:10  iter: 539  total_loss: 21.43  loss_ce: 0.5451  loss_mask: 0.1714  loss_dice: 1.324  loss_ce_0: 0.6068  loss_mask_0: 0.1934  loss_dice_0: 1.546  loss_ce_1: 0.6549  loss_mask_1: 0.1906  loss_dice_1: 1.461  loss_ce_2: 0.6253  loss_mask_2: 0.1705  loss_dice_2: 1.42  loss_ce_3: 0.5493  loss_mask_3: 0.172  loss_dice_3: 1.413  loss_ce_4: 0.5108  loss_mask_4: 0.1738  loss_dice_4: 1.353  loss_ce_5: 0.5237  loss_mask_5: 0.17  loss_dice_5: 1.388  loss_ce_6: 0.5172  loss_mask_6: 0.1709  loss_dice_6: 1.326  loss_ce_7: 0.5323  loss_mask_7: 0.1711  loss_dice_7: 1.334  loss_ce_8: 0.5198  loss_mask_8: 0.171  loss_dice_8: 1.348    time: 1.1056  last_time: 1.2508  data_time: 0.0808  last_data_time: 0.0662   lr: 0.0001  max_mem: 32630M
[10/19 14:16:16] d2.utils.events INFO:  eta: 0:44:48  iter: 559  total_loss: 20.72  loss_ce: 0.5092  loss_mask: 0.1706  loss_dice: 1.284  loss_ce_0: 0.5616  loss_mask_0: 0.2049  loss_dice_0: 1.529  loss_ce_1: 0.6665  loss_mask_1: 0.1856  loss_dice_1: 1.414  loss_ce_2: 0.6156  loss_mask_2: 0.1739  loss_dice_2: 1.332  loss_ce_3: 0.5616  loss_mask_3: 0.1738  loss_dice_3: 1.298  loss_ce_4: 0.4922  loss_mask_4: 0.1727  loss_dice_4: 1.303  loss_ce_5: 0.5359  loss_mask_5: 0.1732  loss_dice_5: 1.306  loss_ce_6: 0.5231  loss_mask_6: 0.1742  loss_dice_6: 1.267  loss_ce_7: 0.5142  loss_mask_7: 0.1734  loss_dice_7: 1.283  loss_ce_8: 0.5155  loss_mask_8: 0.1704  loss_dice_8: 1.262    time: 1.1058  last_time: 1.0757  data_time: 0.0803  last_data_time: 0.0838   lr: 0.0001  max_mem: 32630M
[10/19 14:16:38] d2.utils.events INFO:  eta: 0:44:28  iter: 579  total_loss: 20.97  loss_ce: 0.531  loss_mask: 0.178  loss_dice: 1.233  loss_ce_0: 0.6222  loss_mask_0: 0.2165  loss_dice_0: 1.495  loss_ce_1: 0.7116  loss_mask_1: 0.1948  loss_dice_1: 1.434  loss_ce_2: 0.6712  loss_mask_2: 0.1828  loss_dice_2: 1.346  loss_ce_3: 0.6175  loss_mask_3: 0.1777  loss_dice_3: 1.289  loss_ce_4: 0.5927  loss_mask_4: 0.1798  loss_dice_4: 1.292  loss_ce_5: 0.5761  loss_mask_5: 0.1756  loss_dice_5: 1.278  loss_ce_6: 0.551  loss_mask_6: 0.1781  loss_dice_6: 1.253  loss_ce_7: 0.5599  loss_mask_7: 0.1757  loss_dice_7: 1.274  loss_ce_8: 0.5818  loss_mask_8: 0.1773  loss_dice_8: 1.25    time: 1.1066  last_time: 1.1353  data_time: 0.0833  last_data_time: 0.0678   lr: 0.0001  max_mem: 32630M
[10/19 14:17:01] d2.utils.events INFO:  eta: 0:44:06  iter: 599  total_loss: 19.98  loss_ce: 0.4599  loss_mask: 0.1755  loss_dice: 1.211  loss_ce_0: 0.5869  loss_mask_0: 0.2065  loss_dice_0: 1.46  loss_ce_1: 0.6342  loss_mask_1: 0.1916  loss_dice_1: 1.361  loss_ce_2: 0.5762  loss_mask_2: 0.1817  loss_dice_2: 1.296  loss_ce_3: 0.5053  loss_mask_3: 0.1831  loss_dice_3: 1.226  loss_ce_4: 0.4979  loss_mask_4: 0.1816  loss_dice_4: 1.235  loss_ce_5: 0.49  loss_mask_5: 0.1753  loss_dice_5: 1.247  loss_ce_6: 0.4841  loss_mask_6: 0.1759  loss_dice_6: 1.249  loss_ce_7: 0.4518  loss_mask_7: 0.1767  loss_dice_7: 1.205  loss_ce_8: 0.4796  loss_mask_8: 0.1785  loss_dice_8: 1.203    time: 1.1073  last_time: 1.1025  data_time: 0.0814  last_data_time: 0.0649   lr: 0.0001  max_mem: 32630M
[10/19 14:17:23] d2.utils.events INFO:  eta: 0:43:44  iter: 619  total_loss: 20.57  loss_ce: 0.5291  loss_mask: 0.1878  loss_dice: 1.21  loss_ce_0: 0.6184  loss_mask_0: 0.2206  loss_dice_0: 1.462  loss_ce_1: 0.6941  loss_mask_1: 0.2015  loss_dice_1: 1.38  loss_ce_2: 0.6438  loss_mask_2: 0.1949  loss_dice_2: 1.314  loss_ce_3: 0.5804  loss_mask_3: 0.1935  loss_dice_3: 1.283  loss_ce_4: 0.559  loss_mask_4: 0.1946  loss_dice_4: 1.251  loss_ce_5: 0.5341  loss_mask_5: 0.1894  loss_dice_5: 1.245  loss_ce_6: 0.5155  loss_mask_6: 0.1864  loss_dice_6: 1.226  loss_ce_7: 0.5282  loss_mask_7: 0.1882  loss_dice_7: 1.243  loss_ce_8: 0.5203  loss_mask_8: 0.1903  loss_dice_8: 1.228    time: 1.1070  last_time: 1.0993  data_time: 0.0757  last_data_time: 0.0627   lr: 0.0001  max_mem: 32630M
[10/19 14:17:45] d2.utils.events INFO:  eta: 0:43:22  iter: 639  total_loss: 20.2  loss_ce: 0.5356  loss_mask: 0.1716  loss_dice: 1.204  loss_ce_0: 0.6118  loss_mask_0: 0.204  loss_dice_0: 1.474  loss_ce_1: 0.6407  loss_mask_1: 0.1858  loss_dice_1: 1.391  loss_ce_2: 0.6293  loss_mask_2: 0.1784  loss_dice_2: 1.331  loss_ce_3: 0.564  loss_mask_3: 0.1751  loss_dice_3: 1.25  loss_ce_4: 0.537  loss_mask_4: 0.171  loss_dice_4: 1.277  loss_ce_5: 0.5157  loss_mask_5: 0.1704  loss_dice_5: 1.272  loss_ce_6: 0.5497  loss_mask_6: 0.1704  loss_dice_6: 1.226  loss_ce_7: 0.5185  loss_mask_7: 0.1721  loss_dice_7: 1.207  loss_ce_8: 0.5328  loss_mask_8: 0.17  loss_dice_8: 1.214    time: 1.1071  last_time: 1.0202  data_time: 0.0793  last_data_time: 0.0614   lr: 0.0001  max_mem: 32630M
[10/19 14:18:07] d2.utils.events INFO:  eta: 0:43:01  iter: 659  total_loss: 20.43  loss_ce: 0.5187  loss_mask: 0.1801  loss_dice: 1.217  loss_ce_0: 0.6042  loss_mask_0: 0.2125  loss_dice_0: 1.463  loss_ce_1: 0.665  loss_mask_1: 0.1987  loss_dice_1: 1.399  loss_ce_2: 0.6105  loss_mask_2: 0.1928  loss_dice_2: 1.337  loss_ce_3: 0.5569  loss_mask_3: 0.189  loss_dice_3: 1.253  loss_ce_4: 0.5103  loss_mask_4: 0.1858  loss_dice_4: 1.286  loss_ce_5: 0.5324  loss_mask_5: 0.181  loss_dice_5: 1.293  loss_ce_6: 0.5171  loss_mask_6: 0.1805  loss_dice_6: 1.215  loss_ce_7: 0.4995  loss_mask_7: 0.1808  loss_dice_7: 1.241  loss_ce_8: 0.5346  loss_mask_8: 0.1788  loss_dice_8: 1.254    time: 1.1072  last_time: 1.1721  data_time: 0.0809  last_data_time: 0.0933   lr: 0.0001  max_mem: 32630M
[10/19 14:18:30] d2.utils.events INFO:  eta: 0:42:40  iter: 679  total_loss: 20.5  loss_ce: 0.4832  loss_mask: 0.1744  loss_dice: 1.251  loss_ce_0: 0.5871  loss_mask_0: 0.1996  loss_dice_0: 1.492  loss_ce_1: 0.6413  loss_mask_1: 0.1871  loss_dice_1: 1.415  loss_ce_2: 0.6325  loss_mask_2: 0.1755  loss_dice_2: 1.347  loss_ce_3: 0.5497  loss_mask_3: 0.1744  loss_dice_3: 1.315  loss_ce_4: 0.5109  loss_mask_4: 0.1781  loss_dice_4: 1.317  loss_ce_5: 0.5191  loss_mask_5: 0.1747  loss_dice_5: 1.292  loss_ce_6: 0.5169  loss_mask_6: 0.1745  loss_dice_6: 1.27  loss_ce_7: 0.5032  loss_mask_7: 0.1752  loss_dice_7: 1.298  loss_ce_8: 0.5167  loss_mask_8: 0.1744  loss_dice_8: 1.234    time: 1.1075  last_time: 1.0572  data_time: 0.0818  last_data_time: 0.0828   lr: 0.0001  max_mem: 32630M
[10/19 14:18:52] d2.utils.events INFO:  eta: 0:42:19  iter: 699  total_loss: 20.76  loss_ce: 0.5353  loss_mask: 0.1755  loss_dice: 1.242  loss_ce_0: 0.6546  loss_mask_0: 0.1992  loss_dice_0: 1.47  loss_ce_1: 0.6622  loss_mask_1: 0.1848  loss_dice_1: 1.394  loss_ce_2: 0.5785  loss_mask_2: 0.1812  loss_dice_2: 1.32  loss_ce_3: 0.548  loss_mask_3: 0.1803  loss_dice_3: 1.281  loss_ce_4: 0.5458  loss_mask_4: 0.1804  loss_dice_4: 1.26  loss_ce_5: 0.5086  loss_mask_5: 0.1777  loss_dice_5: 1.278  loss_ce_6: 0.537  loss_mask_6: 0.1742  loss_dice_6: 1.225  loss_ce_7: 0.5164  loss_mask_7: 0.175  loss_dice_7: 1.26  loss_ce_8: 0.5226  loss_mask_8: 0.1755  loss_dice_8: 1.22    time: 1.1078  last_time: 1.1287  data_time: 0.0792  last_data_time: 0.1014   lr: 0.0001  max_mem: 32630M
[10/19 14:19:15] d2.utils.events INFO:  eta: 0:41:58  iter: 719  total_loss: 21.62  loss_ce: 0.5089  loss_mask: 0.172  loss_dice: 1.294  loss_ce_0: 0.6339  loss_mask_0: 0.2065  loss_dice_0: 1.549  loss_ce_1: 0.6567  loss_mask_1: 0.185  loss_dice_1: 1.426  loss_ce_2: 0.6173  loss_mask_2: 0.1817  loss_dice_2: 1.37  loss_ce_3: 0.5467  loss_mask_3: 0.1778  loss_dice_3: 1.343  loss_ce_4: 0.5726  loss_mask_4: 0.1738  loss_dice_4: 1.339  loss_ce_5: 0.5042  loss_mask_5: 0.1729  loss_dice_5: 1.353  loss_ce_6: 0.5049  loss_mask_6: 0.171  loss_dice_6: 1.281  loss_ce_7: 0.5172  loss_mask_7: 0.1721  loss_dice_7: 1.324  loss_ce_8: 0.5102  loss_mask_8: 0.1724  loss_dice_8: 1.295    time: 1.1083  last_time: 1.1116  data_time: 0.0918  last_data_time: 0.0839   lr: 0.0001  max_mem: 32630M
[10/19 14:19:37] d2.utils.events INFO:  eta: 0:41:37  iter: 739  total_loss: 19.29  loss_ce: 0.4804  loss_mask: 0.1616  loss_dice: 1.175  loss_ce_0: 0.5926  loss_mask_0: 0.1887  loss_dice_0: 1.407  loss_ce_1: 0.6599  loss_mask_1: 0.1713  loss_dice_1: 1.338  loss_ce_2: 0.5908  loss_mask_2: 0.1636  loss_dice_2: 1.251  loss_ce_3: 0.516  loss_mask_3: 0.165  loss_dice_3: 1.222  loss_ce_4: 0.4954  loss_mask_4: 0.1665  loss_dice_4: 1.212  loss_ce_5: 0.4991  loss_mask_5: 0.1642  loss_dice_5: 1.194  loss_ce_6: 0.4913  loss_mask_6: 0.1623  loss_dice_6: 1.191  loss_ce_7: 0.4806  loss_mask_7: 0.1619  loss_dice_7: 1.196  loss_ce_8: 0.4725  loss_mask_8: 0.1627  loss_dice_8: 1.216    time: 1.1084  last_time: 1.1059  data_time: 0.0832  last_data_time: 0.0668   lr: 0.0001  max_mem: 32630M
[10/19 14:20:00] d2.utils.events INFO:  eta: 0:41:19  iter: 759  total_loss: 20.34  loss_ce: 0.5135  loss_mask: 0.1952  loss_dice: 1.232  loss_ce_0: 0.5948  loss_mask_0: 0.218  loss_dice_0: 1.406  loss_ce_1: 0.6746  loss_mask_1: 0.2046  loss_dice_1: 1.403  loss_ce_2: 0.6005  loss_mask_2: 0.2009  loss_dice_2: 1.313  loss_ce_3: 0.5569  loss_mask_3: 0.196  loss_dice_3: 1.239  loss_ce_4: 0.5156  loss_mask_4: 0.1988  loss_dice_4: 1.258  loss_ce_5: 0.5355  loss_mask_5: 0.1967  loss_dice_5: 1.231  loss_ce_6: 0.5096  loss_mask_6: 0.1952  loss_dice_6: 1.242  loss_ce_7: 0.5099  loss_mask_7: 0.1937  loss_dice_7: 1.21  loss_ce_8: 0.5121  loss_mask_8: 0.1938  loss_dice_8: 1.216    time: 1.1089  last_time: 1.1005  data_time: 0.0861  last_data_time: 0.0830   lr: 0.0001  max_mem: 32630M
[10/19 14:20:23] d2.utils.events INFO:  eta: 0:40:58  iter: 779  total_loss: 20.77  loss_ce: 0.473  loss_mask: 0.1668  loss_dice: 1.329  loss_ce_0: 0.5922  loss_mask_0: 0.1891  loss_dice_0: 1.526  loss_ce_1: 0.6732  loss_mask_1: 0.1699  loss_dice_1: 1.437  loss_ce_2: 0.6017  loss_mask_2: 0.1693  loss_dice_2: 1.417  loss_ce_3: 0.5318  loss_mask_3: 0.1713  loss_dice_3: 1.338  loss_ce_4: 0.5042  loss_mask_4: 0.1744  loss_dice_4: 1.337  loss_ce_5: 0.5067  loss_mask_5: 0.1717  loss_dice_5: 1.331  loss_ce_6: 0.4873  loss_mask_6: 0.1689  loss_dice_6: 1.306  loss_ce_7: 0.4902  loss_mask_7: 0.1679  loss_dice_7: 1.323  loss_ce_8: 0.4885  loss_mask_8: 0.1697  loss_dice_8: 1.328    time: 1.1097  last_time: 1.1403  data_time: 0.0854  last_data_time: 0.1091   lr: 0.0001  max_mem: 32630M
[10/19 14:20:45] d2.utils.events INFO:  eta: 0:40:36  iter: 799  total_loss: 19.61  loss_ce: 0.4539  loss_mask: 0.1791  loss_dice: 1.176  loss_ce_0: 0.5708  loss_mask_0: 0.2112  loss_dice_0: 1.369  loss_ce_1: 0.649  loss_mask_1: 0.1895  loss_dice_1: 1.334  loss_ce_2: 0.5696  loss_mask_2: 0.1885  loss_dice_2: 1.29  loss_ce_3: 0.5365  loss_mask_3: 0.1855  loss_dice_3: 1.204  loss_ce_4: 0.4999  loss_mask_4: 0.182  loss_dice_4: 1.194  loss_ce_5: 0.5149  loss_mask_5: 0.1818  loss_dice_5: 1.181  loss_ce_6: 0.4722  loss_mask_6: 0.1768  loss_dice_6: 1.179  loss_ce_7: 0.4877  loss_mask_7: 0.1785  loss_dice_7: 1.167  loss_ce_8: 0.4953  loss_mask_8: 0.1783  loss_dice_8: 1.192    time: 1.1100  last_time: 1.1225  data_time: 0.0832  last_data_time: 0.0995   lr: 0.0001  max_mem: 32630M
[10/19 14:21:07] d2.utils.events INFO:  eta: 0:40:14  iter: 819  total_loss: 20.27  loss_ce: 0.4927  loss_mask: 0.1705  loss_dice: 1.211  loss_ce_0: 0.5924  loss_mask_0: 0.1984  loss_dice_0: 1.508  loss_ce_1: 0.6465  loss_mask_1: 0.1832  loss_dice_1: 1.356  loss_ce_2: 0.603  loss_mask_2: 0.176  loss_dice_2: 1.292  loss_ce_3: 0.5415  loss_mask_3: 0.1757  loss_dice_3: 1.262  loss_ce_4: 0.4978  loss_mask_4: 0.1726  loss_dice_4: 1.229  loss_ce_5: 0.4851  loss_mask_5: 0.173  loss_dice_5: 1.241  loss_ce_6: 0.5058  loss_mask_6: 0.1713  loss_dice_6: 1.251  loss_ce_7: 0.5014  loss_mask_7: 0.1739  loss_dice_7: 1.256  loss_ce_8: 0.4959  loss_mask_8: 0.172  loss_dice_8: 1.259    time: 1.1102  last_time: 1.1692  data_time: 0.0854  last_data_time: 0.0839   lr: 0.0001  max_mem: 32630M
[10/19 14:21:30] d2.utils.events INFO:  eta: 0:39:52  iter: 839  total_loss: 19.89  loss_ce: 0.4813  loss_mask: 0.1782  loss_dice: 1.228  loss_ce_0: 0.572  loss_mask_0: 0.1999  loss_dice_0: 1.458  loss_ce_1: 0.6329  loss_mask_1: 0.1855  loss_dice_1: 1.382  loss_ce_2: 0.6054  loss_mask_2: 0.179  loss_dice_2: 1.3  loss_ce_3: 0.5376  loss_mask_3: 0.1794  loss_dice_3: 1.236  loss_ce_4: 0.5062  loss_mask_4: 0.1787  loss_dice_4: 1.266  loss_ce_5: 0.475  loss_mask_5: 0.1754  loss_dice_5: 1.275  loss_ce_6: 0.4913  loss_mask_6: 0.1748  loss_dice_6: 1.228  loss_ce_7: 0.4851  loss_mask_7: 0.1755  loss_dice_7: 1.262  loss_ce_8: 0.4491  loss_mask_8: 0.1744  loss_dice_8: 1.22    time: 1.1103  last_time: 1.1720  data_time: 0.0863  last_data_time: 0.0864   lr: 0.0001  max_mem: 32630M
[10/19 14:21:52] d2.utils.events INFO:  eta: 0:39:30  iter: 859  total_loss: 20.22  loss_ce: 0.4971  loss_mask: 0.1595  loss_dice: 1.25  loss_ce_0: 0.5852  loss_mask_0: 0.1954  loss_dice_0: 1.448  loss_ce_1: 0.6912  loss_mask_1: 0.1778  loss_dice_1: 1.403  loss_ce_2: 0.5843  loss_mask_2: 0.1651  loss_dice_2: 1.318  loss_ce_3: 0.5488  loss_mask_3: 0.1581  loss_dice_3: 1.265  loss_ce_4: 0.5443  loss_mask_4: 0.1623  loss_dice_4: 1.22  loss_ce_5: 0.4835  loss_mask_5: 0.1628  loss_dice_5: 1.282  loss_ce_6: 0.4765  loss_mask_6: 0.1604  loss_dice_6: 1.238  loss_ce_7: 0.4835  loss_mask_7: 0.1587  loss_dice_7: 1.238  loss_ce_8: 0.4981  loss_mask_8: 0.1594  loss_dice_8: 1.252    time: 1.1103  last_time: 1.1186  data_time: 0.0827  last_data_time: 0.0688   lr: 0.0001  max_mem: 32630M
[10/19 14:22:14] d2.utils.events INFO:  eta: 0:39:08  iter: 879  total_loss: 19.93  loss_ce: 0.4501  loss_mask: 0.1743  loss_dice: 1.213  loss_ce_0: 0.5759  loss_mask_0: 0.2095  loss_dice_0: 1.412  loss_ce_1: 0.6487  loss_mask_1: 0.1892  loss_dice_1: 1.367  loss_ce_2: 0.556  loss_mask_2: 0.1778  loss_dice_2: 1.264  loss_ce_3: 0.5063  loss_mask_3: 0.1777  loss_dice_3: 1.269  loss_ce_4: 0.4916  loss_mask_4: 0.1772  loss_dice_4: 1.245  loss_ce_5: 0.4915  loss_mask_5: 0.175  loss_dice_5: 1.218  loss_ce_6: 0.5033  loss_mask_6: 0.1751  loss_dice_6: 1.198  loss_ce_7: 0.4687  loss_mask_7: 0.1739  loss_dice_7: 1.226  loss_ce_8: 0.4612  loss_mask_8: 0.1737  loss_dice_8: 1.213    time: 1.1102  last_time: 1.1488  data_time: 0.0802  last_data_time: 0.0835   lr: 0.0001  max_mem: 32630M
[10/19 14:22:37] d2.utils.events INFO:  eta: 0:38:46  iter: 899  total_loss: 19.62  loss_ce: 0.4771  loss_mask: 0.1703  loss_dice: 1.183  loss_ce_0: 0.5831  loss_mask_0: 0.205  loss_dice_0: 1.473  loss_ce_1: 0.6457  loss_mask_1: 0.181  loss_dice_1: 1.37  loss_ce_2: 0.5721  loss_mask_2: 0.1714  loss_dice_2: 1.251  loss_ce_3: 0.5324  loss_mask_3: 0.1699  loss_dice_3: 1.228  loss_ce_4: 0.4961  loss_mask_4: 0.1688  loss_dice_4: 1.215  loss_ce_5: 0.4927  loss_mask_5: 0.1672  loss_dice_5: 1.214  loss_ce_6: 0.491  loss_mask_6: 0.1704  loss_dice_6: 1.193  loss_ce_7: 0.4734  loss_mask_7: 0.1702  loss_dice_7: 1.161  loss_ce_8: 0.4784  loss_mask_8: 0.168  loss_dice_8: 1.193    time: 1.1104  last_time: 1.1780  data_time: 0.0867  last_data_time: 0.0869   lr: 0.0001  max_mem: 32630M
[10/19 14:22:59] d2.utils.events INFO:  eta: 0:38:24  iter: 919  total_loss: 20.15  loss_ce: 0.4705  loss_mask: 0.18  loss_dice: 1.281  loss_ce_0: 0.5741  loss_mask_0: 0.2018  loss_dice_0: 1.499  loss_ce_1: 0.6166  loss_mask_1: 0.1889  loss_dice_1: 1.421  loss_ce_2: 0.5586  loss_mask_2: 0.1839  loss_dice_2: 1.355  loss_ce_3: 0.5121  loss_mask_3: 0.1849  loss_dice_3: 1.295  loss_ce_4: 0.4824  loss_mask_4: 0.1829  loss_dice_4: 1.287  loss_ce_5: 0.4847  loss_mask_5: 0.1827  loss_dice_5: 1.289  loss_ce_6: 0.4883  loss_mask_6: 0.1763  loss_dice_6: 1.284  loss_ce_7: 0.4649  loss_mask_7: 0.1778  loss_dice_7: 1.269  loss_ce_8: 0.4593  loss_mask_8: 0.1765  loss_dice_8: 1.247    time: 1.1104  last_time: 1.1333  data_time: 0.0813  last_data_time: 0.0686   lr: 0.0001  max_mem: 32630M
[10/19 14:23:22] d2.utils.events INFO:  eta: 0:38:03  iter: 939  total_loss: 21.12  loss_ce: 0.5004  loss_mask: 0.1676  loss_dice: 1.31  loss_ce_0: 0.6152  loss_mask_0: 0.1991  loss_dice_0: 1.54  loss_ce_1: 0.6356  loss_mask_1: 0.1848  loss_dice_1: 1.478  loss_ce_2: 0.5752  loss_mask_2: 0.1708  loss_dice_2: 1.414  loss_ce_3: 0.5601  loss_mask_3: 0.1716  loss_dice_3: 1.37  loss_ce_4: 0.5148  loss_mask_4: 0.1702  loss_dice_4: 1.346  loss_ce_5: 0.5293  loss_mask_5: 0.1676  loss_dice_5: 1.321  loss_ce_6: 0.5226  loss_mask_6: 0.1658  loss_dice_6: 1.328  loss_ce_7: 0.5262  loss_mask_7: 0.1673  loss_dice_7: 1.339  loss_ce_8: 0.4969  loss_mask_8: 0.1668  loss_dice_8: 1.352    time: 1.1110  last_time: 1.1563  data_time: 0.0839  last_data_time: 0.0925   lr: 0.0001  max_mem: 32630M
[10/19 14:23:44] d2.utils.events INFO:  eta: 0:37:41  iter: 959  total_loss: 20.46  loss_ce: 0.498  loss_mask: 0.1605  loss_dice: 1.283  loss_ce_0: 0.563  loss_mask_0: 0.1925  loss_dice_0: 1.483  loss_ce_1: 0.633  loss_mask_1: 0.1884  loss_dice_1: 1.413  loss_ce_2: 0.6017  loss_mask_2: 0.1791  loss_dice_2: 1.359  loss_ce_3: 0.5062  loss_mask_3: 0.1759  loss_dice_3: 1.305  loss_ce_4: 0.5204  loss_mask_4: 0.1745  loss_dice_4: 1.271  loss_ce_5: 0.493  loss_mask_5: 0.166  loss_dice_5: 1.297  loss_ce_6: 0.4767  loss_mask_6: 0.1657  loss_dice_6: 1.277  loss_ce_7: 0.5098  loss_mask_7: 0.1622  loss_dice_7: 1.234  loss_ce_8: 0.4858  loss_mask_8: 0.1613  loss_dice_8: 1.288    time: 1.1111  last_time: 1.1106  data_time: 0.0795  last_data_time: 0.0984   lr: 0.0001  max_mem: 32630M
[10/19 14:24:06] d2.utils.events INFO:  eta: 0:37:17  iter: 979  total_loss: 19.81  loss_ce: 0.4937  loss_mask: 0.1709  loss_dice: 1.22  loss_ce_0: 0.6091  loss_mask_0: 0.1952  loss_dice_0: 1.419  loss_ce_1: 0.6752  loss_mask_1: 0.185  loss_dice_1: 1.335  loss_ce_2: 0.6032  loss_mask_2: 0.1768  loss_dice_2: 1.29  loss_ce_3: 0.5493  loss_mask_3: 0.1764  loss_dice_3: 1.237  loss_ce_4: 0.5021  loss_mask_4: 0.1732  loss_dice_4: 1.223  loss_ce_5: 0.4932  loss_mask_5: 0.1717  loss_dice_5: 1.277  loss_ce_6: 0.4978  loss_mask_6: 0.1695  loss_dice_6: 1.183  loss_ce_7: 0.4894  loss_mask_7: 0.1706  loss_dice_7: 1.209  loss_ce_8: 0.477  loss_mask_8: 0.1703  loss_dice_8: 1.224    time: 1.1103  last_time: 1.0149  data_time: 0.0773  last_data_time: 0.0669   lr: 0.0001  max_mem: 32630M
[10/19 14:24:27] d2.utils.events INFO:  eta: 0:36:55  iter: 999  total_loss: 20.13  loss_ce: 0.5161  loss_mask: 0.1752  loss_dice: 1.247  loss_ce_0: 0.559  loss_mask_0: 0.2022  loss_dice_0: 1.484  loss_ce_1: 0.6333  loss_mask_1: 0.1846  loss_dice_1: 1.381  loss_ce_2: 0.6048  loss_mask_2: 0.1764  loss_dice_2: 1.332  loss_ce_3: 0.5645  loss_mask_3: 0.1816  loss_dice_3: 1.266  loss_ce_4: 0.5184  loss_mask_4: 0.1818  loss_dice_4: 1.278  loss_ce_5: 0.5229  loss_mask_5: 0.1797  loss_dice_5: 1.256  loss_ce_6: 0.507  loss_mask_6: 0.1794  loss_dice_6: 1.232  loss_ce_7: 0.4749  loss_mask_7: 0.1789  loss_dice_7: 1.21  loss_ce_8: 0.4773  loss_mask_8: 0.1779  loss_dice_8: 1.237    time: 1.1097  last_time: 1.1593  data_time: 0.0708  last_data_time: 0.0838   lr: 0.0001  max_mem: 32630M
[10/19 14:24:51] d2.utils.events INFO:  eta: 0:36:33  iter: 1019  total_loss: 20.42  loss_ce: 0.484  loss_mask: 0.1839  loss_dice: 1.213  loss_ce_0: 0.573  loss_mask_0: 0.2101  loss_dice_0: 1.408  loss_ce_1: 0.6318  loss_mask_1: 0.201  loss_dice_1: 1.4  loss_ce_2: 0.6051  loss_mask_2: 0.1944  loss_dice_2: 1.296  loss_ce_3: 0.5456  loss_mask_3: 0.1855  loss_dice_3: 1.251  loss_ce_4: 0.5007  loss_mask_4: 0.1858  loss_dice_4: 1.271  loss_ce_5: 0.5111  loss_mask_5: 0.1833  loss_dice_5: 1.245  loss_ce_6: 0.5009  loss_mask_6: 0.1787  loss_dice_6: 1.218  loss_ce_7: 0.5086  loss_mask_7: 0.1796  loss_dice_7: 1.233  loss_ce_8: 0.4904  loss_mask_8: 0.1821  loss_dice_8: 1.23    time: 1.1095  last_time: 1.1475  data_time: 0.0861  last_data_time: 0.0807   lr: 0.0001  max_mem: 32630M
[10/19 14:25:13] d2.utils.events INFO:  eta: 0:36:11  iter: 1039  total_loss: 20.42  loss_ce: 0.5137  loss_mask: 0.1748  loss_dice: 1.235  loss_ce_0: 0.5819  loss_mask_0: 0.2004  loss_dice_0: 1.451  loss_ce_1: 0.6831  loss_mask_1: 0.1859  loss_dice_1: 1.393  loss_ce_2: 0.6417  loss_mask_2: 0.1754  loss_dice_2: 1.31  loss_ce_3: 0.5622  loss_mask_3: 0.1799  loss_dice_3: 1.253  loss_ce_4: 0.5244  loss_mask_4: 0.1785  loss_dice_4: 1.289  loss_ce_5: 0.5288  loss_mask_5: 0.1746  loss_dice_5: 1.305  loss_ce_6: 0.5296  loss_mask_6: 0.1737  loss_dice_6: 1.237  loss_ce_7: 0.5317  loss_mask_7: 0.1724  loss_dice_7: 1.246  loss_ce_8: 0.5452  loss_mask_8: 0.1717  loss_dice_8: 1.243    time: 1.1092  last_time: 1.0301  data_time: 0.0827  last_data_time: 0.0561   lr: 0.0001  max_mem: 32630M
[10/19 14:25:34] d2.utils.events INFO:  eta: 0:35:48  iter: 1059  total_loss: 19.76  loss_ce: 0.5088  loss_mask: 0.1606  loss_dice: 1.204  loss_ce_0: 0.5834  loss_mask_0: 0.1986  loss_dice_0: 1.412  loss_ce_1: 0.6661  loss_mask_1: 0.1729  loss_dice_1: 1.308  loss_ce_2: 0.6114  loss_mask_2: 0.1641  loss_dice_2: 1.265  loss_ce_3: 0.5271  loss_mask_3: 0.1636  loss_dice_3: 1.231  loss_ce_4: 0.5224  loss_mask_4: 0.164  loss_dice_4: 1.202  loss_ce_5: 0.511  loss_mask_5: 0.159  loss_dice_5: 1.206  loss_ce_6: 0.4842  loss_mask_6: 0.1599  loss_dice_6: 1.195  loss_ce_7: 0.4969  loss_mask_7: 0.1602  loss_dice_7: 1.185  loss_ce_8: 0.5076  loss_mask_8: 0.1626  loss_dice_8: 1.166    time: 1.1085  last_time: 1.0630  data_time: 0.0725  last_data_time: 0.0741   lr: 0.0001  max_mem: 32630M
[10/19 14:25:55] d2.utils.events INFO:  eta: 0:35:25  iter: 1079  total_loss: 20.12  loss_ce: 0.483  loss_mask: 0.1673  loss_dice: 1.228  loss_ce_0: 0.6109  loss_mask_0: 0.2041  loss_dice_0: 1.447  loss_ce_1: 0.6717  loss_mask_1: 0.1851  loss_dice_1: 1.39  loss_ce_2: 0.6188  loss_mask_2: 0.1696  loss_dice_2: 1.35  loss_ce_3: 0.5433  loss_mask_3: 0.1736  loss_dice_3: 1.247  loss_ce_4: 0.5161  loss_mask_4: 0.1733  loss_dice_4: 1.25  loss_ce_5: 0.4971  loss_mask_5: 0.168  loss_dice_5: 1.275  loss_ce_6: 0.4957  loss_mask_6: 0.1733  loss_dice_6: 1.23  loss_ce_7: 0.4688  loss_mask_7: 0.1717  loss_dice_7: 1.246  loss_ce_8: 0.4958  loss_mask_8: 0.1715  loss_dice_8: 1.219    time: 1.1076  last_time: 1.0699  data_time: 0.0732  last_data_time: 0.0754   lr: 0.0001  max_mem: 32630M
[10/19 14:26:17] d2.utils.events INFO:  eta: 0:35:00  iter: 1099  total_loss: 20.64  loss_ce: 0.5129  loss_mask: 0.1545  loss_dice: 1.273  loss_ce_0: 0.5875  loss_mask_0: 0.1889  loss_dice_0: 1.477  loss_ce_1: 0.629  loss_mask_1: 0.1756  loss_dice_1: 1.427  loss_ce_2: 0.5636  loss_mask_2: 0.165  loss_dice_2: 1.372  loss_ce_3: 0.549  loss_mask_3: 0.1656  loss_dice_3: 1.324  loss_ce_4: 0.5176  loss_mask_4: 0.1585  loss_dice_4: 1.319  loss_ce_5: 0.4806  loss_mask_5: 0.1581  loss_dice_5: 1.332  loss_ce_6: 0.4931  loss_mask_6: 0.1576  loss_dice_6: 1.28  loss_ce_7: 0.4922  loss_mask_7: 0.159  loss_dice_7: 1.282  loss_ce_8: 0.4768  loss_mask_8: 0.1598  loss_dice_8: 1.282    time: 1.1071  last_time: 1.0832  data_time: 0.0760  last_data_time: 0.0606   lr: 0.0001  max_mem: 32630M
[10/19 14:26:39] d2.utils.events INFO:  eta: 0:34:36  iter: 1119  total_loss: 20.31  loss_ce: 0.4974  loss_mask: 0.1751  loss_dice: 1.197  loss_ce_0: 0.5842  loss_mask_0: 0.2049  loss_dice_0: 1.416  loss_ce_1: 0.6756  loss_mask_1: 0.1912  loss_dice_1: 1.366  loss_ce_2: 0.6236  loss_mask_2: 0.1804  loss_dice_2: 1.305  loss_ce_3: 0.6001  loss_mask_3: 0.1746  loss_dice_3: 1.242  loss_ce_4: 0.5171  loss_mask_4: 0.1771  loss_dice_4: 1.241  loss_ce_5: 0.4878  loss_mask_5: 0.174  loss_dice_5: 1.233  loss_ce_6: 0.501  loss_mask_6: 0.1738  loss_dice_6: 1.18  loss_ce_7: 0.5157  loss_mask_7: 0.1727  loss_dice_7: 1.248  loss_ce_8: 0.5265  loss_mask_8: 0.1733  loss_dice_8: 1.222    time: 1.1065  last_time: 1.0838  data_time: 0.0725  last_data_time: 0.0707   lr: 0.0001  max_mem: 32630M
[10/19 14:27:00] d2.utils.events INFO:  eta: 0:34:13  iter: 1139  total_loss: 19.85  loss_ce: 0.4669  loss_mask: 0.1737  loss_dice: 1.189  loss_ce_0: 0.5608  loss_mask_0: 0.1989  loss_dice_0: 1.442  loss_ce_1: 0.6313  loss_mask_1: 0.1883  loss_dice_1: 1.335  loss_ce_2: 0.5622  loss_mask_2: 0.1739  loss_dice_2: 1.281  loss_ce_3: 0.5097  loss_mask_3: 0.1732  loss_dice_3: 1.243  loss_ce_4: 0.4952  loss_mask_4: 0.1752  loss_dice_4: 1.252  loss_ce_5: 0.4914  loss_mask_5: 0.1738  loss_dice_5: 1.205  loss_ce_6: 0.4943  loss_mask_6: 0.1741  loss_dice_6: 1.187  loss_ce_7: 0.4935  loss_mask_7: 0.1715  loss_dice_7: 1.197  loss_ce_8: 0.4582  loss_mask_8: 0.1722  loss_dice_8: 1.176    time: 1.1057  last_time: 1.0375  data_time: 0.0744  last_data_time: 0.0769   lr: 0.0001  max_mem: 32630M
[10/19 14:27:21] d2.utils.events INFO:  eta: 0:33:48  iter: 1159  total_loss: 19.35  loss_ce: 0.4466  loss_mask: 0.1637  loss_dice: 1.203  loss_ce_0: 0.5495  loss_mask_0: 0.1954  loss_dice_0: 1.406  loss_ce_1: 0.5919  loss_mask_1: 0.1764  loss_dice_1: 1.325  loss_ce_2: 0.5573  loss_mask_2: 0.1699  loss_dice_2: 1.272  loss_ce_3: 0.4914  loss_mask_3: 0.1681  loss_dice_3: 1.24  loss_ce_4: 0.4683  loss_mask_4: 0.1695  loss_dice_4: 1.218  loss_ce_5: 0.5103  loss_mask_5: 0.1635  loss_dice_5: 1.243  loss_ce_6: 0.4584  loss_mask_6: 0.1666  loss_dice_6: 1.194  loss_ce_7: 0.4999  loss_mask_7: 0.163  loss_dice_7: 1.182  loss_ce_8: 0.4522  loss_mask_8: 0.1665  loss_dice_8: 1.233    time: 1.1049  last_time: 1.0436  data_time: 0.0736  last_data_time: 0.0620   lr: 0.0001  max_mem: 32630M
[10/19 14:27:43] d2.utils.events INFO:  eta: 0:33:26  iter: 1179  total_loss: 19.8  loss_ce: 0.4536  loss_mask: 0.1579  loss_dice: 1.242  loss_ce_0: 0.6067  loss_mask_0: 0.1844  loss_dice_0: 1.462  loss_ce_1: 0.651  loss_mask_1: 0.1699  loss_dice_1: 1.376  loss_ce_2: 0.5295  loss_mask_2: 0.1608  loss_dice_2: 1.322  loss_ce_3: 0.5157  loss_mask_3: 0.16  loss_dice_3: 1.278  loss_ce_4: 0.5141  loss_mask_4: 0.1613  loss_dice_4: 1.261  loss_ce_5: 0.4821  loss_mask_5: 0.1599  loss_dice_5: 1.244  loss_ce_6: 0.4851  loss_mask_6: 0.1562  loss_dice_6: 1.244  loss_ce_7: 0.4603  loss_mask_7: 0.1584  loss_dice_7: 1.248  loss_ce_8: 0.4855  loss_mask_8: 0.1589  loss_dice_8: 1.228    time: 1.1044  last_time: 1.0560  data_time: 0.0762  last_data_time: 0.0991   lr: 0.0001  max_mem: 32630M
[10/19 14:28:04] d2.utils.events INFO:  eta: 0:33:03  iter: 1199  total_loss: 20.58  loss_ce: 0.4622  loss_mask: 0.1645  loss_dice: 1.298  loss_ce_0: 0.5826  loss_mask_0: 0.1908  loss_dice_0: 1.473  loss_ce_1: 0.6109  loss_mask_1: 0.1804  loss_dice_1: 1.444  loss_ce_2: 0.5464  loss_mask_2: 0.1656  loss_dice_2: 1.37  loss_ce_3: 0.5234  loss_mask_3: 0.1654  loss_dice_3: 1.316  loss_ce_4: 0.5067  loss_mask_4: 0.1674  loss_dice_4: 1.297  loss_ce_5: 0.4823  loss_mask_5: 0.1665  loss_dice_5: 1.319  loss_ce_6: 0.4985  loss_mask_6: 0.1653  loss_dice_6: 1.306  loss_ce_7: 0.4801  loss_mask_7: 0.1654  loss_dice_7: 1.317  loss_ce_8: 0.4643  loss_mask_8: 0.1656  loss_dice_8: 1.279    time: 1.1041  last_time: 1.1392  data_time: 0.0757  last_data_time: 0.0835   lr: 0.0001  max_mem: 32630M
[10/19 14:28:26] d2.utils.events INFO:  eta: 0:32:39  iter: 1219  total_loss: 19.52  loss_ce: 0.4765  loss_mask: 0.1661  loss_dice: 1.19  loss_ce_0: 0.5913  loss_mask_0: 0.2023  loss_dice_0: 1.392  loss_ce_1: 0.6389  loss_mask_1: 0.1817  loss_dice_1: 1.304  loss_ce_2: 0.6001  loss_mask_2: 0.1678  loss_dice_2: 1.243  loss_ce_3: 0.5572  loss_mask_3: 0.1652  loss_dice_3: 1.2  loss_ce_4: 0.4882  loss_mask_4: 0.1692  loss_dice_4: 1.225  loss_ce_5: 0.5052  loss_mask_5: 0.1676  loss_dice_5: 1.173  loss_ce_6: 0.4905  loss_mask_6: 0.1685  loss_dice_6: 1.167  loss_ce_7: 0.4877  loss_mask_7: 0.1666  loss_dice_7: 1.17  loss_ce_8: 0.4715  loss_mask_8: 0.1651  loss_dice_8: 1.161    time: 1.1036  last_time: 1.0704  data_time: 0.0738  last_data_time: 0.0695   lr: 0.0001  max_mem: 32630M
[10/19 14:28:48] d2.utils.events INFO:  eta: 0:32:16  iter: 1239  total_loss: 19.12  loss_ce: 0.4427  loss_mask: 0.1672  loss_dice: 1.163  loss_ce_0: 0.6013  loss_mask_0: 0.196  loss_dice_0: 1.387  loss_ce_1: 0.6096  loss_mask_1: 0.1839  loss_dice_1: 1.311  loss_ce_2: 0.5545  loss_mask_2: 0.1746  loss_dice_2: 1.258  loss_ce_3: 0.4853  loss_mask_3: 0.1729  loss_dice_3: 1.233  loss_ce_4: 0.4648  loss_mask_4: 0.1709  loss_dice_4: 1.224  loss_ce_5: 0.4789  loss_mask_5: 0.1717  loss_dice_5: 1.209  loss_ce_6: 0.4352  loss_mask_6: 0.1702  loss_dice_6: 1.19  loss_ce_7: 0.4417  loss_mask_7: 0.1669  loss_dice_7: 1.19  loss_ce_8: 0.4435  loss_mask_8: 0.1692  loss_dice_8: 1.181    time: 1.1032  last_time: 1.0574  data_time: 0.0773  last_data_time: 0.0972   lr: 0.0001  max_mem: 32630M
[10/19 14:29:09] d2.utils.events INFO:  eta: 0:31:52  iter: 1259  total_loss: 19.9  loss_ce: 0.4671  loss_mask: 0.1815  loss_dice: 1.278  loss_ce_0: 0.5473  loss_mask_0: 0.2096  loss_dice_0: 1.457  loss_ce_1: 0.5747  loss_mask_1: 0.2002  loss_dice_1: 1.362  loss_ce_2: 0.5459  loss_mask_2: 0.1863  loss_dice_2: 1.324  loss_ce_3: 0.5209  loss_mask_3: 0.1855  loss_dice_3: 1.241  loss_ce_4: 0.4707  loss_mask_4: 0.1838  loss_dice_4: 1.283  loss_ce_5: 0.474  loss_mask_5: 0.1839  loss_dice_5: 1.249  loss_ce_6: 0.4984  loss_mask_6: 0.1829  loss_dice_6: 1.238  loss_ce_7: 0.476  loss_mask_7: 0.1822  loss_dice_7: 1.223  loss_ce_8: 0.4888  loss_mask_8: 0.1828  loss_dice_8: 1.234    time: 1.1025  last_time: 1.0803  data_time: 0.0738  last_data_time: 0.0665   lr: 0.0001  max_mem: 32630M
[10/19 14:29:30] d2.utils.events INFO:  eta: 0:31:29  iter: 1279  total_loss: 19.64  loss_ce: 0.4556  loss_mask: 0.1698  loss_dice: 1.199  loss_ce_0: 0.5992  loss_mask_0: 0.2013  loss_dice_0: 1.376  loss_ce_1: 0.6422  loss_mask_1: 0.1857  loss_dice_1: 1.32  loss_ce_2: 0.579  loss_mask_2: 0.1804  loss_dice_2: 1.26  loss_ce_3: 0.5286  loss_mask_3: 0.1755  loss_dice_3: 1.229  loss_ce_4: 0.5237  loss_mask_4: 0.1742  loss_dice_4: 1.208  loss_ce_5: 0.488  loss_mask_5: 0.1736  loss_dice_5: 1.242  loss_ce_6: 0.5032  loss_mask_6: 0.1724  loss_dice_6: 1.233  loss_ce_7: 0.4611  loss_mask_7: 0.174  loss_dice_7: 1.197  loss_ce_8: 0.4638  loss_mask_8: 0.1728  loss_dice_8: 1.202    time: 1.1020  last_time: 1.0551  data_time: 0.0719  last_data_time: 0.0624   lr: 0.0001  max_mem: 32630M
[10/19 14:29:52] d2.utils.events INFO:  eta: 0:31:06  iter: 1299  total_loss: 19.61  loss_ce: 0.4497  loss_mask: 0.1566  loss_dice: 1.26  loss_ce_0: 0.5742  loss_mask_0: 0.1908  loss_dice_0: 1.443  loss_ce_1: 0.6195  loss_mask_1: 0.1763  loss_dice_1: 1.407  loss_ce_2: 0.5647  loss_mask_2: 0.1612  loss_dice_2: 1.319  loss_ce_3: 0.5133  loss_mask_3: 0.1591  loss_dice_3: 1.234  loss_ce_4: 0.5171  loss_mask_4: 0.1572  loss_dice_4: 1.256  loss_ce_5: 0.5  loss_mask_5: 0.1568  loss_dice_5: 1.258  loss_ce_6: 0.476  loss_mask_6: 0.1583  loss_dice_6: 1.25  loss_ce_7: 0.4768  loss_mask_7: 0.1584  loss_dice_7: 1.235  loss_ce_8: 0.5163  loss_mask_8: 0.1576  loss_dice_8: 1.235    time: 1.1018  last_time: 1.0605  data_time: 0.0825  last_data_time: 0.0819   lr: 0.0001  max_mem: 32630M
[10/19 14:30:13] d2.utils.events INFO:  eta: 0:30:42  iter: 1319  total_loss: 18.53  loss_ce: 0.4107  loss_mask: 0.1582  loss_dice: 1.139  loss_ce_0: 0.548  loss_mask_0: 0.1925  loss_dice_0: 1.383  loss_ce_1: 0.5868  loss_mask_1: 0.1737  loss_dice_1: 1.281  loss_ce_2: 0.5331  loss_mask_2: 0.165  loss_dice_2: 1.235  loss_ce_3: 0.4787  loss_mask_3: 0.1629  loss_dice_3: 1.179  loss_ce_4: 0.433  loss_mask_4: 0.1632  loss_dice_4: 1.179  loss_ce_5: 0.4324  loss_mask_5: 0.1603  loss_dice_5: 1.156  loss_ce_6: 0.4584  loss_mask_6: 0.1587  loss_dice_6: 1.142  loss_ce_7: 0.4225  loss_mask_7: 0.1583  loss_dice_7: 1.185  loss_ce_8: 0.41  loss_mask_8: 0.1587  loss_dice_8: 1.193    time: 1.1012  last_time: 1.0722  data_time: 0.0779  last_data_time: 0.0530   lr: 0.0001  max_mem: 32630M
[10/19 14:30:35] d2.utils.events INFO:  eta: 0:30:19  iter: 1339  total_loss: 18.91  loss_ce: 0.4208  loss_mask: 0.1629  loss_dice: 1.197  loss_ce_0: 0.5762  loss_mask_0: 0.1792  loss_dice_0: 1.455  loss_ce_1: 0.6489  loss_mask_1: 0.1719  loss_dice_1: 1.345  loss_ce_2: 0.5502  loss_mask_2: 0.1663  loss_dice_2: 1.261  loss_ce_3: 0.4972  loss_mask_3: 0.1657  loss_dice_3: 1.255  loss_ce_4: 0.4511  loss_mask_4: 0.1637  loss_dice_4: 1.234  loss_ce_5: 0.4319  loss_mask_5: 0.16  loss_dice_5: 1.217  loss_ce_6: 0.438  loss_mask_6: 0.1596  loss_dice_6: 1.191  loss_ce_7: 0.4412  loss_mask_7: 0.1645  loss_dice_7: 1.169  loss_ce_8: 0.4227  loss_mask_8: 0.1624  loss_dice_8: 1.16    time: 1.1010  last_time: 1.1236  data_time: 0.0776  last_data_time: 0.0623   lr: 0.0001  max_mem: 32630M
[10/19 14:30:57] d2.utils.events INFO:  eta: 0:29:57  iter: 1359  total_loss: 19.65  loss_ce: 0.4728  loss_mask: 0.1681  loss_dice: 1.2  loss_ce_0: 0.6027  loss_mask_0: 0.1916  loss_dice_0: 1.429  loss_ce_1: 0.6367  loss_mask_1: 0.1782  loss_dice_1: 1.343  loss_ce_2: 0.5924  loss_mask_2: 0.1801  loss_dice_2: 1.309  loss_ce_3: 0.5428  loss_mask_3: 0.1736  loss_dice_3: 1.217  loss_ce_4: 0.5052  loss_mask_4: 0.1711  loss_dice_4: 1.256  loss_ce_5: 0.4745  loss_mask_5: 0.1729  loss_dice_5: 1.222  loss_ce_6: 0.4898  loss_mask_6: 0.1693  loss_dice_6: 1.185  loss_ce_7: 0.4651  loss_mask_7: 0.1715  loss_dice_7: 1.225  loss_ce_8: 0.4995  loss_mask_8: 0.1701  loss_dice_8: 1.211    time: 1.1005  last_time: 1.0561  data_time: 0.0773  last_data_time: 0.1029   lr: 0.0001  max_mem: 32630M
[10/19 14:31:18] d2.utils.events INFO:  eta: 0:29:34  iter: 1379  total_loss: 20.27  loss_ce: 0.503  loss_mask: 0.1615  loss_dice: 1.194  loss_ce_0: 0.5475  loss_mask_0: 0.1869  loss_dice_0: 1.424  loss_ce_1: 0.5818  loss_mask_1: 0.1805  loss_dice_1: 1.37  loss_ce_2: 0.5814  loss_mask_2: 0.1746  loss_dice_2: 1.325  loss_ce_3: 0.5326  loss_mask_3: 0.1692  loss_dice_3: 1.294  loss_ce_4: 0.4883  loss_mask_4: 0.1681  loss_dice_4: 1.279  loss_ce_5: 0.5274  loss_mask_5: 0.1672  loss_dice_5: 1.261  loss_ce_6: 0.5073  loss_mask_6: 0.1665  loss_dice_6: 1.223  loss_ce_7: 0.4927  loss_mask_7: 0.1658  loss_dice_7: 1.226  loss_ce_8: 0.4792  loss_mask_8: 0.1647  loss_dice_8: 1.241    time: 1.1003  last_time: 1.0979  data_time: 0.0762  last_data_time: 0.0828   lr: 0.0001  max_mem: 32630M
[10/19 14:31:40] d2.utils.events INFO:  eta: 0:29:11  iter: 1399  total_loss: 20.08  loss_ce: 0.444  loss_mask: 0.1515  loss_dice: 1.213  loss_ce_0: 0.5782  loss_mask_0: 0.1817  loss_dice_0: 1.441  loss_ce_1: 0.608  loss_mask_1: 0.1714  loss_dice_1: 1.363  loss_ce_2: 0.5756  loss_mask_2: 0.1643  loss_dice_2: 1.301  loss_ce_3: 0.527  loss_mask_3: 0.1592  loss_dice_3: 1.251  loss_ce_4: 0.5119  loss_mask_4: 0.1581  loss_dice_4: 1.254  loss_ce_5: 0.5025  loss_mask_5: 0.1567  loss_dice_5: 1.254  loss_ce_6: 0.4871  loss_mask_6: 0.1551  loss_dice_6: 1.229  loss_ce_7: 0.471  loss_mask_7: 0.1537  loss_dice_7: 1.242  loss_ce_8: 0.4544  loss_mask_8: 0.153  loss_dice_8: 1.22    time: 1.1002  last_time: 1.0880  data_time: 0.0760  last_data_time: 0.0705   lr: 0.0001  max_mem: 32630M
[10/19 14:32:02] d2.utils.events INFO:  eta: 0:28:47  iter: 1419  total_loss: 20.06  loss_ce: 0.4655  loss_mask: 0.165  loss_dice: 1.209  loss_ce_0: 0.5756  loss_mask_0: 0.2059  loss_dice_0: 1.466  loss_ce_1: 0.6161  loss_mask_1: 0.1843  loss_dice_1: 1.369  loss_ce_2: 0.5735  loss_mask_2: 0.1742  loss_dice_2: 1.291  loss_ce_3: 0.5193  loss_mask_3: 0.1714  loss_dice_3: 1.252  loss_ce_4: 0.4973  loss_mask_4: 0.1685  loss_dice_4: 1.245  loss_ce_5: 0.4772  loss_mask_5: 0.1712  loss_dice_5: 1.267  loss_ce_6: 0.4998  loss_mask_6: 0.1712  loss_dice_6: 1.213  loss_ce_7: 0.4655  loss_mask_7: 0.1693  loss_dice_7: 1.221  loss_ce_8: 0.4576  loss_mask_8: 0.1654  loss_dice_8: 1.239    time: 1.0998  last_time: 1.1289  data_time: 0.0775  last_data_time: 0.0658   lr: 0.0001  max_mem: 32630M
[10/19 14:32:24] d2.utils.events INFO:  eta: 0:28:25  iter: 1439  total_loss: 19.95  loss_ce: 0.4401  loss_mask: 0.1734  loss_dice: 1.302  loss_ce_0: 0.5373  loss_mask_0: 0.2027  loss_dice_0: 1.493  loss_ce_1: 0.5309  loss_mask_1: 0.1816  loss_dice_1: 1.413  loss_ce_2: 0.5007  loss_mask_2: 0.178  loss_dice_2: 1.342  loss_ce_3: 0.464  loss_mask_3: 0.1731  loss_dice_3: 1.314  loss_ce_4: 0.4595  loss_mask_4: 0.1723  loss_dice_4: 1.318  loss_ce_5: 0.4799  loss_mask_5: 0.1686  loss_dice_5: 1.317  loss_ce_6: 0.4373  loss_mask_6: 0.1746  loss_dice_6: 1.291  loss_ce_7: 0.4706  loss_mask_7: 0.1749  loss_dice_7: 1.311  loss_ce_8: 0.426  loss_mask_8: 0.1711  loss_dice_8: 1.285    time: 1.0996  last_time: 1.0879  data_time: 0.0787  last_data_time: 0.0770   lr: 0.0001  max_mem: 32630M
[10/19 14:32:45] d2.utils.events INFO:  eta: 0:28:01  iter: 1459  total_loss: 19.53  loss_ce: 0.4904  loss_mask: 0.1523  loss_dice: 1.213  loss_ce_0: 0.5923  loss_mask_0: 0.1836  loss_dice_0: 1.418  loss_ce_1: 0.633  loss_mask_1: 0.1649  loss_dice_1: 1.385  loss_ce_2: 0.5696  loss_mask_2: 0.1587  loss_dice_2: 1.297  loss_ce_3: 0.5179  loss_mask_3: 0.157  loss_dice_3: 1.28  loss_ce_4: 0.5031  loss_mask_4: 0.1549  loss_dice_4: 1.278  loss_ce_5: 0.4916  loss_mask_5: 0.1528  loss_dice_5: 1.262  loss_ce_6: 0.4803  loss_mask_6: 0.1513  loss_dice_6: 1.245  loss_ce_7: 0.5061  loss_mask_7: 0.154  loss_dice_7: 1.249  loss_ce_8: 0.4661  loss_mask_8: 0.151  loss_dice_8: 1.25    time: 1.0991  last_time: 1.0617  data_time: 0.0776  last_data_time: 0.0756   lr: 0.0001  max_mem: 32630M
[10/19 14:33:07] d2.utils.events INFO:  eta: 0:27:39  iter: 1479  total_loss: 20.88  loss_ce: 0.4985  loss_mask: 0.1603  loss_dice: 1.354  loss_ce_0: 0.5804  loss_mask_0: 0.1842  loss_dice_0: 1.517  loss_ce_1: 0.6619  loss_mask_1: 0.1704  loss_dice_1: 1.493  loss_ce_2: 0.5795  loss_mask_2: 0.1662  loss_dice_2: 1.437  loss_ce_3: 0.5434  loss_mask_3: 0.1648  loss_dice_3: 1.358  loss_ce_4: 0.5256  loss_mask_4: 0.1663  loss_dice_4: 1.358  loss_ce_5: 0.544  loss_mask_5: 0.1626  loss_dice_5: 1.362  loss_ce_6: 0.5223  loss_mask_6: 0.1622  loss_dice_6: 1.349  loss_ce_7: 0.5108  loss_mask_7: 0.1606  loss_dice_7: 1.294  loss_ce_8: 0.5049  loss_mask_8: 0.1617  loss_dice_8: 1.303    time: 1.0990  last_time: 1.0479  data_time: 0.0827  last_data_time: 0.0952   lr: 0.0001  max_mem: 32630M
[10/19 14:33:28] d2.utils.events INFO:  eta: 0:27:16  iter: 1499  total_loss: 19.89  loss_ce: 0.4805  loss_mask: 0.1746  loss_dice: 1.232  loss_ce_0: 0.5798  loss_mask_0: 0.1953  loss_dice_0: 1.434  loss_ce_1: 0.6591  loss_mask_1: 0.186  loss_dice_1: 1.37  loss_ce_2: 0.5732  loss_mask_2: 0.1805  loss_dice_2: 1.291  loss_ce_3: 0.5288  loss_mask_3: 0.1784  loss_dice_3: 1.245  loss_ce_4: 0.5046  loss_mask_4: 0.1745  loss_dice_4: 1.27  loss_ce_5: 0.5174  loss_mask_5: 0.173  loss_dice_5: 1.263  loss_ce_6: 0.4948  loss_mask_6: 0.174  loss_dice_6: 1.219  loss_ce_7: 0.477  loss_mask_7: 0.1744  loss_dice_7: 1.208  loss_ce_8: 0.4807  loss_mask_8: 0.1729  loss_dice_8: 1.218    time: 1.0987  last_time: 1.0275  data_time: 0.0756  last_data_time: 0.0678   lr: 0.0001  max_mem: 32630M
[10/19 14:33:50] d2.utils.events INFO:  eta: 0:26:53  iter: 1519  total_loss: 18.87  loss_ce: 0.466  loss_mask: 0.1617  loss_dice: 1.212  loss_ce_0: 0.5592  loss_mask_0: 0.1959  loss_dice_0: 1.435  loss_ce_1: 0.6412  loss_mask_1: 0.1752  loss_dice_1: 1.362  loss_ce_2: 0.5616  loss_mask_2: 0.1657  loss_dice_2: 1.27  loss_ce_3: 0.4966  loss_mask_3: 0.1642  loss_dice_3: 1.232  loss_ce_4: 0.5043  loss_mask_4: 0.1625  loss_dice_4: 1.272  loss_ce_5: 0.4889  loss_mask_5: 0.1593  loss_dice_5: 1.228  loss_ce_6: 0.478  loss_mask_6: 0.1635  loss_dice_6: 1.182  loss_ce_7: 0.4563  loss_mask_7: 0.1616  loss_dice_7: 1.211  loss_ce_8: 0.446  loss_mask_8: 0.1592  loss_dice_8: 1.168    time: 1.0983  last_time: 1.0828  data_time: 0.0736  last_data_time: 0.0879   lr: 0.0001  max_mem: 32630M
[10/19 14:34:11] d2.utils.events INFO:  eta: 0:26:30  iter: 1539  total_loss: 19.03  loss_ce: 0.4329  loss_mask: 0.1733  loss_dice: 1.176  loss_ce_0: 0.5454  loss_mask_0: 0.1997  loss_dice_0: 1.397  loss_ce_1: 0.6144  loss_mask_1: 0.1872  loss_dice_1: 1.335  loss_ce_2: 0.5363  loss_mask_2: 0.1734  loss_dice_2: 1.253  loss_ce_3: 0.4583  loss_mask_3: 0.173  loss_dice_3: 1.201  loss_ce_4: 0.4492  loss_mask_4: 0.1736  loss_dice_4: 1.233  loss_ce_5: 0.4522  loss_mask_5: 0.1718  loss_dice_5: 1.218  loss_ce_6: 0.4773  loss_mask_6: 0.1726  loss_dice_6: 1.217  loss_ce_7: 0.4562  loss_mask_7: 0.173  loss_dice_7: 1.167  loss_ce_8: 0.4434  loss_mask_8: 0.1728  loss_dice_8: 1.184    time: 1.0979  last_time: 1.0981  data_time: 0.0728  last_data_time: 0.0730   lr: 0.0001  max_mem: 32630M
[10/19 14:34:33] d2.utils.events INFO:  eta: 0:26:07  iter: 1559  total_loss: 19.82  loss_ce: 0.4749  loss_mask: 0.155  loss_dice: 1.162  loss_ce_0: 0.5923  loss_mask_0: 0.1803  loss_dice_0: 1.445  loss_ce_1: 0.6654  loss_mask_1: 0.1667  loss_dice_1: 1.338  loss_ce_2: 0.5831  loss_mask_2: 0.1568  loss_dice_2: 1.259  loss_ce_3: 0.5362  loss_mask_3: 0.1554  loss_dice_3: 1.226  loss_ce_4: 0.5325  loss_mask_4: 0.1554  loss_dice_4: 1.239  loss_ce_5: 0.5326  loss_mask_5: 0.1531  loss_dice_5: 1.23  loss_ce_6: 0.4964  loss_mask_6: 0.1547  loss_dice_6: 1.185  loss_ce_7: 0.4869  loss_mask_7: 0.154  loss_dice_7: 1.204  loss_ce_8: 0.4896  loss_mask_8: 0.1549  loss_dice_8: 1.219    time: 1.0978  last_time: 1.1669  data_time: 0.0828  last_data_time: 0.0839   lr: 0.0001  max_mem: 32630M
[10/19 14:34:54] d2.utils.events INFO:  eta: 0:25:43  iter: 1579  total_loss: 18.7  loss_ce: 0.4415  loss_mask: 0.1743  loss_dice: 1.161  loss_ce_0: 0.549  loss_mask_0: 0.2112  loss_dice_0: 1.371  loss_ce_1: 0.5804  loss_mask_1: 0.1869  loss_dice_1: 1.306  loss_ce_2: 0.5247  loss_mask_2: 0.1793  loss_dice_2: 1.247  loss_ce_3: 0.4739  loss_mask_3: 0.181  loss_dice_3: 1.209  loss_ce_4: 0.4725  loss_mask_4: 0.1802  loss_dice_4: 1.211  loss_ce_5: 0.4459  loss_mask_5: 0.1756  loss_dice_5: 1.155  loss_ce_6: 0.4413  loss_mask_6: 0.1741  loss_dice_6: 1.19  loss_ce_7: 0.4447  loss_mask_7: 0.1737  loss_dice_7: 1.189  loss_ce_8: 0.4256  loss_mask_8: 0.1738  loss_dice_8: 1.179    time: 1.0973  last_time: 1.0666  data_time: 0.0764  last_data_time: 0.0920   lr: 0.0001  max_mem: 32630M
[10/19 14:35:16] d2.utils.events INFO:  eta: 0:25:21  iter: 1599  total_loss: 19.48  loss_ce: 0.4349  loss_mask: 0.1451  loss_dice: 1.253  loss_ce_0: 0.5687  loss_mask_0: 0.1733  loss_dice_0: 1.425  loss_ce_1: 0.6706  loss_mask_1: 0.1613  loss_dice_1: 1.359  loss_ce_2: 0.5804  loss_mask_2: 0.1528  loss_dice_2: 1.268  loss_ce_3: 0.5051  loss_mask_3: 0.146  loss_dice_3: 1.216  loss_ce_4: 0.4939  loss_mask_4: 0.1476  loss_dice_4: 1.229  loss_ce_5: 0.4857  loss_mask_5: 0.1466  loss_dice_5: 1.223  loss_ce_6: 0.447  loss_mask_6: 0.1454  loss_dice_6: 1.238  loss_ce_7: 0.475  loss_mask_7: 0.1452  loss_dice_7: 1.219  loss_ce_8: 0.4665  loss_mask_8: 0.1457  loss_dice_8: 1.23    time: 1.0972  last_time: 1.0875  data_time: 0.0788  last_data_time: 0.0790   lr: 0.0001  max_mem: 32630M
[10/19 14:35:38] d2.utils.events INFO:  eta: 0:24:58  iter: 1619  total_loss: 19.42  loss_ce: 0.4323  loss_mask: 0.1905  loss_dice: 1.228  loss_ce_0: 0.577  loss_mask_0: 0.215  loss_dice_0: 1.378  loss_ce_1: 0.6156  loss_mask_1: 0.1909  loss_dice_1: 1.303  loss_ce_2: 0.5878  loss_mask_2: 0.1911  loss_dice_2: 1.208  loss_ce_3: 0.5198  loss_mask_3: 0.1927  loss_dice_3: 1.231  loss_ce_4: 0.4642  loss_mask_4: 0.1889  loss_dice_4: 1.184  loss_ce_5: 0.4655  loss_mask_5: 0.1877  loss_dice_5: 1.215  loss_ce_6: 0.444  loss_mask_6: 0.1846  loss_dice_6: 1.181  loss_ce_7: 0.4486  loss_mask_7: 0.1848  loss_dice_7: 1.202  loss_ce_8: 0.4271  loss_mask_8: 0.1865  loss_dice_8: 1.189    time: 1.0969  last_time: 1.0214  data_time: 0.0783  last_data_time: 0.0685   lr: 0.0001  max_mem: 32630M
[10/19 14:35:59] d2.utils.events INFO:  eta: 0:24:35  iter: 1639  total_loss: 19.78  loss_ce: 0.4799  loss_mask: 0.1575  loss_dice: 1.251  loss_ce_0: 0.585  loss_mask_0: 0.1772  loss_dice_0: 1.427  loss_ce_1: 0.6359  loss_mask_1: 0.1719  loss_dice_1: 1.369  loss_ce_2: 0.6092  loss_mask_2: 0.1663  loss_dice_2: 1.335  loss_ce_3: 0.5331  loss_mask_3: 0.1611  loss_dice_3: 1.27  loss_ce_4: 0.5198  loss_mask_4: 0.163  loss_dice_4: 1.247  loss_ce_5: 0.5002  loss_mask_5: 0.1621  loss_dice_5: 1.24  loss_ce_6: 0.5022  loss_mask_6: 0.1582  loss_dice_6: 1.237  loss_ce_7: 0.4895  loss_mask_7: 0.1586  loss_dice_7: 1.242  loss_ce_8: 0.4823  loss_mask_8: 0.1588  loss_dice_8: 1.264    time: 1.0967  last_time: 1.2403  data_time: 0.0762  last_data_time: 0.0811   lr: 0.0001  max_mem: 32630M
[10/19 14:36:21] d2.utils.events INFO:  eta: 0:24:13  iter: 1659  total_loss: 18.61  loss_ce: 0.4033  loss_mask: 0.1632  loss_dice: 1.211  loss_ce_0: 0.5401  loss_mask_0: 0.185  loss_dice_0: 1.377  loss_ce_1: 0.6203  loss_mask_1: 0.1747  loss_dice_1: 1.302  loss_ce_2: 0.4946  loss_mask_2: 0.1696  loss_dice_2: 1.265  loss_ce_3: 0.504  loss_mask_3: 0.166  loss_dice_3: 1.255  loss_ce_4: 0.4678  loss_mask_4: 0.1659  loss_dice_4: 1.209  loss_ce_5: 0.4346  loss_mask_5: 0.166  loss_dice_5: 1.184  loss_ce_6: 0.4266  loss_mask_6: 0.1632  loss_dice_6: 1.182  loss_ce_7: 0.4285  loss_mask_7: 0.1638  loss_dice_7: 1.19  loss_ce_8: 0.4154  loss_mask_8: 0.1624  loss_dice_8: 1.196    time: 1.0967  last_time: 1.0620  data_time: 0.0989  last_data_time: 0.0708   lr: 0.0001  max_mem: 32630M
[10/19 14:36:43] d2.utils.events INFO:  eta: 0:23:51  iter: 1679  total_loss: 19.57  loss_ce: 0.4303  loss_mask: 0.1637  loss_dice: 1.236  loss_ce_0: 0.53  loss_mask_0: 0.1883  loss_dice_0: 1.459  loss_ce_1: 0.6213  loss_mask_1: 0.1766  loss_dice_1: 1.35  loss_ce_2: 0.5682  loss_mask_2: 0.1701  loss_dice_2: 1.299  loss_ce_3: 0.4949  loss_mask_3: 0.1673  loss_dice_3: 1.252  loss_ce_4: 0.4713  loss_mask_4: 0.169  loss_dice_4: 1.284  loss_ce_5: 0.4747  loss_mask_5: 0.1674  loss_dice_5: 1.259  loss_ce_6: 0.4571  loss_mask_6: 0.1655  loss_dice_6: 1.252  loss_ce_7: 0.4298  loss_mask_7: 0.1657  loss_dice_7: 1.239  loss_ce_8: 0.4289  loss_mask_8: 0.1653  loss_dice_8: 1.274    time: 1.0965  last_time: 1.0643  data_time: 0.0793  last_data_time: 0.0643   lr: 0.0001  max_mem: 32630M
[10/19 14:37:04] d2.utils.events INFO:  eta: 0:23:26  iter: 1699  total_loss: 20.06  loss_ce: 0.4291  loss_mask: 0.1629  loss_dice: 1.246  loss_ce_0: 0.5554  loss_mask_0: 0.193  loss_dice_0: 1.45  loss_ce_1: 0.6321  loss_mask_1: 0.177  loss_dice_1: 1.393  loss_ce_2: 0.5855  loss_mask_2: 0.1668  loss_dice_2: 1.287  loss_ce_3: 0.5377  loss_mask_3: 0.1689  loss_dice_3: 1.261  loss_ce_4: 0.5021  loss_mask_4: 0.1692  loss_dice_4: 1.279  loss_ce_5: 0.5003  loss_mask_5: 0.1652  loss_dice_5: 1.265  loss_ce_6: 0.4653  loss_mask_6: 0.1646  loss_dice_6: 1.241  loss_ce_7: 0.447  loss_mask_7: 0.1645  loss_dice_7: 1.233  loss_ce_8: 0.4483  loss_mask_8: 0.163  loss_dice_8: 1.257    time: 1.0962  last_time: 1.0655  data_time: 0.0746  last_data_time: 0.0799   lr: 0.0001  max_mem: 32630M
[10/19 14:37:26] d2.utils.events INFO:  eta: 0:23:03  iter: 1719  total_loss: 19.71  loss_ce: 0.4298  loss_mask: 0.167  loss_dice: 1.263  loss_ce_0: 0.5656  loss_mask_0: 0.1872  loss_dice_0: 1.439  loss_ce_1: 0.6284  loss_mask_1: 0.1758  loss_dice_1: 1.357  loss_ce_2: 0.5846  loss_mask_2: 0.1702  loss_dice_2: 1.322  loss_ce_3: 0.5386  loss_mask_3: 0.168  loss_dice_3: 1.251  loss_ce_4: 0.4893  loss_mask_4: 0.1661  loss_dice_4: 1.238  loss_ce_5: 0.5007  loss_mask_5: 0.1659  loss_dice_5: 1.243  loss_ce_6: 0.4826  loss_mask_6: 0.1637  loss_dice_6: 1.233  loss_ce_7: 0.4698  loss_mask_7: 0.1648  loss_dice_7: 1.213  loss_ce_8: 0.4485  loss_mask_8: 0.1658  loss_dice_8: 1.233    time: 1.0958  last_time: 1.0377  data_time: 0.0699  last_data_time: 0.0670   lr: 0.0001  max_mem: 32630M
[10/19 14:37:48] d2.utils.events INFO:  eta: 0:22:41  iter: 1739  total_loss: 20.2  loss_ce: 0.4472  loss_mask: 0.1551  loss_dice: 1.276  loss_ce_0: 0.6127  loss_mask_0: 0.1806  loss_dice_0: 1.435  loss_ce_1: 0.6527  loss_mask_1: 0.1656  loss_dice_1: 1.413  loss_ce_2: 0.6006  loss_mask_2: 0.1594  loss_dice_2: 1.385  loss_ce_3: 0.5252  loss_mask_3: 0.1566  loss_dice_3: 1.249  loss_ce_4: 0.4877  loss_mask_4: 0.157  loss_dice_4: 1.257  loss_ce_5: 0.4799  loss_mask_5: 0.1567  loss_dice_5: 1.287  loss_ce_6: 0.4904  loss_mask_6: 0.1569  loss_dice_6: 1.256  loss_ce_7: 0.4615  loss_mask_7: 0.1552  loss_dice_7: 1.254  loss_ce_8: 0.4412  loss_mask_8: 0.1556  loss_dice_8: 1.268    time: 1.0961  last_time: 1.1189  data_time: 0.0790  last_data_time: 0.0802   lr: 0.0001  max_mem: 32630M
[10/19 14:38:11] d2.utils.events INFO:  eta: 0:22:19  iter: 1759  total_loss: 18.5  loss_ce: 0.4047  loss_mask: 0.1616  loss_dice: 1.167  loss_ce_0: 0.5316  loss_mask_0: 0.19  loss_dice_0: 1.339  loss_ce_1: 0.5707  loss_mask_1: 0.1735  loss_dice_1: 1.283  loss_ce_2: 0.5241  loss_mask_2: 0.17  loss_dice_2: 1.225  loss_ce_3: 0.4628  loss_mask_3: 0.1654  loss_dice_3: 1.194  loss_ce_4: 0.4623  loss_mask_4: 0.1655  loss_dice_4: 1.2  loss_ce_5: 0.4328  loss_mask_5: 0.1651  loss_dice_5: 1.186  loss_ce_6: 0.4219  loss_mask_6: 0.1646  loss_dice_6: 1.147  loss_ce_7: 0.411  loss_mask_7: 0.1638  loss_dice_7: 1.183  loss_ce_8: 0.4154  loss_mask_8: 0.1638  loss_dice_8: 1.182    time: 1.0964  last_time: 1.0599  data_time: 0.0842  last_data_time: 0.0665   lr: 0.0001  max_mem: 32630M
[10/19 14:38:33] d2.utils.events INFO:  eta: 0:21:57  iter: 1779  total_loss: 19.86  loss_ce: 0.4292  loss_mask: 0.1654  loss_dice: 1.272  loss_ce_0: 0.5523  loss_mask_0: 0.1882  loss_dice_0: 1.467  loss_ce_1: 0.6291  loss_mask_1: 0.1745  loss_dice_1: 1.399  loss_ce_2: 0.558  loss_mask_2: 0.1708  loss_dice_2: 1.322  loss_ce_3: 0.5077  loss_mask_3: 0.1684  loss_dice_3: 1.282  loss_ce_4: 0.4987  loss_mask_4: 0.1672  loss_dice_4: 1.272  loss_ce_5: 0.4692  loss_mask_5: 0.1671  loss_dice_5: 1.275  loss_ce_6: 0.4526  loss_mask_6: 0.1654  loss_dice_6: 1.239  loss_ce_7: 0.4398  loss_mask_7: 0.1628  loss_dice_7: 1.234  loss_ce_8: 0.4536  loss_mask_8: 0.1645  loss_dice_8: 1.276    time: 1.0964  last_time: 1.0722  data_time: 0.0830  last_data_time: 0.1183   lr: 0.0001  max_mem: 32630M
[10/19 14:38:55] d2.utils.events INFO:  eta: 0:21:34  iter: 1799  total_loss: 19.16  loss_ce: 0.4032  loss_mask: 0.1629  loss_dice: 1.245  loss_ce_0: 0.5489  loss_mask_0: 0.1768  loss_dice_0: 1.442  loss_ce_1: 0.5896  loss_mask_1: 0.1743  loss_dice_1: 1.379  loss_ce_2: 0.5346  loss_mask_2: 0.1643  loss_dice_2: 1.304  loss_ce_3: 0.502  loss_mask_3: 0.1649  loss_dice_3: 1.262  loss_ce_4: 0.494  loss_mask_4: 0.1608  loss_dice_4: 1.267  loss_ce_5: 0.4528  loss_mask_5: 0.162  loss_dice_5: 1.224  loss_ce_6: 0.4398  loss_mask_6: 0.1622  loss_dice_6: 1.225  loss_ce_7: 0.4213  loss_mask_7: 0.162  loss_dice_7: 1.221  loss_ce_8: 0.4423  loss_mask_8: 0.164  loss_dice_8: 1.221    time: 1.0964  last_time: 1.1228  data_time: 0.0832  last_data_time: 0.0818   lr: 0.0001  max_mem: 32630M
[10/19 14:39:16] d2.utils.events INFO:  eta: 0:21:12  iter: 1819  total_loss: 19.25  loss_ce: 0.4423  loss_mask: 0.1761  loss_dice: 1.207  loss_ce_0: 0.5257  loss_mask_0: 0.2051  loss_dice_0: 1.41  loss_ce_1: 0.5921  loss_mask_1: 0.1862  loss_dice_1: 1.329  loss_ce_2: 0.5511  loss_mask_2: 0.1746  loss_dice_2: 1.281  loss_ce_3: 0.4843  loss_mask_3: 0.1752  loss_dice_3: 1.232  loss_ce_4: 0.4797  loss_mask_4: 0.1771  loss_dice_4: 1.244  loss_ce_5: 0.4613  loss_mask_5: 0.1727  loss_dice_5: 1.233  loss_ce_6: 0.4805  loss_mask_6: 0.1724  loss_dice_6: 1.203  loss_ce_7: 0.4494  loss_mask_7: 0.1732  loss_dice_7: 1.172  loss_ce_8: 0.4595  loss_mask_8: 0.1741  loss_dice_8: 1.188    time: 1.0963  last_time: 1.1420  data_time: 0.0770  last_data_time: 0.0766   lr: 0.0001  max_mem: 32637M
[10/19 14:39:38] d2.utils.events INFO:  eta: 0:20:50  iter: 1839  total_loss: 19.1  loss_ce: 0.4192  loss_mask: 0.1789  loss_dice: 1.19  loss_ce_0: 0.5628  loss_mask_0: 0.2027  loss_dice_0: 1.39  loss_ce_1: 0.5862  loss_mask_1: 0.1876  loss_dice_1: 1.317  loss_ce_2: 0.5533  loss_mask_2: 0.1842  loss_dice_2: 1.289  loss_ce_3: 0.474  loss_mask_3: 0.1826  loss_dice_3: 1.217  loss_ce_4: 0.4661  loss_mask_4: 0.1861  loss_dice_4: 1.2  loss_ce_5: 0.4693  loss_mask_5: 0.1818  loss_dice_5: 1.197  loss_ce_6: 0.4492  loss_mask_6: 0.179  loss_dice_6: 1.211  loss_ce_7: 0.4466  loss_mask_7: 0.1795  loss_dice_7: 1.192  loss_ce_8: 0.4529  loss_mask_8: 0.1778  loss_dice_8: 1.152    time: 1.0961  last_time: 1.1193  data_time: 0.0783  last_data_time: 0.0620   lr: 0.0001  max_mem: 32637M
[10/19 14:40:00] d2.utils.events INFO:  eta: 0:20:28  iter: 1859  total_loss: 18.81  loss_ce: 0.4184  loss_mask: 0.1526  loss_dice: 1.196  loss_ce_0: 0.4925  loss_mask_0: 0.1784  loss_dice_0: 1.382  loss_ce_1: 0.5889  loss_mask_1: 0.1669  loss_dice_1: 1.319  loss_ce_2: 0.5327  loss_mask_2: 0.1577  loss_dice_2: 1.253  loss_ce_3: 0.5135  loss_mask_3: 0.1582  loss_dice_3: 1.211  loss_ce_4: 0.4786  loss_mask_4: 0.1597  loss_dice_4: 1.173  loss_ce_5: 0.4601  loss_mask_5: 0.1548  loss_dice_5: 1.231  loss_ce_6: 0.4712  loss_mask_6: 0.1546  loss_dice_6: 1.151  loss_ce_7: 0.4292  loss_mask_7: 0.1513  loss_dice_7: 1.169  loss_ce_8: 0.4162  loss_mask_8: 0.1515  loss_dice_8: 1.155    time: 1.0961  last_time: 1.1019  data_time: 0.0815  last_data_time: 0.0795   lr: 0.0001  max_mem: 32637M
[10/19 14:40:22] d2.utils.events INFO:  eta: 0:20:06  iter: 1879  total_loss: 18.51  loss_ce: 0.4424  loss_mask: 0.1569  loss_dice: 1.144  loss_ce_0: 0.5626  loss_mask_0: 0.1873  loss_dice_0: 1.393  loss_ce_1: 0.6  loss_mask_1: 0.172  loss_dice_1: 1.275  loss_ce_2: 0.5469  loss_mask_2: 0.1618  loss_dice_2: 1.203  loss_ce_3: 0.5111  loss_mask_3: 0.1592  loss_dice_3: 1.148  loss_ce_4: 0.4941  loss_mask_4: 0.1607  loss_dice_4: 1.176  loss_ce_5: 0.4509  loss_mask_5: 0.1612  loss_dice_5: 1.174  loss_ce_6: 0.4606  loss_mask_6: 0.1588  loss_dice_6: 1.163  loss_ce_7: 0.4264  loss_mask_7: 0.1557  loss_dice_7: 1.153  loss_ce_8: 0.4633  loss_mask_8: 0.1555  loss_dice_8: 1.145    time: 1.0961  last_time: 1.1031  data_time: 0.0821  last_data_time: 0.0759   lr: 0.0001  max_mem: 32637M
[10/19 14:40:44] d2.utils.events INFO:  eta: 0:19:45  iter: 1899  total_loss: 18.85  loss_ce: 0.4077  loss_mask: 0.1599  loss_dice: 1.255  loss_ce_0: 0.5062  loss_mask_0: 0.1889  loss_dice_0: 1.453  loss_ce_1: 0.594  loss_mask_1: 0.17  loss_dice_1: 1.349  loss_ce_2: 0.5383  loss_mask_2: 0.1619  loss_dice_2: 1.302  loss_ce_3: 0.4817  loss_mask_3: 0.1616  loss_dice_3: 1.191  loss_ce_4: 0.4483  loss_mask_4: 0.1628  loss_dice_4: 1.208  loss_ce_5: 0.4536  loss_mask_5: 0.1631  loss_dice_5: 1.25  loss_ce_6: 0.443  loss_mask_6: 0.1598  loss_dice_6: 1.244  loss_ce_7: 0.4424  loss_mask_7: 0.1589  loss_dice_7: 1.203  loss_ce_8: 0.4136  loss_mask_8: 0.1593  loss_dice_8: 1.218    time: 1.0962  last_time: 1.2301  data_time: 0.0796  last_data_time: 0.0988   lr: 0.0001  max_mem: 32637M
[10/19 14:41:06] d2.utils.events INFO:  eta: 0:19:23  iter: 1919  total_loss: 18.3  loss_ce: 0.4124  loss_mask: 0.1567  loss_dice: 1.163  loss_ce_0: 0.546  loss_mask_0: 0.1811  loss_dice_0: 1.354  loss_ce_1: 0.6203  loss_mask_1: 0.1713  loss_dice_1: 1.257  loss_ce_2: 0.5711  loss_mask_2: 0.166  loss_dice_2: 1.183  loss_ce_3: 0.5157  loss_mask_3: 0.1619  loss_dice_3: 1.159  loss_ce_4: 0.4725  loss_mask_4: 0.1599  loss_dice_4: 1.156  loss_ce_5: 0.4338  loss_mask_5: 0.1592  loss_dice_5: 1.17  loss_ce_6: 0.4643  loss_mask_6: 0.1622  loss_dice_6: 1.129  loss_ce_7: 0.4468  loss_mask_7: 0.1594  loss_dice_7: 1.131  loss_ce_8: 0.441  loss_mask_8: 0.1608  loss_dice_8: 1.14    time: 1.0960  last_time: 1.0486  data_time: 0.0824  last_data_time: 0.0680   lr: 0.0001  max_mem: 32637M
[10/19 14:41:28] d2.utils.events INFO:  eta: 0:19:01  iter: 1939  total_loss: 18.24  loss_ce: 0.4182  loss_mask: 0.1587  loss_dice: 1.128  loss_ce_0: 0.5401  loss_mask_0: 0.1958  loss_dice_0: 1.341  loss_ce_1: 0.6018  loss_mask_1: 0.1792  loss_dice_1: 1.267  loss_ce_2: 0.5498  loss_mask_2: 0.1664  loss_dice_2: 1.198  loss_ce_3: 0.4682  loss_mask_3: 0.1687  loss_dice_3: 1.14  loss_ce_4: 0.4707  loss_mask_4: 0.1667  loss_dice_4: 1.151  loss_ce_5: 0.4315  loss_mask_5: 0.162  loss_dice_5: 1.153  loss_ce_6: 0.434  loss_mask_6: 0.1614  loss_dice_6: 1.129  loss_ce_7: 0.4482  loss_mask_7: 0.1602  loss_dice_7: 1.122  loss_ce_8: 0.4432  loss_mask_8: 0.1602  loss_dice_8: 1.132    time: 1.0959  last_time: 1.1519  data_time: 0.0776  last_data_time: 0.0771   lr: 0.0001  max_mem: 32637M
[10/19 14:41:50] d2.utils.events INFO:  eta: 0:18:39  iter: 1959  total_loss: 19.4  loss_ce: 0.4491  loss_mask: 0.1642  loss_dice: 1.18  loss_ce_0: 0.5408  loss_mask_0: 0.19  loss_dice_0: 1.411  loss_ce_1: 0.6074  loss_mask_1: 0.1785  loss_dice_1: 1.318  loss_ce_2: 0.5463  loss_mask_2: 0.1694  loss_dice_2: 1.26  loss_ce_3: 0.4848  loss_mask_3: 0.1679  loss_dice_3: 1.195  loss_ce_4: 0.4753  loss_mask_4: 0.1678  loss_dice_4: 1.198  loss_ce_5: 0.4435  loss_mask_5: 0.1658  loss_dice_5: 1.194  loss_ce_6: 0.4632  loss_mask_6: 0.1668  loss_dice_6: 1.177  loss_ce_7: 0.4593  loss_mask_7: 0.1624  loss_dice_7: 1.159  loss_ce_8: 0.4417  loss_mask_8: 0.1623  loss_dice_8: 1.196    time: 1.0961  last_time: 1.1800  data_time: 0.0880  last_data_time: 0.1733   lr: 0.0001  max_mem: 32637M
[10/19 14:42:12] d2.utils.events INFO:  eta: 0:18:18  iter: 1979  total_loss: 19.48  loss_ce: 0.4231  loss_mask: 0.1512  loss_dice: 1.265  loss_ce_0: 0.5465  loss_mask_0: 0.1709  loss_dice_0: 1.457  loss_ce_1: 0.6081  loss_mask_1: 0.1587  loss_dice_1: 1.379  loss_ce_2: 0.5667  loss_mask_2: 0.1524  loss_dice_2: 1.293  loss_ce_3: 0.4941  loss_mask_3: 0.1491  loss_dice_3: 1.258  loss_ce_4: 0.4815  loss_mask_4: 0.154  loss_dice_4: 1.249  loss_ce_5: 0.4749  loss_mask_5: 0.1505  loss_dice_5: 1.244  loss_ce_6: 0.4611  loss_mask_6: 0.1511  loss_dice_6: 1.233  loss_ce_7: 0.4753  loss_mask_7: 0.1509  loss_dice_7: 1.208  loss_ce_8: 0.4761  loss_mask_8: 0.1533  loss_dice_8: 1.233    time: 1.0961  last_time: 1.0823  data_time: 0.0767  last_data_time: 0.0762   lr: 0.0001  max_mem: 32637M
[10/19 14:42:34] d2.utils.events INFO:  eta: 0:17:57  iter: 1999  total_loss: 18.73  loss_ce: 0.4261  loss_mask: 0.1527  loss_dice: 1.176  loss_ce_0: 0.5631  loss_mask_0: 0.1952  loss_dice_0: 1.362  loss_ce_1: 0.5845  loss_mask_1: 0.1704  loss_dice_1: 1.332  loss_ce_2: 0.5389  loss_mask_2: 0.164  loss_dice_2: 1.221  loss_ce_3: 0.5104  loss_mask_3: 0.1618  loss_dice_3: 1.187  loss_ce_4: 0.4606  loss_mask_4: 0.1572  loss_dice_4: 1.183  loss_ce_5: 0.4502  loss_mask_5: 0.1567  loss_dice_5: 1.158  loss_ce_6: 0.4617  loss_mask_6: 0.1524  loss_dice_6: 1.164  loss_ce_7: 0.4276  loss_mask_7: 0.1547  loss_dice_7: 1.142  loss_ce_8: 0.4283  loss_mask_8: 0.1552  loss_dice_8: 1.186    time: 1.0962  last_time: 1.1607  data_time: 0.0786  last_data_time: 0.0826   lr: 0.0001  max_mem: 32637M
[10/19 14:42:56] d2.utils.events INFO:  eta: 0:17:35  iter: 2019  total_loss: 19.2  loss_ce: 0.3973  loss_mask: 0.1501  loss_dice: 1.248  loss_ce_0: 0.5436  loss_mask_0: 0.1802  loss_dice_0: 1.463  loss_ce_1: 0.602  loss_mask_1: 0.1636  loss_dice_1: 1.368  loss_ce_2: 0.5223  loss_mask_2: 0.1541  loss_dice_2: 1.319  loss_ce_3: 0.4842  loss_mask_3: 0.1533  loss_dice_3: 1.267  loss_ce_4: 0.4608  loss_mask_4: 0.1524  loss_dice_4: 1.277  loss_ce_5: 0.4429  loss_mask_5: 0.1493  loss_dice_5: 1.304  loss_ce_6: 0.4266  loss_mask_6: 0.1498  loss_dice_6: 1.243  loss_ce_7: 0.4148  loss_mask_7: 0.1495  loss_dice_7: 1.213  loss_ce_8: 0.4079  loss_mask_8: 0.1526  loss_dice_8: 1.269    time: 1.0962  last_time: 1.1475  data_time: 0.0793  last_data_time: 0.0952   lr: 0.0001  max_mem: 32637M
[10/19 14:43:18] d2.utils.events INFO:  eta: 0:17:14  iter: 2039  total_loss: 19.54  loss_ce: 0.4298  loss_mask: 0.1692  loss_dice: 1.255  loss_ce_0: 0.5952  loss_mask_0: 0.1937  loss_dice_0: 1.392  loss_ce_1: 0.6361  loss_mask_1: 0.1837  loss_dice_1: 1.323  loss_ce_2: 0.5775  loss_mask_2: 0.1744  loss_dice_2: 1.307  loss_ce_3: 0.4881  loss_mask_3: 0.1716  loss_dice_3: 1.251  loss_ce_4: 0.4626  loss_mask_4: 0.1716  loss_dice_4: 1.234  loss_ce_5: 0.4799  loss_mask_5: 0.1688  loss_dice_5: 1.254  loss_ce_6: 0.4761  loss_mask_6: 0.1712  loss_dice_6: 1.239  loss_ce_7: 0.4497  loss_mask_7: 0.1693  loss_dice_7: 1.24  loss_ce_8: 0.4462  loss_mask_8: 0.1686  loss_dice_8: 1.225    time: 1.0963  last_time: 1.0489  data_time: 0.0829  last_data_time: 0.0798   lr: 0.0001  max_mem: 32637M
[10/19 14:43:40] d2.utils.events INFO:  eta: 0:16:53  iter: 2059  total_loss: 18.93  loss_ce: 0.4518  loss_mask: 0.1701  loss_dice: 1.191  loss_ce_0: 0.5095  loss_mask_0: 0.1969  loss_dice_0: 1.423  loss_ce_1: 0.5604  loss_mask_1: 0.1794  loss_dice_1: 1.35  loss_ce_2: 0.4914  loss_mask_2: 0.1745  loss_dice_2: 1.267  loss_ce_3: 0.4788  loss_mask_3: 0.1741  loss_dice_3: 1.254  loss_ce_4: 0.4625  loss_mask_4: 0.1709  loss_dice_4: 1.209  loss_ce_5: 0.4502  loss_mask_5: 0.1697  loss_dice_5: 1.251  loss_ce_6: 0.4791  loss_mask_6: 0.1716  loss_dice_6: 1.224  loss_ce_7: 0.4546  loss_mask_7: 0.1705  loss_dice_7: 1.236  loss_ce_8: 0.4261  loss_mask_8: 0.1696  loss_dice_8: 1.239    time: 1.0964  last_time: 1.2162  data_time: 0.0825  last_data_time: 0.0739   lr: 0.0001  max_mem: 32637M
[10/19 14:44:02] d2.utils.events INFO:  eta: 0:16:32  iter: 2079  total_loss: 19.95  loss_ce: 0.4887  loss_mask: 0.1696  loss_dice: 1.206  loss_ce_0: 0.5786  loss_mask_0: 0.1903  loss_dice_0: 1.41  loss_ce_1: 0.6059  loss_mask_1: 0.1822  loss_dice_1: 1.313  loss_ce_2: 0.5383  loss_mask_2: 0.1744  loss_dice_2: 1.283  loss_ce_3: 0.5239  loss_mask_3: 0.1733  loss_dice_3: 1.239  loss_ce_4: 0.5011  loss_mask_4: 0.1733  loss_dice_4: 1.234  loss_ce_5: 0.5027  loss_mask_5: 0.1722  loss_dice_5: 1.232  loss_ce_6: 0.506  loss_mask_6: 0.1721  loss_dice_6: 1.21  loss_ce_7: 0.4752  loss_mask_7: 0.173  loss_dice_7: 1.231  loss_ce_8: 0.5071  loss_mask_8: 0.167  loss_dice_8: 1.2    time: 1.0961  last_time: 1.0871  data_time: 0.0787  last_data_time: 0.0956   lr: 0.0001  max_mem: 32637M
[10/19 14:44:24] d2.utils.events INFO:  eta: 0:16:10  iter: 2099  total_loss: 18.69  loss_ce: 0.449  loss_mask: 0.1661  loss_dice: 1.203  loss_ce_0: 0.5161  loss_mask_0: 0.1923  loss_dice_0: 1.389  loss_ce_1: 0.52  loss_mask_1: 0.1764  loss_dice_1: 1.335  loss_ce_2: 0.5025  loss_mask_2: 0.1782  loss_dice_2: 1.288  loss_ce_3: 0.4471  loss_mask_3: 0.1702  loss_dice_3: 1.217  loss_ce_4: 0.4256  loss_mask_4: 0.1694  loss_dice_4: 1.22  loss_ce_5: 0.4329  loss_mask_5: 0.1683  loss_dice_5: 1.204  loss_ce_6: 0.4272  loss_mask_6: 0.1644  loss_dice_6: 1.19  loss_ce_7: 0.4593  loss_mask_7: 0.1659  loss_dice_7: 1.189  loss_ce_8: 0.4511  loss_mask_8: 0.1667  loss_dice_8: 1.14    time: 1.0960  last_time: 1.1443  data_time: 0.0793  last_data_time: 0.0941   lr: 0.0001  max_mem: 32637M
[10/19 14:44:46] d2.utils.events INFO:  eta: 0:15:50  iter: 2119  total_loss: 18.74  loss_ce: 0.4564  loss_mask: 0.1629  loss_dice: 1.156  loss_ce_0: 0.5279  loss_mask_0: 0.1799  loss_dice_0: 1.348  loss_ce_1: 0.5828  loss_mask_1: 0.1682  loss_dice_1: 1.362  loss_ce_2: 0.532  loss_mask_2: 0.1651  loss_dice_2: 1.263  loss_ce_3: 0.4766  loss_mask_3: 0.165  loss_dice_3: 1.213  loss_ce_4: 0.4542  loss_mask_4: 0.163  loss_dice_4: 1.228  loss_ce_5: 0.457  loss_mask_5: 0.1651  loss_dice_5: 1.195  loss_ce_6: 0.4557  loss_mask_6: 0.1647  loss_dice_6: 1.145  loss_ce_7: 0.4598  loss_mask_7: 0.1644  loss_dice_7: 1.175  loss_ce_8: 0.454  loss_mask_8: 0.1639  loss_dice_8: 1.159    time: 1.0961  last_time: 1.0872  data_time: 0.0779  last_data_time: 0.0607   lr: 0.0001  max_mem: 32637M
[10/19 14:45:08] d2.utils.events INFO:  eta: 0:15:28  iter: 2139  total_loss: 18.22  loss_ce: 0.4672  loss_mask: 0.1737  loss_dice: 1.096  loss_ce_0: 0.5242  loss_mask_0: 0.2079  loss_dice_0: 1.31  loss_ce_1: 0.5555  loss_mask_1: 0.1918  loss_dice_1: 1.235  loss_ce_2: 0.5174  loss_mask_2: 0.1857  loss_dice_2: 1.169  loss_ce_3: 0.4637  loss_mask_3: 0.1853  loss_dice_3: 1.168  loss_ce_4: 0.4678  loss_mask_4: 0.1815  loss_dice_4: 1.131  loss_ce_5: 0.444  loss_mask_5: 0.1746  loss_dice_5: 1.112  loss_ce_6: 0.4596  loss_mask_6: 0.1778  loss_dice_6: 1.086  loss_ce_7: 0.4347  loss_mask_7: 0.1766  loss_dice_7: 1.134  loss_ce_8: 0.4766  loss_mask_8: 0.1768  loss_dice_8: 1.124    time: 1.0960  last_time: 1.1069  data_time: 0.0777  last_data_time: 0.0755   lr: 0.0001  max_mem: 32637M
[10/19 14:45:30] d2.utils.events INFO:  eta: 0:15:07  iter: 2159  total_loss: 18.79  loss_ce: 0.4786  loss_mask: 0.1684  loss_dice: 1.135  loss_ce_0: 0.5558  loss_mask_0: 0.1878  loss_dice_0: 1.347  loss_ce_1: 0.5924  loss_mask_1: 0.1827  loss_dice_1: 1.262  loss_ce_2: 0.5451  loss_mask_2: 0.172  loss_dice_2: 1.207  loss_ce_3: 0.5025  loss_mask_3: 0.1711  loss_dice_3: 1.15  loss_ce_4: 0.4898  loss_mask_4: 0.1726  loss_dice_4: 1.139  loss_ce_5: 0.488  loss_mask_5: 0.1721  loss_dice_5: 1.149  loss_ce_6: 0.4614  loss_mask_6: 0.1704  loss_dice_6: 1.131  loss_ce_7: 0.4823  loss_mask_7: 0.1716  loss_dice_7: 1.147  loss_ce_8: 0.4352  loss_mask_8: 0.1707  loss_dice_8: 1.142    time: 1.0960  last_time: 1.1145  data_time: 0.0770  last_data_time: 0.0734   lr: 0.0001  max_mem: 32637M
[10/19 14:45:52] d2.utils.events INFO:  eta: 0:14:47  iter: 2179  total_loss: 19.88  loss_ce: 0.4465  loss_mask: 0.1469  loss_dice: 1.291  loss_ce_0: 0.5194  loss_mask_0: 0.1687  loss_dice_0: 1.441  loss_ce_1: 0.5958  loss_mask_1: 0.161  loss_dice_1: 1.407  loss_ce_2: 0.5404  loss_mask_2: 0.1508  loss_dice_2: 1.323  loss_ce_3: 0.5057  loss_mask_3: 0.1507  loss_dice_3: 1.274  loss_ce_4: 0.4939  loss_mask_4: 0.1495  loss_dice_4: 1.295  loss_ce_5: 0.4798  loss_mask_5: 0.1486  loss_dice_5: 1.303  loss_ce_6: 0.4378  loss_mask_6: 0.1481  loss_dice_6: 1.275  loss_ce_7: 0.4457  loss_mask_7: 0.1482  loss_dice_7: 1.29  loss_ce_8: 0.4462  loss_mask_8: 0.1483  loss_dice_8: 1.248    time: 1.0964  last_time: 1.0510  data_time: 0.0864  last_data_time: 0.0835   lr: 0.0001  max_mem: 32900M
[10/19 14:46:15] d2.utils.events INFO:  eta: 0:14:26  iter: 2199  total_loss: 19.29  loss_ce: 0.4579  loss_mask: 0.1518  loss_dice: 1.228  loss_ce_0: 0.6211  loss_mask_0: 0.1722  loss_dice_0: 1.379  loss_ce_1: 0.6295  loss_mask_1: 0.1627  loss_dice_1: 1.333  loss_ce_2: 0.6163  loss_mask_2: 0.1559  loss_dice_2: 1.258  loss_ce_3: 0.4941  loss_mask_3: 0.1558  loss_dice_3: 1.243  loss_ce_4: 0.484  loss_mask_4: 0.1557  loss_dice_4: 1.192  loss_ce_5: 0.4764  loss_mask_5: 0.152  loss_dice_5: 1.216  loss_ce_6: 0.4558  loss_mask_6: 0.1531  loss_dice_6: 1.177  loss_ce_7: 0.4697  loss_mask_7: 0.1549  loss_dice_7: 1.193  loss_ce_8: 0.4713  loss_mask_8: 0.1531  loss_dice_8: 1.205    time: 1.0966  last_time: 1.0548  data_time: 0.0888  last_data_time: 0.0782   lr: 0.0001  max_mem: 32900M
[10/19 14:46:37] d2.utils.events INFO:  eta: 0:14:05  iter: 2219  total_loss: 19.68  loss_ce: 0.4468  loss_mask: 0.1515  loss_dice: 1.229  loss_ce_0: 0.5615  loss_mask_0: 0.1752  loss_dice_0: 1.42  loss_ce_1: 0.6067  loss_mask_1: 0.1652  loss_dice_1: 1.345  loss_ce_2: 0.5466  loss_mask_2: 0.1608  loss_dice_2: 1.315  loss_ce_3: 0.4891  loss_mask_3: 0.1581  loss_dice_3: 1.249  loss_ce_4: 0.4666  loss_mask_4: 0.1636  loss_dice_4: 1.267  loss_ce_5: 0.4555  loss_mask_5: 0.1503  loss_dice_5: 1.242  loss_ce_6: 0.4564  loss_mask_6: 0.1537  loss_dice_6: 1.224  loss_ce_7: 0.4508  loss_mask_7: 0.1549  loss_dice_7: 1.208  loss_ce_8: 0.4453  loss_mask_8: 0.1537  loss_dice_8: 1.243    time: 1.0967  last_time: 1.0956  data_time: 0.0841  last_data_time: 0.1025   lr: 0.0001  max_mem: 32900M
[10/19 14:46:59] d2.utils.events INFO:  eta: 0:13:44  iter: 2239  total_loss: 19.23  loss_ce: 0.4278  loss_mask: 0.1617  loss_dice: 1.181  loss_ce_0: 0.5375  loss_mask_0: 0.1843  loss_dice_0: 1.395  loss_ce_1: 0.6199  loss_mask_1: 0.1753  loss_dice_1: 1.342  loss_ce_2: 0.5593  loss_mask_2: 0.1672  loss_dice_2: 1.268  loss_ce_3: 0.532  loss_mask_3: 0.1656  loss_dice_3: 1.229  loss_ce_4: 0.4906  loss_mask_4: 0.1654  loss_dice_4: 1.209  loss_ce_5: 0.4538  loss_mask_5: 0.1649  loss_dice_5: 1.181  loss_ce_6: 0.4232  loss_mask_6: 0.1632  loss_dice_6: 1.182  loss_ce_7: 0.4552  loss_mask_7: 0.1631  loss_dice_7: 1.155  loss_ce_8: 0.4472  loss_mask_8: 0.164  loss_dice_8: 1.205    time: 1.0966  last_time: 1.1129  data_time: 0.0827  last_data_time: 0.0908   lr: 0.0001  max_mem: 32900M
[10/19 14:47:21] d2.utils.events INFO:  eta: 0:13:22  iter: 2259  total_loss: 19.16  loss_ce: 0.4159  loss_mask: 0.1612  loss_dice: 1.23  loss_ce_0: 0.5351  loss_mask_0: 0.195  loss_dice_0: 1.428  loss_ce_1: 0.574  loss_mask_1: 0.1717  loss_dice_1: 1.374  loss_ce_2: 0.549  loss_mask_2: 0.166  loss_dice_2: 1.333  loss_ce_3: 0.4921  loss_mask_3: 0.1648  loss_dice_3: 1.262  loss_ce_4: 0.4561  loss_mask_4: 0.1635  loss_dice_4: 1.248  loss_ce_5: 0.4496  loss_mask_5: 0.1611  loss_dice_5: 1.255  loss_ce_6: 0.4511  loss_mask_6: 0.1642  loss_dice_6: 1.234  loss_ce_7: 0.43  loss_mask_7: 0.1615  loss_dice_7: 1.226  loss_ce_8: 0.4363  loss_mask_8: 0.163  loss_dice_8: 1.223    time: 1.0966  last_time: 1.0576  data_time: 0.0816  last_data_time: 0.0731   lr: 0.0001  max_mem: 32900M
[10/19 14:47:43] d2.utils.events INFO:  eta: 0:13:01  iter: 2279  total_loss: 20.2  loss_ce: 0.4168  loss_mask: 0.1508  loss_dice: 1.288  loss_ce_0: 0.5712  loss_mask_0: 0.1822  loss_dice_0: 1.48  loss_ce_1: 0.6195  loss_mask_1: 0.1664  loss_dice_1: 1.463  loss_ce_2: 0.5232  loss_mask_2: 0.1603  loss_dice_2: 1.379  loss_ce_3: 0.4839  loss_mask_3: 0.1562  loss_dice_3: 1.336  loss_ce_4: 0.4589  loss_mask_4: 0.155  loss_dice_4: 1.304  loss_ce_5: 0.4649  loss_mask_5: 0.1542  loss_dice_5: 1.323  loss_ce_6: 0.4453  loss_mask_6: 0.1508  loss_dice_6: 1.325  loss_ce_7: 0.4388  loss_mask_7: 0.1528  loss_dice_7: 1.29  loss_ce_8: 0.4478  loss_mask_8: 0.1551  loss_dice_8: 1.293    time: 1.0965  last_time: 1.0681  data_time: 0.0804  last_data_time: 0.0709   lr: 0.0001  max_mem: 32900M
[10/19 14:48:04] d2.utils.events INFO:  eta: 0:12:39  iter: 2299  total_loss: 17.79  loss_ce: 0.3743  loss_mask: 0.1517  loss_dice: 1.14  loss_ce_0: 0.5142  loss_mask_0: 0.1841  loss_dice_0: 1.34  loss_ce_1: 0.5595  loss_mask_1: 0.1616  loss_dice_1: 1.259  loss_ce_2: 0.4862  loss_mask_2: 0.1568  loss_dice_2: 1.177  loss_ce_3: 0.4398  loss_mask_3: 0.1591  loss_dice_3: 1.185  loss_ce_4: 0.3868  loss_mask_4: 0.1571  loss_dice_4: 1.155  loss_ce_5: 0.3998  loss_mask_5: 0.157  loss_dice_5: 1.121  loss_ce_6: 0.3868  loss_mask_6: 0.1542  loss_dice_6: 1.178  loss_ce_7: 0.3655  loss_mask_7: 0.1534  loss_dice_7: 1.13  loss_ce_8: 0.3696  loss_mask_8: 0.156  loss_dice_8: 1.123    time: 1.0963  last_time: 1.0320  data_time: 0.0794  last_data_time: 0.0918   lr: 0.0001  max_mem: 32900M
[10/19 14:48:26] d2.utils.events INFO:  eta: 0:12:17  iter: 2319  total_loss: 17.47  loss_ce: 0.3842  loss_mask: 0.152  loss_dice: 1.092  loss_ce_0: 0.5176  loss_mask_0: 0.1885  loss_dice_0: 1.288  loss_ce_1: 0.5147  loss_mask_1: 0.1644  loss_dice_1: 1.232  loss_ce_2: 0.4836  loss_mask_2: 0.1592  loss_dice_2: 1.149  loss_ce_3: 0.4154  loss_mask_3: 0.1583  loss_dice_3: 1.109  loss_ce_4: 0.4212  loss_mask_4: 0.1577  loss_dice_4: 1.125  loss_ce_5: 0.3891  loss_mask_5: 0.1547  loss_dice_5: 1.12  loss_ce_6: 0.4177  loss_mask_6: 0.1529  loss_dice_6: 1.087  loss_ce_7: 0.4072  loss_mask_7: 0.1509  loss_dice_7: 1.093  loss_ce_8: 0.4127  loss_mask_8: 0.1525  loss_dice_8: 1.078    time: 1.0961  last_time: 1.0809  data_time: 0.0807  last_data_time: 0.0580   lr: 0.0001  max_mem: 32900M
[10/19 14:48:47] d2.utils.events INFO:  eta: 0:11:55  iter: 2339  total_loss: 19.78  loss_ce: 0.4326  loss_mask: 0.1519  loss_dice: 1.273  loss_ce_0: 0.5583  loss_mask_0: 0.1829  loss_dice_0: 1.486  loss_ce_1: 0.6179  loss_mask_1: 0.1671  loss_dice_1: 1.408  loss_ce_2: 0.593  loss_mask_2: 0.1582  loss_dice_2: 1.337  loss_ce_3: 0.4884  loss_mask_3: 0.1541  loss_dice_3: 1.301  loss_ce_4: 0.4664  loss_mask_4: 0.1506  loss_dice_4: 1.306  loss_ce_5: 0.472  loss_mask_5: 0.1489  loss_dice_5: 1.271  loss_ce_6: 0.4474  loss_mask_6: 0.1511  loss_dice_6: 1.29  loss_ce_7: 0.4637  loss_mask_7: 0.1482  loss_dice_7: 1.257  loss_ce_8: 0.4664  loss_mask_8: 0.1498  loss_dice_8: 1.259    time: 1.0960  last_time: 1.1016  data_time: 0.0794  last_data_time: 0.0726   lr: 0.0001  max_mem: 32900M
[10/19 14:49:09] d2.utils.events INFO:  eta: 0:11:34  iter: 2359  total_loss: 18.38  loss_ce: 0.3974  loss_mask: 0.1438  loss_dice: 1.161  loss_ce_0: 0.5468  loss_mask_0: 0.166  loss_dice_0: 1.388  loss_ce_1: 0.5447  loss_mask_1: 0.1518  loss_dice_1: 1.305  loss_ce_2: 0.5352  loss_mask_2: 0.1476  loss_dice_2: 1.266  loss_ce_3: 0.4636  loss_mask_3: 0.1437  loss_dice_3: 1.185  loss_ce_4: 0.4351  loss_mask_4: 0.1411  loss_dice_4: 1.181  loss_ce_5: 0.4184  loss_mask_5: 0.1423  loss_dice_5: 1.202  loss_ce_6: 0.4264  loss_mask_6: 0.1454  loss_dice_6: 1.188  loss_ce_7: 0.4071  loss_mask_7: 0.1468  loss_dice_7: 1.158  loss_ce_8: 0.4479  loss_mask_8: 0.145  loss_dice_8: 1.164    time: 1.0958  last_time: 1.0575  data_time: 0.0785  last_data_time: 0.0698   lr: 0.0001  max_mem: 32900M
[10/19 14:49:30] d2.utils.events INFO:  eta: 0:11:12  iter: 2379  total_loss: 19.19  loss_ce: 0.439  loss_mask: 0.1515  loss_dice: 1.239  loss_ce_0: 0.5585  loss_mask_0: 0.1777  loss_dice_0: 1.368  loss_ce_1: 0.5958  loss_mask_1: 0.1681  loss_dice_1: 1.307  loss_ce_2: 0.5533  loss_mask_2: 0.1582  loss_dice_2: 1.293  loss_ce_3: 0.4997  loss_mask_3: 0.1604  loss_dice_3: 1.247  loss_ce_4: 0.4656  loss_mask_4: 0.155  loss_dice_4: 1.218  loss_ce_5: 0.4674  loss_mask_5: 0.1536  loss_dice_5: 1.218  loss_ce_6: 0.44  loss_mask_6: 0.1515  loss_dice_6: 1.211  loss_ce_7: 0.4391  loss_mask_7: 0.1523  loss_dice_7: 1.211  loss_ce_8: 0.4723  loss_mask_8: 0.1499  loss_dice_8: 1.181    time: 1.0955  last_time: 1.0984  data_time: 0.0739  last_data_time: 0.0832   lr: 0.0001  max_mem: 32900M
[10/19 14:49:52] d2.utils.events INFO:  eta: 0:10:50  iter: 2399  total_loss: 19  loss_ce: 0.4287  loss_mask: 0.1595  loss_dice: 1.193  loss_ce_0: 0.5584  loss_mask_0: 0.1965  loss_dice_0: 1.418  loss_ce_1: 0.5922  loss_mask_1: 0.1771  loss_dice_1: 1.319  loss_ce_2: 0.5598  loss_mask_2: 0.1703  loss_dice_2: 1.283  loss_ce_3: 0.4847  loss_mask_3: 0.1686  loss_dice_3: 1.232  loss_ce_4: 0.4497  loss_mask_4: 0.1611  loss_dice_4: 1.183  loss_ce_5: 0.4532  loss_mask_5: 0.161  loss_dice_5: 1.207  loss_ce_6: 0.4374  loss_mask_6: 0.1601  loss_dice_6: 1.15  loss_ce_7: 0.4331  loss_mask_7: 0.1595  loss_dice_7: 1.179  loss_ce_8: 0.4296  loss_mask_8: 0.1606  loss_dice_8: 1.172    time: 1.0953  last_time: 1.0805  data_time: 0.0754  last_data_time: 0.0963   lr: 0.0001  max_mem: 32900M
[10/19 14:50:13] d2.utils.events INFO:  eta: 0:10:28  iter: 2419  total_loss: 18.49  loss_ce: 0.394  loss_mask: 0.156  loss_dice: 1.17  loss_ce_0: 0.538  loss_mask_0: 0.1792  loss_dice_0: 1.378  loss_ce_1: 0.5423  loss_mask_1: 0.1704  loss_dice_1: 1.276  loss_ce_2: 0.5166  loss_mask_2: 0.1617  loss_dice_2: 1.213  loss_ce_3: 0.4458  loss_mask_3: 0.1589  loss_dice_3: 1.19  loss_ce_4: 0.4124  loss_mask_4: 0.1614  loss_dice_4: 1.191  loss_ce_5: 0.415  loss_mask_5: 0.164  loss_dice_5: 1.172  loss_ce_6: 0.3964  loss_mask_6: 0.1605  loss_dice_6: 1.124  loss_ce_7: 0.4102  loss_mask_7: 0.1576  loss_dice_7: 1.146  loss_ce_8: 0.4118  loss_mask_8: 0.1575  loss_dice_8: 1.175    time: 1.0951  last_time: 1.0271  data_time: 0.0759  last_data_time: 0.0752   lr: 0.0001  max_mem: 32900M
[10/19 14:50:34] d2.utils.events INFO:  eta: 0:10:06  iter: 2439  total_loss: 19.39  loss_ce: 0.4403  loss_mask: 0.1542  loss_dice: 1.194  loss_ce_0: 0.5827  loss_mask_0: 0.1789  loss_dice_0: 1.416  loss_ce_1: 0.6271  loss_mask_1: 0.1623  loss_dice_1: 1.312  loss_ce_2: 0.5462  loss_mask_2: 0.1577  loss_dice_2: 1.254  loss_ce_3: 0.4933  loss_mask_3: 0.1556  loss_dice_3: 1.242  loss_ce_4: 0.4494  loss_mask_4: 0.1596  loss_dice_4: 1.22  loss_ce_5: 0.4598  loss_mask_5: 0.1603  loss_dice_5: 1.22  loss_ce_6: 0.4667  loss_mask_6: 0.1579  loss_dice_6: 1.198  loss_ce_7: 0.4535  loss_mask_7: 0.157  loss_dice_7: 1.204  loss_ce_8: 0.4424  loss_mask_8: 0.155  loss_dice_8: 1.207    time: 1.0948  last_time: 1.0162  data_time: 0.0716  last_data_time: 0.0738   lr: 0.0001  max_mem: 32900M
[10/19 14:50:56] d2.utils.events INFO:  eta: 0:09:45  iter: 2459  total_loss: 19.53  loss_ce: 0.4179  loss_mask: 0.1693  loss_dice: 1.222  loss_ce_0: 0.5373  loss_mask_0: 0.1968  loss_dice_0: 1.424  loss_ce_1: 0.5891  loss_mask_1: 0.1815  loss_dice_1: 1.366  loss_ce_2: 0.5428  loss_mask_2: 0.1692  loss_dice_2: 1.288  loss_ce_3: 0.4542  loss_mask_3: 0.1727  loss_dice_3: 1.266  loss_ce_4: 0.4513  loss_mask_4: 0.1704  loss_dice_4: 1.268  loss_ce_5: 0.4519  loss_mask_5: 0.1709  loss_dice_5: 1.254  loss_ce_6: 0.4043  loss_mask_6: 0.1702  loss_dice_6: 1.234  loss_ce_7: 0.4314  loss_mask_7: 0.1709  loss_dice_7: 1.225  loss_ce_8: 0.4397  loss_mask_8: 0.1691  loss_dice_8: 1.198    time: 1.0946  last_time: 1.0608  data_time: 0.0712  last_data_time: 0.0658   lr: 0.0001  max_mem: 32900M
[10/19 14:51:18] d2.utils.events INFO:  eta: 0:09:23  iter: 2479  total_loss: 18.76  loss_ce: 0.4341  loss_mask: 0.1576  loss_dice: 1.175  loss_ce_0: 0.5473  loss_mask_0: 0.1812  loss_dice_0: 1.37  loss_ce_1: 0.554  loss_mask_1: 0.1716  loss_dice_1: 1.295  loss_ce_2: 0.5639  loss_mask_2: 0.1659  loss_dice_2: 1.255  loss_ce_3: 0.4756  loss_mask_3: 0.1619  loss_dice_3: 1.198  loss_ce_4: 0.4933  loss_mask_4: 0.1627  loss_dice_4: 1.182  loss_ce_5: 0.4278  loss_mask_5: 0.1619  loss_dice_5: 1.188  loss_ce_6: 0.4043  loss_mask_6: 0.1624  loss_dice_6: 1.157  loss_ce_7: 0.4127  loss_mask_7: 0.1619  loss_dice_7: 1.145  loss_ce_8: 0.4328  loss_mask_8: 0.1628  loss_dice_8: 1.184    time: 1.0946  last_time: 1.0499  data_time: 0.0799  last_data_time: 0.0804   lr: 0.0001  max_mem: 32900M
[10/19 14:51:39] d2.utils.events INFO:  eta: 0:09:01  iter: 2499  total_loss: 19.07  loss_ce: 0.4229  loss_mask: 0.1602  loss_dice: 1.218  loss_ce_0: 0.54  loss_mask_0: 0.1781  loss_dice_0: 1.422  loss_ce_1: 0.5797  loss_mask_1: 0.1655  loss_dice_1: 1.358  loss_ce_2: 0.5479  loss_mask_2: 0.1615  loss_dice_2: 1.307  loss_ce_3: 0.5076  loss_mask_3: 0.1659  loss_dice_3: 1.216  loss_ce_4: 0.4723  loss_mask_4: 0.1601  loss_dice_4: 1.231  loss_ce_5: 0.4473  loss_mask_5: 0.1605  loss_dice_5: 1.227  loss_ce_6: 0.4369  loss_mask_6: 0.1603  loss_dice_6: 1.223  loss_ce_7: 0.432  loss_mask_7: 0.1597  loss_dice_7: 1.267  loss_ce_8: 0.413  loss_mask_8: 0.1594  loss_dice_8: 1.244    time: 1.0945  last_time: 1.1226  data_time: 0.0769  last_data_time: 0.0946   lr: 0.0001  max_mem: 32900M
[10/19 14:52:01] d2.utils.events INFO:  eta: 0:08:40  iter: 2519  total_loss: 18.65  loss_ce: 0.4471  loss_mask: 0.1607  loss_dice: 1.163  loss_ce_0: 0.5338  loss_mask_0: 0.1929  loss_dice_0: 1.358  loss_ce_1: 0.5876  loss_mask_1: 0.1712  loss_dice_1: 1.24  loss_ce_2: 0.5308  loss_mask_2: 0.164  loss_dice_2: 1.247  loss_ce_3: 0.4997  loss_mask_3: 0.1666  loss_dice_3: 1.174  loss_ce_4: 0.4961  loss_mask_4: 0.1641  loss_dice_4: 1.199  loss_ce_5: 0.479  loss_mask_5: 0.1611  loss_dice_5: 1.165  loss_ce_6: 0.4275  loss_mask_6: 0.1612  loss_dice_6: 1.187  loss_ce_7: 0.4416  loss_mask_7: 0.16  loss_dice_7: 1.16  loss_ce_8: 0.4281  loss_mask_8: 0.1629  loss_dice_8: 1.18    time: 1.0944  last_time: 1.0166  data_time: 0.0795  last_data_time: 0.0704   lr: 0.0001  max_mem: 32900M
[10/19 14:52:23] d2.utils.events INFO:  eta: 0:08:18  iter: 2539  total_loss: 18.17  loss_ce: 0.411  loss_mask: 0.1561  loss_dice: 1.158  loss_ce_0: 0.5295  loss_mask_0: 0.1944  loss_dice_0: 1.35  loss_ce_1: 0.5466  loss_mask_1: 0.1814  loss_dice_1: 1.267  loss_ce_2: 0.5085  loss_mask_2: 0.1633  loss_dice_2: 1.218  loss_ce_3: 0.4708  loss_mask_3: 0.1614  loss_dice_3: 1.178  loss_ce_4: 0.4255  loss_mask_4: 0.1609  loss_dice_4: 1.203  loss_ce_5: 0.4418  loss_mask_5: 0.161  loss_dice_5: 1.153  loss_ce_6: 0.417  loss_mask_6: 0.161  loss_dice_6: 1.15  loss_ce_7: 0.4139  loss_mask_7: 0.1591  loss_dice_7: 1.15  loss_ce_8: 0.4131  loss_mask_8: 0.1579  loss_dice_8: 1.153    time: 1.0942  last_time: 1.0701  data_time: 0.0801  last_data_time: 0.0792   lr: 0.0001  max_mem: 32900M
[10/19 14:52:45] d2.utils.events INFO:  eta: 0:07:57  iter: 2559  total_loss: 19.67  loss_ce: 0.426  loss_mask: 0.1574  loss_dice: 1.264  loss_ce_0: 0.5697  loss_mask_0: 0.182  loss_dice_0: 1.455  loss_ce_1: 0.5951  loss_mask_1: 0.1636  loss_dice_1: 1.402  loss_ce_2: 0.5577  loss_mask_2: 0.1536  loss_dice_2: 1.326  loss_ce_3: 0.4962  loss_mask_3: 0.1577  loss_dice_3: 1.328  loss_ce_4: 0.4743  loss_mask_4: 0.1559  loss_dice_4: 1.316  loss_ce_5: 0.4805  loss_mask_5: 0.1562  loss_dice_5: 1.291  loss_ce_6: 0.4429  loss_mask_6: 0.1569  loss_dice_6: 1.256  loss_ce_7: 0.4296  loss_mask_7: 0.1576  loss_dice_7: 1.277  loss_ce_8: 0.4296  loss_mask_8: 0.159  loss_dice_8: 1.287    time: 1.0944  last_time: 1.1004  data_time: 0.0906  last_data_time: 0.1035   lr: 0.0001  max_mem: 32900M
[10/19 14:53:07] d2.utils.events INFO:  eta: 0:07:35  iter: 2579  total_loss: 18.44  loss_ce: 0.4269  loss_mask: 0.1535  loss_dice: 1.189  loss_ce_0: 0.5258  loss_mask_0: 0.1856  loss_dice_0: 1.359  loss_ce_1: 0.5681  loss_mask_1: 0.1646  loss_dice_1: 1.277  loss_ce_2: 0.5451  loss_mask_2: 0.1547  loss_dice_2: 1.263  loss_ce_3: 0.4898  loss_mask_3: 0.1577  loss_dice_3: 1.203  loss_ce_4: 0.4458  loss_mask_4: 0.1541  loss_dice_4: 1.193  loss_ce_5: 0.4266  loss_mask_5: 0.1548  loss_dice_5: 1.185  loss_ce_6: 0.4053  loss_mask_6: 0.1561  loss_dice_6: 1.159  loss_ce_7: 0.4082  loss_mask_7: 0.1555  loss_dice_7: 1.188  loss_ce_8: 0.4081  loss_mask_8: 0.1539  loss_dice_8: 1.174    time: 1.0944  last_time: 1.0374  data_time: 0.0772  last_data_time: 0.0674   lr: 0.0001  max_mem: 32900M
[10/19 14:53:29] d2.utils.events INFO:  eta: 0:07:14  iter: 2599  total_loss: 18.5  loss_ce: 0.4142  loss_mask: 0.1587  loss_dice: 1.206  loss_ce_0: 0.5689  loss_mask_0: 0.1902  loss_dice_0: 1.378  loss_ce_1: 0.5991  loss_mask_1: 0.1755  loss_dice_1: 1.332  loss_ce_2: 0.5254  loss_mask_2: 0.1641  loss_dice_2: 1.254  loss_ce_3: 0.5142  loss_mask_3: 0.1634  loss_dice_3: 1.219  loss_ce_4: 0.4288  loss_mask_4: 0.1604  loss_dice_4: 1.188  loss_ce_5: 0.4235  loss_mask_5: 0.1592  loss_dice_5: 1.23  loss_ce_6: 0.4012  loss_mask_6: 0.1602  loss_dice_6: 1.187  loss_ce_7: 0.4153  loss_mask_7: 0.1578  loss_dice_7: 1.185  loss_ce_8: 0.4189  loss_mask_8: 0.1587  loss_dice_8: 1.197    time: 1.0944  last_time: 1.1169  data_time: 0.0786  last_data_time: 0.0881   lr: 0.0001  max_mem: 32900M
[10/19 14:53:51] d2.utils.events INFO:  eta: 0:06:52  iter: 2619  total_loss: 18.38  loss_ce: 0.4044  loss_mask: 0.1487  loss_dice: 1.158  loss_ce_0: 0.5163  loss_mask_0: 0.1801  loss_dice_0: 1.405  loss_ce_1: 0.6142  loss_mask_1: 0.1608  loss_dice_1: 1.256  loss_ce_2: 0.5165  loss_mask_2: 0.153  loss_dice_2: 1.218  loss_ce_3: 0.4722  loss_mask_3: 0.1527  loss_dice_3: 1.165  loss_ce_4: 0.4497  loss_mask_4: 0.1532  loss_dice_4: 1.162  loss_ce_5: 0.4304  loss_mask_5: 0.1501  loss_dice_5: 1.14  loss_ce_6: 0.4105  loss_mask_6: 0.1499  loss_dice_6: 1.16  loss_ce_7: 0.39  loss_mask_7: 0.1498  loss_dice_7: 1.153  loss_ce_8: 0.3964  loss_mask_8: 0.1501  loss_dice_8: 1.169    time: 1.0945  last_time: 1.1377  data_time: 0.0805  last_data_time: 0.0672   lr: 0.0001  max_mem: 32900M
[10/19 14:54:14] d2.utils.events INFO:  eta: 0:06:31  iter: 2639  total_loss: 19.16  loss_ce: 0.4097  loss_mask: 0.1493  loss_dice: 1.227  loss_ce_0: 0.522  loss_mask_0: 0.1816  loss_dice_0: 1.403  loss_ce_1: 0.5927  loss_mask_1: 0.1655  loss_dice_1: 1.325  loss_ce_2: 0.5061  loss_mask_2: 0.1584  loss_dice_2: 1.272  loss_ce_3: 0.4609  loss_mask_3: 0.159  loss_dice_3: 1.229  loss_ce_4: 0.463  loss_mask_4: 0.1544  loss_dice_4: 1.236  loss_ce_5: 0.4684  loss_mask_5: 0.1552  loss_dice_5: 1.261  loss_ce_6: 0.3959  loss_mask_6: 0.1524  loss_dice_6: 1.203  loss_ce_7: 0.3996  loss_mask_7: 0.1521  loss_dice_7: 1.222  loss_ce_8: 0.3913  loss_mask_8: 0.1478  loss_dice_8: 1.191    time: 1.0948  last_time: 1.0890  data_time: 0.0796  last_data_time: 0.0765   lr: 0.0001  max_mem: 32900M
[10/19 14:54:36] d2.utils.events INFO:  eta: 0:06:09  iter: 2659  total_loss: 18.75  loss_ce: 0.4157  loss_mask: 0.168  loss_dice: 1.165  loss_ce_0: 0.5398  loss_mask_0: 0.1937  loss_dice_0: 1.354  loss_ce_1: 0.5941  loss_mask_1: 0.1744  loss_dice_1: 1.32  loss_ce_2: 0.5306  loss_mask_2: 0.1682  loss_dice_2: 1.252  loss_ce_3: 0.466  loss_mask_3: 0.1668  loss_dice_3: 1.222  loss_ce_4: 0.4608  loss_mask_4: 0.1689  loss_dice_4: 1.213  loss_ce_5: 0.4546  loss_mask_5: 0.1681  loss_dice_5: 1.181  loss_ce_6: 0.3996  loss_mask_6: 0.17  loss_dice_6: 1.163  loss_ce_7: 0.3956  loss_mask_7: 0.1692  loss_dice_7: 1.18  loss_ce_8: 0.3951  loss_mask_8: 0.169  loss_dice_8: 1.157    time: 1.0948  last_time: 1.1292  data_time: 0.0772  last_data_time: 0.0856   lr: 0.0001  max_mem: 32900M
[10/19 14:54:57] d2.utils.events INFO:  eta: 0:05:47  iter: 2679  total_loss: 17.83  loss_ce: 0.4037  loss_mask: 0.1632  loss_dice: 1.116  loss_ce_0: 0.4806  loss_mask_0: 0.1838  loss_dice_0: 1.319  loss_ce_1: 0.5601  loss_mask_1: 0.1786  loss_dice_1: 1.258  loss_ce_2: 0.4795  loss_mask_2: 0.1748  loss_dice_2: 1.224  loss_ce_3: 0.4156  loss_mask_3: 0.1684  loss_dice_3: 1.162  loss_ce_4: 0.3712  loss_mask_4: 0.1655  loss_dice_4: 1.164  loss_ce_5: 0.3845  loss_mask_5: 0.1642  loss_dice_5: 1.149  loss_ce_6: 0.4031  loss_mask_6: 0.1635  loss_dice_6: 1.144  loss_ce_7: 0.3845  loss_mask_7: 0.1638  loss_dice_7: 1.123  loss_ce_8: 0.3947  loss_mask_8: 0.1612  loss_dice_8: 1.119    time: 1.0947  last_time: 1.0385  data_time: 0.0785  last_data_time: 0.0688   lr: 0.0001  max_mem: 32900M
[10/19 14:55:19] d2.utils.events INFO:  eta: 0:05:26  iter: 2699  total_loss: 18.96  loss_ce: 0.4602  loss_mask: 0.1549  loss_dice: 1.176  loss_ce_0: 0.5195  loss_mask_0: 0.1799  loss_dice_0: 1.372  loss_ce_1: 0.6213  loss_mask_1: 0.1647  loss_dice_1: 1.306  loss_ce_2: 0.5551  loss_mask_2: 0.1615  loss_dice_2: 1.268  loss_ce_3: 0.541  loss_mask_3: 0.1592  loss_dice_3: 1.22  loss_ce_4: 0.4709  loss_mask_4: 0.155  loss_dice_4: 1.188  loss_ce_5: 0.4635  loss_mask_5: 0.1545  loss_dice_5: 1.193  loss_ce_6: 0.4521  loss_mask_6: 0.156  loss_dice_6: 1.173  loss_ce_7: 0.4328  loss_mask_7: 0.157  loss_dice_7: 1.183  loss_ce_8: 0.4303  loss_mask_8: 0.1531  loss_dice_8: 1.158    time: 1.0947  last_time: 1.0870  data_time: 0.0800  last_data_time: 0.0601   lr: 0.0001  max_mem: 32900M
[10/19 14:55:41] d2.utils.events INFO:  eta: 0:05:04  iter: 2719  total_loss: 18.05  loss_ce: 0.4321  loss_mask: 0.1578  loss_dice: 1.11  loss_ce_0: 0.523  loss_mask_0: 0.187  loss_dice_0: 1.331  loss_ce_1: 0.5839  loss_mask_1: 0.1713  loss_dice_1: 1.255  loss_ce_2: 0.5153  loss_mask_2: 0.1618  loss_dice_2: 1.171  loss_ce_3: 0.4647  loss_mask_3: 0.1601  loss_dice_3: 1.153  loss_ce_4: 0.444  loss_mask_4: 0.1624  loss_dice_4: 1.136  loss_ce_5: 0.4335  loss_mask_5: 0.1604  loss_dice_5: 1.13  loss_ce_6: 0.4265  loss_mask_6: 0.1585  loss_dice_6: 1.116  loss_ce_7: 0.4216  loss_mask_7: 0.159  loss_dice_7: 1.107  loss_ce_8: 0.4193  loss_mask_8: 0.1594  loss_dice_8: 1.112    time: 1.0947  last_time: 1.1006  data_time: 0.0743  last_data_time: 0.0853   lr: 0.0001  max_mem: 32900M
[10/19 14:56:03] d2.utils.events INFO:  eta: 0:04:43  iter: 2739  total_loss: 18.32  loss_ce: 0.3756  loss_mask: 0.1507  loss_dice: 1.177  loss_ce_0: 0.5285  loss_mask_0: 0.1767  loss_dice_0: 1.391  loss_ce_1: 0.5585  loss_mask_1: 0.1627  loss_dice_1: 1.319  loss_ce_2: 0.4985  loss_mask_2: 0.1563  loss_dice_2: 1.246  loss_ce_3: 0.403  loss_mask_3: 0.1563  loss_dice_3: 1.209  loss_ce_4: 0.4108  loss_mask_4: 0.151  loss_dice_4: 1.218  loss_ce_5: 0.4151  loss_mask_5: 0.1521  loss_dice_5: 1.2  loss_ce_6: 0.3806  loss_mask_6: 0.1503  loss_dice_6: 1.181  loss_ce_7: 0.3983  loss_mask_7: 0.1504  loss_dice_7: 1.174  loss_ce_8: 0.3731  loss_mask_8: 0.1488  loss_dice_8: 1.204    time: 1.0947  last_time: 1.0806  data_time: 0.0757  last_data_time: 0.0697   lr: 0.0001  max_mem: 32900M
[10/19 14:56:26] d2.utils.events INFO:  eta: 0:04:21  iter: 2759  total_loss: 18.59  loss_ce: 0.4222  loss_mask: 0.166  loss_dice: 1.157  loss_ce_0: 0.5631  loss_mask_0: 0.1951  loss_dice_0: 1.31  loss_ce_1: 0.6062  loss_mask_1: 0.1742  loss_dice_1: 1.273  loss_ce_2: 0.5714  loss_mask_2: 0.167  loss_dice_2: 1.233  loss_ce_3: 0.4836  loss_mask_3: 0.1698  loss_dice_3: 1.183  loss_ce_4: 0.4692  loss_mask_4: 0.1703  loss_dice_4: 1.19  loss_ce_5: 0.4444  loss_mask_5: 0.1676  loss_dice_5: 1.138  loss_ce_6: 0.4214  loss_mask_6: 0.1661  loss_dice_6: 1.154  loss_ce_7: 0.4394  loss_mask_7: 0.1678  loss_dice_7: 1.162  loss_ce_8: 0.4557  loss_mask_8: 0.1651  loss_dice_8: 1.134    time: 1.0948  last_time: 1.1536  data_time: 0.0850  last_data_time: 0.0750   lr: 0.0001  max_mem: 32900M
[10/19 14:56:48] d2.utils.events INFO:  eta: 0:03:59  iter: 2779  total_loss: 18.74  loss_ce: 0.3884  loss_mask: 0.1475  loss_dice: 1.224  loss_ce_0: 0.5191  loss_mask_0: 0.1819  loss_dice_0: 1.41  loss_ce_1: 0.5669  loss_mask_1: 0.1661  loss_dice_1: 1.326  loss_ce_2: 0.518  loss_mask_2: 0.161  loss_dice_2: 1.27  loss_ce_3: 0.4552  loss_mask_3: 0.1532  loss_dice_3: 1.242  loss_ce_4: 0.4722  loss_mask_4: 0.1495  loss_dice_4: 1.238  loss_ce_5: 0.4171  loss_mask_5: 0.1491  loss_dice_5: 1.233  loss_ce_6: 0.4171  loss_mask_6: 0.1498  loss_dice_6: 1.238  loss_ce_7: 0.4176  loss_mask_7: 0.1468  loss_dice_7: 1.226  loss_ce_8: 0.3938  loss_mask_8: 0.1478  loss_dice_8: 1.247    time: 1.0949  last_time: 1.1808  data_time: 0.0853  last_data_time: 0.0823   lr: 0.0001  max_mem: 32900M
[10/19 14:57:10] d2.utils.events INFO:  eta: 0:03:37  iter: 2799  total_loss: 18.62  loss_ce: 0.3857  loss_mask: 0.1594  loss_dice: 1.175  loss_ce_0: 0.5219  loss_mask_0: 0.1971  loss_dice_0: 1.356  loss_ce_1: 0.5878  loss_mask_1: 0.1816  loss_dice_1: 1.305  loss_ce_2: 0.535  loss_mask_2: 0.1733  loss_dice_2: 1.264  loss_ce_3: 0.4262  loss_mask_3: 0.1654  loss_dice_3: 1.206  loss_ce_4: 0.4225  loss_mask_4: 0.165  loss_dice_4: 1.172  loss_ce_5: 0.418  loss_mask_5: 0.1634  loss_dice_5: 1.206  loss_ce_6: 0.4145  loss_mask_6: 0.1628  loss_dice_6: 1.202  loss_ce_7: 0.3824  loss_mask_7: 0.1604  loss_dice_7: 1.174  loss_ce_8: 0.4005  loss_mask_8: 0.1595  loss_dice_8: 1.191    time: 1.0950  last_time: 1.1520  data_time: 0.0792  last_data_time: 0.0849   lr: 0.0001  max_mem: 32900M
[10/19 14:57:33] d2.utils.events INFO:  eta: 0:03:16  iter: 2819  total_loss: 19.55  loss_ce: 0.4661  loss_mask: 0.165  loss_dice: 1.21  loss_ce_0: 0.5602  loss_mask_0: 0.1873  loss_dice_0: 1.44  loss_ce_1: 0.6222  loss_mask_1: 0.1787  loss_dice_1: 1.367  loss_ce_2: 0.5529  loss_mask_2: 0.1691  loss_dice_2: 1.325  loss_ce_3: 0.5226  loss_mask_3: 0.1693  loss_dice_3: 1.235  loss_ce_4: 0.4732  loss_mask_4: 0.1673  loss_dice_4: 1.247  loss_ce_5: 0.4649  loss_mask_5: 0.1658  loss_dice_5: 1.291  loss_ce_6: 0.4428  loss_mask_6: 0.1671  loss_dice_6: 1.229  loss_ce_7: 0.4536  loss_mask_7: 0.167  loss_dice_7: 1.227  loss_ce_8: 0.4542  loss_mask_8: 0.1662  loss_dice_8: 1.188    time: 1.0952  last_time: 1.0983  data_time: 0.0813  last_data_time: 0.1098   lr: 0.0001  max_mem: 32900M
[10/19 14:57:55] d2.utils.events INFO:  eta: 0:02:54  iter: 2839  total_loss: 18.68  loss_ce: 0.4301  loss_mask: 0.1625  loss_dice: 1.196  loss_ce_0: 0.5456  loss_mask_0: 0.1846  loss_dice_0: 1.342  loss_ce_1: 0.5605  loss_mask_1: 0.178  loss_dice_1: 1.305  loss_ce_2: 0.4969  loss_mask_2: 0.1693  loss_dice_2: 1.238  loss_ce_3: 0.4678  loss_mask_3: 0.1644  loss_dice_3: 1.236  loss_ce_4: 0.4298  loss_mask_4: 0.1659  loss_dice_4: 1.213  loss_ce_5: 0.4476  loss_mask_5: 0.1654  loss_dice_5: 1.17  loss_ce_6: 0.4416  loss_mask_6: 0.1616  loss_dice_6: 1.197  loss_ce_7: 0.447  loss_mask_7: 0.1621  loss_dice_7: 1.174  loss_ce_8: 0.4164  loss_mask_8: 0.1629  loss_dice_8: 1.171    time: 1.0953  last_time: 1.0905  data_time: 0.0777  last_data_time: 0.0722   lr: 0.0001  max_mem: 32900M
[10/19 14:58:17] d2.utils.events INFO:  eta: 0:02:32  iter: 2859  total_loss: 18.14  loss_ce: 0.4  loss_mask: 0.1494  loss_dice: 1.215  loss_ce_0: 0.5287  loss_mask_0: 0.1709  loss_dice_0: 1.357  loss_ce_1: 0.5939  loss_mask_1: 0.1561  loss_dice_1: 1.276  loss_ce_2: 0.5246  loss_mask_2: 0.148  loss_dice_2: 1.283  loss_ce_3: 0.4524  loss_mask_3: 0.1488  loss_dice_3: 1.202  loss_ce_4: 0.4432  loss_mask_4: 0.1485  loss_dice_4: 1.184  loss_ce_5: 0.4297  loss_mask_5: 0.1491  loss_dice_5: 1.203  loss_ce_6: 0.419  loss_mask_6: 0.151  loss_dice_6: 1.192  loss_ce_7: 0.3912  loss_mask_7: 0.1509  loss_dice_7: 1.191  loss_ce_8: 0.3818  loss_mask_8: 0.1492  loss_dice_8: 1.168    time: 1.0953  last_time: 1.1656  data_time: 0.0820  last_data_time: 0.0788   lr: 0.0001  max_mem: 32900M
[10/19 14:58:39] d2.utils.events INFO:  eta: 0:02:10  iter: 2879  total_loss: 17.6  loss_ce: 0.4008  loss_mask: 0.1669  loss_dice: 1.117  loss_ce_0: 0.5898  loss_mask_0: 0.1955  loss_dice_0: 1.242  loss_ce_1: 0.5989  loss_mask_1: 0.1792  loss_dice_1: 1.192  loss_ce_2: 0.5399  loss_mask_2: 0.1746  loss_dice_2: 1.128  loss_ce_3: 0.4634  loss_mask_3: 0.1661  loss_dice_3: 1.084  loss_ce_4: 0.4537  loss_mask_4: 0.1705  loss_dice_4: 1.13  loss_ce_5: 0.4236  loss_mask_5: 0.1691  loss_dice_5: 1.079  loss_ce_6: 0.4462  loss_mask_6: 0.1666  loss_dice_6: 1.086  loss_ce_7: 0.4123  loss_mask_7: 0.1653  loss_dice_7: 1.098  loss_ce_8: 0.4462  loss_mask_8: 0.1671  loss_dice_8: 1.108    time: 1.0952  last_time: 1.0000  data_time: 0.0742  last_data_time: 0.0562   lr: 0.0001  max_mem: 32900M
[10/19 14:59:01] d2.utils.events INFO:  eta: 0:01:48  iter: 2899  total_loss: 19.13  loss_ce: 0.4048  loss_mask: 0.1516  loss_dice: 1.217  loss_ce_0: 0.5159  loss_mask_0: 0.1758  loss_dice_0: 1.456  loss_ce_1: 0.575  loss_mask_1: 0.1594  loss_dice_1: 1.347  loss_ce_2: 0.5106  loss_mask_2: 0.1543  loss_dice_2: 1.284  loss_ce_3: 0.4548  loss_mask_3: 0.1549  loss_dice_3: 1.227  loss_ce_4: 0.4607  loss_mask_4: 0.1513  loss_dice_4: 1.233  loss_ce_5: 0.4425  loss_mask_5: 0.1512  loss_dice_5: 1.245  loss_ce_6: 0.4229  loss_mask_6: 0.1507  loss_dice_6: 1.215  loss_ce_7: 0.4017  loss_mask_7: 0.1516  loss_dice_7: 1.198  loss_ce_8: 0.3863  loss_mask_8: 0.1514  loss_dice_8: 1.228    time: 1.0953  last_time: 1.0815  data_time: 0.0834  last_data_time: 0.0742   lr: 0.0001  max_mem: 32900M
[10/19 14:59:23] d2.utils.events INFO:  eta: 0:01:27  iter: 2919  total_loss: 18.91  loss_ce: 0.3648  loss_mask: 0.1511  loss_dice: 1.283  loss_ce_0: 0.4789  loss_mask_0: 0.1756  loss_dice_0: 1.426  loss_ce_1: 0.5113  loss_mask_1: 0.1616  loss_dice_1: 1.35  loss_ce_2: 0.4437  loss_mask_2: 0.1588  loss_dice_2: 1.348  loss_ce_3: 0.3889  loss_mask_3: 0.1618  loss_dice_3: 1.316  loss_ce_4: 0.3729  loss_mask_4: 0.1573  loss_dice_4: 1.306  loss_ce_5: 0.3974  loss_mask_5: 0.1529  loss_dice_5: 1.287  loss_ce_6: 0.3721  loss_mask_6: 0.1546  loss_dice_6: 1.282  loss_ce_7: 0.346  loss_mask_7: 0.1522  loss_dice_7: 1.288  loss_ce_8: 0.3609  loss_mask_8: 0.1528  loss_dice_8: 1.269    time: 1.0954  last_time: 1.1841  data_time: 0.0818  last_data_time: 0.1200   lr: 0.0001  max_mem: 32900M
[10/19 14:59:45] d2.utils.events INFO:  eta: 0:01:05  iter: 2939  total_loss: 18.32  loss_ce: 0.4365  loss_mask: 0.153  loss_dice: 1.114  loss_ce_0: 0.5726  loss_mask_0: 0.1826  loss_dice_0: 1.336  loss_ce_1: 0.6074  loss_mask_1: 0.1669  loss_dice_1: 1.301  loss_ce_2: 0.5407  loss_mask_2: 0.1564  loss_dice_2: 1.248  loss_ce_3: 0.4728  loss_mask_3: 0.1572  loss_dice_3: 1.189  loss_ce_4: 0.4584  loss_mask_4: 0.1576  loss_dice_4: 1.162  loss_ce_5: 0.4377  loss_mask_5: 0.156  loss_dice_5: 1.159  loss_ce_6: 0.4286  loss_mask_6: 0.1553  loss_dice_6: 1.137  loss_ce_7: 0.404  loss_mask_7: 0.1526  loss_dice_7: 1.177  loss_ce_8: 0.4538  loss_mask_8: 0.1548  loss_dice_8: 1.161    time: 1.0955  last_time: 1.0472  data_time: 0.0802  last_data_time: 0.0683   lr: 0.0001  max_mem: 32900M
[10/19 15:00:08] d2.utils.events INFO:  eta: 0:00:43  iter: 2959  total_loss: 19.1  loss_ce: 0.4174  loss_mask: 0.146  loss_dice: 1.224  loss_ce_0: 0.5555  loss_mask_0: 0.1773  loss_dice_0: 1.379  loss_ce_1: 0.5898  loss_mask_1: 0.1611  loss_dice_1: 1.344  loss_ce_2: 0.5467  loss_mask_2: 0.1543  loss_dice_2: 1.309  loss_ce_3: 0.4829  loss_mask_3: 0.1538  loss_dice_3: 1.249  loss_ce_4: 0.4753  loss_mask_4: 0.1511  loss_dice_4: 1.232  loss_ce_5: 0.4816  loss_mask_5: 0.1502  loss_dice_5: 1.23  loss_ce_6: 0.4592  loss_mask_6: 0.1475  loss_dice_6: 1.178  loss_ce_7: 0.4332  loss_mask_7: 0.1468  loss_dice_7: 1.242  loss_ce_8: 0.4586  loss_mask_8: 0.1464  loss_dice_8: 1.252    time: 1.0956  last_time: 1.1727  data_time: 0.0844  last_data_time: 0.0617   lr: 0.0001  max_mem: 32900M
[10/19 15:00:30] d2.utils.events INFO:  eta: 0:00:21  iter: 2979  total_loss: 18.5  loss_ce: 0.4295  loss_mask: 0.1529  loss_dice: 1.128  loss_ce_0: 0.5283  loss_mask_0: 0.1877  loss_dice_0: 1.302  loss_ce_1: 0.6093  loss_mask_1: 0.1773  loss_dice_1: 1.22  loss_ce_2: 0.5269  loss_mask_2: 0.1673  loss_dice_2: 1.21  loss_ce_3: 0.4678  loss_mask_3: 0.1631  loss_dice_3: 1.159  loss_ce_4: 0.4483  loss_mask_4: 0.1571  loss_dice_4: 1.155  loss_ce_5: 0.4528  loss_mask_5: 0.1558  loss_dice_5: 1.123  loss_ce_6: 0.4122  loss_mask_6: 0.1549  loss_dice_6: 1.105  loss_ce_7: 0.4028  loss_mask_7: 0.1541  loss_dice_7: 1.12  loss_ce_8: 0.4248  loss_mask_8: 0.1551  loss_dice_8: 1.102    time: 1.0957  last_time: 1.1022  data_time: 0.0743  last_data_time: 0.0718   lr: 0.0001  max_mem: 32900M
[10/19 15:00:52] fvcore.common.checkpoint INFO: Saving checkpoint to /tsi/hi-paris/GB/segmentation/results/In_vocab/r50_008_3000_19_a_decoder_pruning_i_15_f_0_5/model_final.pth
[10/19 15:00:55] d2.utils.events INFO:  eta: 0:00:00  iter: 2999  total_loss: 18.77  loss_ce: 0.3568  loss_mask: 0.153  loss_dice: 1.217  loss_ce_0: 0.5362  loss_mask_0: 0.1768  loss_dice_0: 1.42  loss_ce_1: 0.5914  loss_mask_1: 0.1607  loss_dice_1: 1.315  loss_ce_2: 0.5052  loss_mask_2: 0.1571  loss_dice_2: 1.313  loss_ce_3: 0.4516  loss_mask_3: 0.1557  loss_dice_3: 1.23  loss_ce_4: 0.4352  loss_mask_4: 0.1548  loss_dice_4: 1.255  loss_ce_5: 0.4284  loss_mask_5: 0.1534  loss_dice_5: 1.225  loss_ce_6: 0.4189  loss_mask_6: 0.1517  loss_dice_6: 1.245  loss_ce_7: 0.3884  loss_mask_7: 0.1529  loss_dice_7: 1.208  loss_ce_8: 0.4014  loss_mask_8: 0.1535  loss_dice_8: 1.242    time: 1.0958  last_time: 1.0726  data_time: 0.0801  last_data_time: 0.0685   lr: 0.0001  max_mem: 32900M
[10/19 15:00:55] d2.engine.hooks INFO: Overall training speed: 2998 iterations in 0:54:45 (1.0958 s / it)
[10/19 15:00:55] d2.engine.hooks INFO: Total training time: 0:54:58 (0:00:13 on hooks)
[10/19 15:00:56] fcclip.data.datasets.register_cityscapes_panoptic INFO: 3 cities found in 'datasets/cityscapes/leftImg8bit/val'.
[10/19 15:00:56] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(1024, 1024), max_size=2560, sample_style='choice')]
[10/19 15:00:56] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/19 15:00:56] d2.data.common INFO: Serializing 500 elements to byte tensors and concatenating them all ...
[10/19 15:00:56] d2.data.common INFO: Serialized dataset takes 0.74 MiB
[10/19 15:00:56] d2.evaluation.evaluator INFO: Start inference on 500 batches
[10/19 15:00:59] d2.evaluation.evaluator INFO: Inference done 11/500. Dataloading: 0.0039 s/iter. Inference: 0.1440 s/iter. Eval: 0.0699 s/iter. Total: 0.2178 s/iter. ETA=0:01:46
[10/19 15:01:04] d2.evaluation.evaluator INFO: Inference done 34/500. Dataloading: 0.0051 s/iter. Inference: 0.1471 s/iter. Eval: 0.0682 s/iter. Total: 0.2205 s/iter. ETA=0:01:42
[10/19 15:01:09] d2.evaluation.evaluator INFO: Inference done 55/500. Dataloading: 0.0052 s/iter. Inference: 0.1549 s/iter. Eval: 0.0690 s/iter. Total: 0.2291 s/iter. ETA=0:01:41
[10/19 15:01:14] d2.evaluation.evaluator INFO: Inference done 76/500. Dataloading: 0.0053 s/iter. Inference: 0.1630 s/iter. Eval: 0.0667 s/iter. Total: 0.2350 s/iter. ETA=0:01:39
[10/19 15:01:20] d2.evaluation.evaluator INFO: Inference done 97/500. Dataloading: 0.0053 s/iter. Inference: 0.1651 s/iter. Eval: 0.0671 s/iter. Total: 0.2375 s/iter. ETA=0:01:35
[10/19 15:01:25] d2.evaluation.evaluator INFO: Inference done 118/500. Dataloading: 0.0052 s/iter. Inference: 0.1672 s/iter. Eval: 0.0666 s/iter. Total: 0.2391 s/iter. ETA=0:01:31
[10/19 15:01:30] d2.evaluation.evaluator INFO: Inference done 140/500. Dataloading: 0.0053 s/iter. Inference: 0.1653 s/iter. Eval: 0.0672 s/iter. Total: 0.2378 s/iter. ETA=0:01:25
[10/19 15:01:35] d2.evaluation.evaluator INFO: Inference done 161/500. Dataloading: 0.0053 s/iter. Inference: 0.1673 s/iter. Eval: 0.0665 s/iter. Total: 0.2392 s/iter. ETA=0:01:21
[10/19 15:01:40] d2.evaluation.evaluator INFO: Inference done 183/500. Dataloading: 0.0053 s/iter. Inference: 0.1667 s/iter. Eval: 0.0663 s/iter. Total: 0.2384 s/iter. ETA=0:01:15
[10/19 15:01:45] d2.evaluation.evaluator INFO: Inference done 204/500. Dataloading: 0.0053 s/iter. Inference: 0.1669 s/iter. Eval: 0.0665 s/iter. Total: 0.2387 s/iter. ETA=0:01:10
[10/19 15:01:50] d2.evaluation.evaluator INFO: Inference done 226/500. Dataloading: 0.0053 s/iter. Inference: 0.1666 s/iter. Eval: 0.0664 s/iter. Total: 0.2385 s/iter. ETA=0:01:05
[10/19 15:01:56] d2.evaluation.evaluator INFO: Inference done 248/500. Dataloading: 0.0053 s/iter. Inference: 0.1662 s/iter. Eval: 0.0666 s/iter. Total: 0.2382 s/iter. ETA=0:01:00
[10/19 15:02:01] d2.evaluation.evaluator INFO: Inference done 271/500. Dataloading: 0.0053 s/iter. Inference: 0.1641 s/iter. Eval: 0.0670 s/iter. Total: 0.2365 s/iter. ETA=0:00:54
[10/19 15:02:06] d2.evaluation.evaluator INFO: Inference done 295/500. Dataloading: 0.0053 s/iter. Inference: 0.1615 s/iter. Eval: 0.0674 s/iter. Total: 0.2342 s/iter. ETA=0:00:48
[10/19 15:02:11] d2.evaluation.evaluator INFO: Inference done 319/500. Dataloading: 0.0053 s/iter. Inference: 0.1599 s/iter. Eval: 0.0676 s/iter. Total: 0.2329 s/iter. ETA=0:00:42
[10/19 15:02:16] d2.evaluation.evaluator INFO: Inference done 341/500. Dataloading: 0.0053 s/iter. Inference: 0.1593 s/iter. Eval: 0.0679 s/iter. Total: 0.2326 s/iter. ETA=0:00:36
[10/19 15:02:21] d2.evaluation.evaluator INFO: Inference done 364/500. Dataloading: 0.0053 s/iter. Inference: 0.1585 s/iter. Eval: 0.0681 s/iter. Total: 0.2320 s/iter. ETA=0:00:31
[10/19 15:02:26] d2.evaluation.evaluator INFO: Inference done 386/500. Dataloading: 0.0053 s/iter. Inference: 0.1583 s/iter. Eval: 0.0684 s/iter. Total: 0.2320 s/iter. ETA=0:00:26
[10/19 15:02:31] d2.evaluation.evaluator INFO: Inference done 406/500. Dataloading: 0.0053 s/iter. Inference: 0.1592 s/iter. Eval: 0.0683 s/iter. Total: 0.2329 s/iter. ETA=0:00:21
[10/19 15:02:36] d2.evaluation.evaluator INFO: Inference done 428/500. Dataloading: 0.0053 s/iter. Inference: 0.1591 s/iter. Eval: 0.0684 s/iter. Total: 0.2328 s/iter. ETA=0:00:16
[10/19 15:02:41] d2.evaluation.evaluator INFO: Inference done 451/500. Dataloading: 0.0053 s/iter. Inference: 0.1587 s/iter. Eval: 0.0684 s/iter. Total: 0.2325 s/iter. ETA=0:00:11
[10/19 15:02:47] d2.evaluation.evaluator INFO: Inference done 474/500. Dataloading: 0.0053 s/iter. Inference: 0.1581 s/iter. Eval: 0.0686 s/iter. Total: 0.2320 s/iter. ETA=0:00:06
[10/19 15:02:52] d2.evaluation.evaluator INFO: Inference done 496/500. Dataloading: 0.0053 s/iter. Inference: 0.1580 s/iter. Eval: 0.0687 s/iter. Total: 0.2320 s/iter. ETA=0:00:00
[10/19 15:02:53] d2.evaluation.evaluator INFO: Total inference time: 0:01:55.008411 (0.232340 s / iter per device, on 1 devices)
[10/19 15:02:53] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:01:18 (0.158198 s / iter per device, on 1 devices)
[10/19 15:02:53] d2.evaluation.panoptic_evaluation INFO: Writing all panoptic predictions to /tmp/panoptic_eval_y7ic5xg ...
[10/19 15:03:29] d2.evaluation.panoptic_evaluation INFO: Panoptic Evaluation Results:
|        |   PQ   |   SQ   |   RQ   |  #categories  |
|:------:|:------:|:------:|:------:|:-------------:|
|  All   | 56.779 | 79.901 | 69.867 |      19       |
| Things | 48.501 | 79.112 | 61.315 |       8       |
| Stuff  | 62.799 | 80.475 | 76.086 |      11       |
[10/19 15:03:29] d2.evaluation.panoptic_evaluation INFO: Panoptic Evaluation Results:
|          |   PQ   |   SQ   |   RQ   |  #categories  |
|:--------:|:------:|:------:|:------:|:-------------:|
| class_7  | 97.193 | 97.495 | 99.690 |     Stuff     |
| class_8  | 73.258 | 83.026 | 88.235 |     Stuff     |
| class_11 | 86.897 | 89.270 | 97.342 |     Stuff     |
| class_12 | 40.025 | 77.304 | 51.777 |     Stuff     |
| class_13 | 33.861 | 75.206 | 45.024 |     Stuff     |
| class_17 | 38.273 | 63.053 | 60.700 |     Stuff     |
| class_19 | 47.940 | 67.701 | 70.812 |     Stuff     |
| class_20 | 64.081 | 75.924 | 84.402 |     Stuff     |
| class_21 | 88.884 | 89.528 | 99.281 |     Stuff     |
| class_22 | 33.921 | 75.445 | 44.961 |     Stuff     |
| class_23 | 86.458 | 91.273 | 94.725 |     Stuff     |
| class_24 | 50.645 | 74.651 | 67.842 |    Things     |
| class_25 | 45.747 | 73.228 | 62.472 |    Things     |
| class_26 | 63.736 | 81.483 | 78.220 |    Things     |
| class_27 | 50.134 | 86.967 | 57.647 |    Things     |
| class_28 | 56.991 | 88.434 | 64.444 |    Things     |
| class_31 | 39.265 | 83.765 | 46.875 |    Things     |
| class_32 | 40.141 | 74.127 | 54.152 |    Things     |
| class_33 | 41.348 | 70.237 | 58.868 |    Things     |
[10/19 15:03:29] d2.engine.defaults INFO: Evaluation results for openvocab_cityscapes_fine_panoptic_val in csv format:
[10/19 15:03:29] d2.evaluation.testing INFO: copypaste: Task: panoptic_seg
[10/19 15:03:29] d2.evaluation.testing INFO: copypaste: PQ,SQ,RQ,PQ_th,SQ_th,RQ_th,PQ_st,SQ_st,RQ_st,PQ_7,SQ_7,RQ_7,PQ_8,SQ_8,RQ_8,PQ_11,SQ_11,RQ_11,PQ_12,SQ_12,RQ_12,PQ_13,SQ_13,RQ_13,PQ_17,SQ_17,RQ_17,PQ_19,SQ_19,RQ_19,PQ_20,SQ_20,RQ_20,PQ_21,SQ_21,RQ_21,PQ_22,SQ_22,RQ_22,PQ_23,SQ_23,RQ_23,PQ_24,SQ_24,RQ_24,PQ_25,SQ_25,RQ_25,PQ_26,SQ_26,RQ_26,PQ_27,SQ_27,RQ_27,PQ_28,SQ_28,RQ_28,PQ_31,SQ_31,RQ_31,PQ_32,SQ_32,RQ_32,PQ_33,SQ_33,RQ_33
[10/19 15:03:29] d2.evaluation.testing INFO: copypaste: 56.7788,79.9009,69.8667,48.5007,79.1115,61.3150,62.7992,80.4751,76.0860,97.1927,97.4952,99.6898,73.2581,83.0258,88.2353,86.8972,89.2704,97.3415,40.0255,77.3041,51.7766,33.8606,75.2062,45.0237,38.2732,63.0534,60.6996,47.9401,67.7008,70.8117,64.0811,75.9239,84.4017,88.8840,89.5281,99.2806,33.9209,75.4449,44.9612,86.4579,91.2728,94.7248,50.6446,74.6508,67.8421,45.7470,73.2281,62.4719,63.7359,81.4834,78.2195,50.1342,86.9675,57.6471,56.9906,88.4337,64.4444,39.2648,83.7649,46.8750,40.1408,74.1267,54.1516,41.3475,70.2374,58.8683
