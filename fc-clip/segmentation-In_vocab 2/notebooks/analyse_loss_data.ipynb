{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path='/tsi/hi-paris/FCCLIP_results/All_results/INvocab/fcclip_cocopan_soft_label_calcul_loss_'\n",
    "image_id='frankfurt_000000_003025'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(results_path, image_id, iter):\n",
    "    path_cls=os.path.join(results_path, \"Softlabels\", \"iter_\"+str(iter), \"mask_cls_\"+image_id)\n",
    "    path_pred=os.path.join(results_path, \"Softlabels\", \"iter_\"+str(iter), \"mask_pred_\"+image_id)\n",
    "    path_cls_results=os.path.join(results_path, \"Softlabels\", \"iter_\"+str(iter), \"cls_results_\"+image_id) \n",
    "    with open(path_cls, 'rb') as file:\n",
    "        mask_cls_results=pickle.load(file)\n",
    "    with open(path_pred, 'rb') as file:\n",
    "        mask_pred_results=pickle.load(file)\n",
    "    with open(path_cls_results, 'rb') as file:\n",
    "        cls_results=pickle.load(file)\n",
    "    return mask_cls_results, mask_pred_results, cls_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_jsons():\n",
    "    path_json_data=\"/tsi/hi-paris/FCCLIP_results/All_results/INvocab/fcclip_cocopan_soft_label_calcul_loss_new/datalosses/iter_0/json_data_0\"\n",
    "    path_output_0=\"/tsi/hi-paris/FCCLIP_results/All_results/INvocab/fcclip_cocopan_soft_label_calcul_loss_new/datalosses/iter_0/outputs_0\"\n",
    "    path_targets_0=\"/tsi/hi-paris/FCCLIP_results/All_results/INvocab/fcclip_cocopan_soft_label_calcul_loss_new/datalosses/iter_0/targets_0\"\n",
    "    with open(path_json_data, 'rb') as file:\n",
    "        json_data=pickle.load(file) \n",
    "    with open(path_output_0, 'rb') as file:\n",
    "        outputs=pickle.load(file)\n",
    "    with open(path_targets_0, 'rb') as file:\n",
    "        targets=pickle.load(file)\n",
    "    return json_data, outputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data, outputs, targets=read_data_jsons()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_logits = outputs.float()\n",
    "idx = json_data['idx']\n",
    "target_classes_o = torch.cat([t[\"labels\"][J] for t, (_, J) in zip(targets, json_data['indices'])])\n",
    "target_classes = torch.full(\n",
    "            src_logits.shape[:2], json_data['num_classes'], dtype=torch.int64, device=src_logits.device)\n",
    "target_classes[idx] = target_classes_o\n",
    "\n",
    "loss_ce = F.cross_entropy(src_logits.transpose(1, 2), target_classes, json_data['weight'])\n",
    "losses = {\"loss_ce\": loss_ce}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20]) torch.Size([4, 250])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 0.1000])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(json_data['weight'].shape, target_classes.shape)\n",
    "json_data['weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[19, 19, 19, 19, 19, 19, 19, 19, 11, 19, 19, 19, 19, 13, 13, 19, 19, 19,\n",
       "         13, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 18, 19, 19, 19, 19, 19,\n",
       "         13, 19, 19, 19,  5, 19, 19, 19, 19, 19, 19, 11, 19, 19, 19, 19, 19, 13,\n",
       "         19, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 13, 19, 19, 19, 13, 19,\n",
       "         19, 19, 19, 19, 19, 13, 19, 19, 18, 13, 19, 19, 19, 19, 19, 10, 19, 19,\n",
       "          8, 19, 19, 19, 19, 19, 13, 19, 19, 19, 19, 19, 19, 19, 19, 13, 19, 19,\n",
       "         19, 19, 19, 13, 19, 19, 19,  7, 19, 19, 19, 19, 19, 19, 11, 19, 19, 19,\n",
       "         19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 13, 19, 19, 19, 19, 19,\n",
       "         19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 13, 11, 19, 19, 19, 19, 19,\n",
       "         19, 19, 19, 19, 13, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "         19, 13, 13, 19, 19, 13, 13, 19, 19, 19, 19,  0, 19, 19, 19, 19, 19, 12,\n",
       "         19, 19, 19, 11, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "         19, 19, 19, 19,  2, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 11, 19, 19,\n",
       "         19, 19,  1, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19],\n",
       "        [19,  1, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "         19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 13, 19, 19,\n",
       "         19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "          7, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 13, 19, 19, 19, 13, 19,\n",
       "         19, 19, 19, 19, 19, 13, 19, 19, 19, 19, 19, 19, 19, 19, 19, 10, 19,  4,\n",
       "          8, 19, 19, 13, 19,  5, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "         19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "         19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "         19, 19, 19, 19,  3, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "         19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "         19, 19, 13, 19, 19, 19, 19, 19, 19, 19, 19,  0, 19, 19, 19, 19, 19, 19,\n",
       "         19, 19, 19, 11, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "         19, 19, 19, 19,  2, 19, 19, 19, 19, 13, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "         19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 11, 19, 19, 19, 19],\n",
       "        [19,  1, 11, 19, 19, 11, 11, 11, 11, 11, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "         19, 19, 11, 19, 19, 19, 19, 11, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "         19, 19, 19, 19, 19, 19, 19, 11, 11, 11, 19, 19, 19, 11, 19, 19, 19, 19,\n",
       "         19, 11, 19, 19, 11, 19, 19, 19, 19,  3, 11, 19, 11, 19, 19, 19, 19, 19,\n",
       "          9, 19, 19, 12, 19, 19, 19, 19, 19, 19, 11, 11, 19, 19, 19, 10, 11, 19,\n",
       "          8, 19, 19, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "         19, 19, 19, 19, 19, 11, 19, 19, 13, 19, 19, 19, 19, 19, 19, 11, 19, 11,\n",
       "         19, 19, 19,  7, 11, 11, 19, 19, 19, 19, 11,  5, 19, 19, 19, 19, 19, 19,\n",
       "         19, 19, 19, 19, 19, 11, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 11,\n",
       "         19, 19, 19, 19, 19, 19, 19, 19, 11, 19, 11, 19, 11, 19, 19, 19, 19, 19,\n",
       "         19, 19, 19, 19,  4, 11, 11, 19, 19, 19, 19,  0, 19, 19, 19, 19, 19, 11,\n",
       "         19, 19, 19, 11, 19, 19, 19, 11, 11, 19, 11, 19, 19, 19, 19, 19, 19, 19,\n",
       "         19, 19, 19, 19,  2, 19, 19, 19, 19, 19, 11, 19, 11, 19, 19, 19, 19,  6,\n",
       "         19, 11, 19, 19, 19, 19, 19, 19, 19, 11, 19, 11, 19, 19, 19, 19],\n",
       "        [19,  1, 19, 19, 19, 11, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "         13, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "         19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "         19, 13, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 11, 19,\n",
       "          9, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 10, 19,  4,\n",
       "          8, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "         19, 19, 19, 19, 13, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,  7,\n",
       "         19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "         19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "         19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "         19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,  0, 19, 19, 19, 19, 19, 19,\n",
       "         19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "         19, 19, 19, 19, 19, 19, 19, 19, 19, 11, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "         19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,  5, 19, 19, 19]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4469, grad_fn=<NllLoss2DBackward0>)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0405, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(src_logits.transpose(1, 2)[0].T[0], target_classes[0][0], json_data['weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3586, -0.4592,  2.8711,  2.5078,  0.3867,  1.1680,  6.5586,  2.6660,\n",
       "         1.7539, -1.3252,  2.9688,  0.8325, -0.3364,  0.8530,  0.7373, -0.0676,\n",
       "        -0.9175, -2.3281, -1.3447,  9.8594], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_logits.transpose(1, 2)[0].T[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(19)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_classes[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-10.2585, -10.3591,  -7.0287,  -7.3920,  -9.5131,  -8.7319,  -3.3412,\n",
       "         -7.2338,  -8.1459, -11.2250,  -6.9311,  -9.0673, -10.2363,  -9.0468,\n",
       "         -9.1625,  -9.9674, -10.8173, -12.2280, -11.2446,  -0.0405],\n",
       "       grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0405], grad_fn=<GatherBackward0>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_softmax.gather(-1, target.unsqueeze(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_260685/153533964.py:7: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  log_softmax = F.log_softmax(input)\n"
     ]
    }
   ],
   "source": [
    "input=src_logits.transpose(1, 2)[0].T[0]\n",
    "target=target_classes[0][0]\n",
    "weight= json_data['weight']\n",
    "\n",
    "\n",
    "# Apply log softmax to the input along the class dimension\n",
    "log_softmax = F.log_softmax(input)\n",
    "\n",
    "# Convert target to long type if not already\n",
    "target = target.long()\n",
    "\n",
    "# Gather the log softmax values corresponding to the target classes\n",
    "log_probs = log_softmax.gather(-1, target.unsqueeze(-1)).squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0405, grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss\n",
    "log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-10.2585, -10.3591,  -7.0287,  -7.3920,  -9.5131,  -8.7319,  -3.3412,\n",
       "         -7.2338,  -8.1459, -11.2250,  -6.9311,  -9.0673, -10.2363,  -9.0468,\n",
       "         -9.1625,  -9.9674, -10.8173, -12.2280, -11.2446,  -0.0405],\n",
       "       grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if weight is not None:\n",
    "    # Apply class weights if provided\n",
    "    log_probs = log_probs * weight[target]\n",
    "\n",
    "# Compute the negative log likelihood loss\n",
    "loss = -log_probs.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0040, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs : \n",
      "pred_logits :  tensor([[[-3.5864e-01, -4.5923e-01,  2.8711e+00,  ..., -2.3281e+00,\n",
      "          -1.3447e+00,  9.8594e+00],\n",
      "         [ 3.8945e+00,  8.5781e+00,  2.6831e-01,  ..., -3.5742e-01,\n",
      "           1.2451e-02,  9.9375e+00],\n",
      "         [ 1.0234e+00,  1.1924e+00,  4.3915e-02,  ...,  3.7891e+00,\n",
      "           2.7188e+00,  1.1656e+01],\n",
      "         ...,\n",
      "         [ 4.7461e+00,  8.8750e+00,  8.9795e-01,  ..., -1.7432e+00,\n",
      "          -1.8445e-01,  1.0820e+01],\n",
      "         [-1.6914e+00, -1.7783e+00,  1.0352e+00,  ..., -1.8564e+00,\n",
      "          -4.6973e-01,  1.0852e+01],\n",
      "         [-2.0020e+00, -1.6787e+00,  3.1836e+00,  ..., -3.5117e+00,\n",
      "          -1.3008e+00,  1.0461e+01]],\n",
      "\n",
      "        [[-3.8110e-01, -3.4570e-01,  6.0898e+00,  ..., -2.6406e+00,\n",
      "          -8.7549e-01,  1.0281e+01],\n",
      "         [ 5.1289e+00,  1.2453e+01,  8.8818e-01,  ..., -2.7168e+00,\n",
      "          -1.1309e+00,  7.3516e+00],\n",
      "         [ 4.9072e-01,  1.6504e+00,  8.3691e-01,  ...,  2.3711e+00,\n",
      "           1.7812e+00,  1.0109e+01],\n",
      "         ...,\n",
      "         [ 3.3926e+00,  5.3203e+00,  3.5098e+00,  ..., -1.1250e+00,\n",
      "           1.1592e+00,  1.2227e+01],\n",
      "         [ 4.7925e-01, -6.3843e-02,  6.1445e+00,  ..., -2.9902e+00,\n",
      "          -8.7109e-01,  9.5859e+00],\n",
      "         [-1.8828e+00, -1.4014e+00,  3.9980e+00,  ..., -3.3398e+00,\n",
      "          -1.3730e+00,  1.0438e+01]],\n",
      "\n",
      "        [[-1.2305e+00, -2.3535e+00,  7.2327e-02,  ..., -2.4512e+00,\n",
      "          -2.1113e+00,  1.1445e+01],\n",
      "         [ 4.9844e+00,  1.2141e+01,  6.1035e-01,  ..., -2.7344e+00,\n",
      "          -1.1162e+00,  7.1445e+00],\n",
      "         [-4.0771e-01,  9.1113e-01,  1.5215e+00,  ..., -5.7178e-01,\n",
      "           4.9756e-01,  8.1328e+00],\n",
      "         ...,\n",
      "         [ 2.1855e+00,  4.6836e+00,  3.5527e+00,  ...,  8.9893e-01,\n",
      "           1.2227e+00,  9.7969e+00],\n",
      "         [-3.8428e-01, -1.2178e+00,  6.8994e-01,  ..., -9.1650e-01,\n",
      "          -5.6396e-01,  1.1266e+01],\n",
      "         [-1.4316e+00, -1.4736e+00,  3.2988e+00,  ..., -2.9180e+00,\n",
      "          -6.2939e-01,  1.0852e+01]],\n",
      "\n",
      "        [[-1.3799e+00, -1.3350e+00,  2.1680e-01,  ...,  2.6636e-01,\n",
      "           7.1484e-01,  1.1195e+01],\n",
      "         [ 5.1992e+00,  1.2023e+01,  5.9619e-01,  ..., -2.5977e+00,\n",
      "          -1.0547e+00,  7.9023e+00],\n",
      "         [-1.2158e+00,  3.9209e-01, -4.5312e-01,  ..., -1.0566e+00,\n",
      "           8.4326e-01,  1.1312e+01],\n",
      "         ...,\n",
      "         [ 3.0195e+00,  5.1367e+00,  6.0254e-01,  ..., -1.2578e+00,\n",
      "          -1.0381e+00,  1.0578e+01],\n",
      "         [ 1.1582e+00,  1.0010e+00,  6.4414e+00,  ..., -2.1562e+00,\n",
      "          -1.2812e+00,  1.1617e+01],\n",
      "         [ 5.9277e-01,  7.1289e-01,  6.5898e+00,  ..., -2.5840e+00,\n",
      "          -1.8096e+00,  1.1289e+01]]], dtype=torch.float16, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(\"Inputs : \")\n",
    "print(\"pred_logits : \",outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_logits shape :  torch.Size([4, 250, 20])\n"
     ]
    }
   ],
   "source": [
    "print(\"pred_logits shape : \",outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 250])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_classes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 250]),\n",
       " tensor([[19, 19, 19, 19, 19, 19, 19, 19, 11, 19, 19, 19, 19, 13, 13, 19, 19, 19,\n",
       "          13, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 18, 19, 19, 19, 19, 19,\n",
       "          13, 19, 19, 19,  5, 19, 19, 19, 19, 19, 19, 11, 19, 19, 19, 19, 19, 13,\n",
       "          19, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 13, 19, 19, 19, 13, 19,\n",
       "          19, 19, 19, 19, 19, 13, 19, 19, 18, 13, 19, 19, 19, 19, 19, 10, 19, 19,\n",
       "           8, 19, 19, 19, 19, 19, 13, 19, 19, 19, 19, 19, 19, 19, 19, 13, 19, 19,\n",
       "          19, 19, 19, 13, 19, 19, 19,  7, 19, 19, 19, 19, 19, 19, 11, 19, 19, 19,\n",
       "          19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 13, 19, 19, 19, 19, 19,\n",
       "          19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 13, 11, 19, 19, 19, 19, 19,\n",
       "          19, 19, 19, 19, 13, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "          19, 13, 13, 19, 19, 13, 13, 19, 19, 19, 19,  0, 19, 19, 19, 19, 19, 12,\n",
       "          19, 19, 19, 11, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "          19, 19, 19, 19,  2, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 11, 19, 19,\n",
       "          19, 19,  1, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19],\n",
       "         [19,  1, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "          19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 13, 19, 19,\n",
       "          19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "           7, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 13, 19, 19, 19, 13, 19,\n",
       "          19, 19, 19, 19, 19, 13, 19, 19, 19, 19, 19, 19, 19, 19, 19, 10, 19,  4,\n",
       "           8, 19, 19, 13, 19,  5, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "          19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "          19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "          19, 19, 19, 19,  3, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "          19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "          19, 19, 13, 19, 19, 19, 19, 19, 19, 19, 19,  0, 19, 19, 19, 19, 19, 19,\n",
       "          19, 19, 19, 11, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "          19, 19, 19, 19,  2, 19, 19, 19, 19, 13, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "          19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 11, 19, 19, 19, 19],\n",
       "         [19,  1, 11, 19, 19, 11, 11, 11, 11, 11, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "          19, 19, 11, 19, 19, 19, 19, 11, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "          19, 19, 19, 19, 19, 19, 19, 11, 11, 11, 19, 19, 19, 11, 19, 19, 19, 19,\n",
       "          19, 11, 19, 19, 11, 19, 19, 19, 19,  3, 11, 19, 11, 19, 19, 19, 19, 19,\n",
       "           9, 19, 19, 12, 19, 19, 19, 19, 19, 19, 11, 11, 19, 19, 19, 10, 11, 19,\n",
       "           8, 19, 19, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "          19, 19, 19, 19, 19, 11, 19, 19, 13, 19, 19, 19, 19, 19, 19, 11, 19, 11,\n",
       "          19, 19, 19,  7, 11, 11, 19, 19, 19, 19, 11,  5, 19, 19, 19, 19, 19, 19,\n",
       "          19, 19, 19, 19, 19, 11, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 11,\n",
       "          19, 19, 19, 19, 19, 19, 19, 19, 11, 19, 11, 19, 11, 19, 19, 19, 19, 19,\n",
       "          19, 19, 19, 19,  4, 11, 11, 19, 19, 19, 19,  0, 19, 19, 19, 19, 19, 11,\n",
       "          19, 19, 19, 11, 19, 19, 19, 11, 11, 19, 11, 19, 19, 19, 19, 19, 19, 19,\n",
       "          19, 19, 19, 19,  2, 19, 19, 19, 19, 19, 11, 19, 11, 19, 19, 19, 19,  6,\n",
       "          19, 11, 19, 19, 19, 19, 19, 19, 19, 11, 19, 11, 19, 19, 19, 19],\n",
       "         [19,  1, 19, 19, 19, 11, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "          13, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "          19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "          19, 13, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 11, 19,\n",
       "           9, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 10, 19,  4,\n",
       "           8, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "          19, 19, 19, 19, 13, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,  7,\n",
       "          19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "          19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "          19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "          19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,  0, 19, 19, 19, 19, 19, 19,\n",
       "          19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "          19, 19, 19, 19, 19, 19, 19, 19, 19, 11, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "          19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,  5, 19, 19, 19]]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_classes.shape,target_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element  0\n",
      "len(eleme['labels']) : 36\n",
      "len(eleme[\"masks\"]) : 36\n",
      "{'labels': tensor([ 0,  1,  2,  5,  7,  8, 10, 11, 11, 11, 11, 11, 11, 12, 13, 13, 13, 13,\n",
      "        13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 18, 18, 18]), 'masks': tensor([[[False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         ...,\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False]],\n",
      "\n",
      "        [[False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         ...,\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False]],\n",
      "\n",
      "        [[False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         ...,\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         ...,\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False]],\n",
      "\n",
      "        [[False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         ...,\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False]],\n",
      "\n",
      "        [[False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         ...,\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False]]])}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#print(\"targets  : \",targets)\n",
    "\n",
    "\n",
    "for i in  range(len(targets)):\n",
    "    print(\"Element \", i)\n",
    "    eleme=targets[i]\n",
    "    print(\"len(eleme['labels']) :\",len(eleme['labels']))\n",
    "    print('len(eleme[\"masks\"]) :',len(eleme[\"masks\"]))\n",
    "    print(eleme)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_float=outputs.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_loss=out_float.transpose(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 20, 250])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_loss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(273.5000, dtype=torch.float16, grad_fn=<AddBackward0>),\n",
       " tensor(402.5000, dtype=torch.float16, grad_fn=<AddBackward0>),\n",
       " tensor(377., dtype=torch.float16, grad_fn=<AddBackward0>),\n",
       " tensor(390.5000, dtype=torch.float16, grad_fn=<AddBackward0>),\n",
       " tensor(224.5000, dtype=torch.float16, grad_fn=<AddBackward0>),\n",
       " tensor(92.8750, dtype=torch.float16, grad_fn=<AddBackward0>),\n",
       " tensor(380.2500, dtype=torch.float16, grad_fn=<AddBackward0>),\n",
       " tensor(67.8125, dtype=torch.float16, grad_fn=<AddBackward0>),\n",
       " tensor(215.2500, dtype=torch.float16, grad_fn=<AddBackward0>),\n",
       " tensor(72.9375, dtype=torch.float16, grad_fn=<AddBackward0>),\n",
       " tensor(47.3125, dtype=torch.float16, grad_fn=<AddBackward0>),\n",
       " tensor(383., dtype=torch.float16, grad_fn=<AddBackward0>),\n",
       " tensor(239.8750, dtype=torch.float16, grad_fn=<AddBackward0>),\n",
       " tensor(1025., dtype=torch.float16, grad_fn=<AddBackward0>),\n",
       " tensor(562.5000, dtype=torch.float16, grad_fn=<AddBackward0>),\n",
       " tensor(304.5000, dtype=torch.float16, grad_fn=<AddBackward0>),\n",
       " tensor(-50.4375, dtype=torch.float16, grad_fn=<AddBackward0>),\n",
       " tensor(362.7500, dtype=torch.float16, grad_fn=<AddBackward0>),\n",
       " tensor(383., dtype=torch.float16, grad_fn=<AddBackward0>),\n",
       " tensor(2494., dtype=torch.float16, grad_fn=<AddBackward0>)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[sum(k) for k in outputs[0].T]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 250]), torch.Size([4, 250, 20]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_classes.shape, outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([19, 19, 19, 19, 19, 19, 19, 19, 11, 19, 19, 19, 19, 13, 13, 19, 19, 19,\n",
       "        13, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 18, 19, 19, 19, 19, 19,\n",
       "        13, 19, 19, 19,  5, 19, 19, 19, 19, 19, 19, 11, 19, 19, 19, 19, 19, 13,\n",
       "        19, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 13, 19, 19, 19, 13, 19,\n",
       "        19, 19, 19, 19, 19, 13, 19, 19, 18, 13, 19, 19, 19, 19, 19, 10, 19, 19,\n",
       "         8, 19, 19, 19, 19, 19, 13, 19, 19, 19, 19, 19, 19, 19, 19, 13, 19, 19,\n",
       "        19, 19, 19, 13, 19, 19, 19,  7, 19, 19, 19, 19, 19, 19, 11, 19, 19, 19,\n",
       "        19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 13, 19, 19, 19, 19, 19,\n",
       "        19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 13, 11, 19, 19, 19, 19, 19,\n",
       "        19, 19, 19, 19, 13, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "        19, 13, 13, 19, 19, 13, 13, 19, 19, 19, 19,  0, 19, 19, 19, 19, 19, 12,\n",
       "        19, 19, 19, 11, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "        19, 19, 19, 19,  2, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 11, 19, 19,\n",
       "        19, 19,  1, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_classes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3586,  3.8945,  1.0234,  2.1309,  1.2266, -0.2834,  0.0754, -0.3267,\n",
       "         0.6885,  1.9277,  2.0684,  0.5454,  3.1543,  0.2759,  2.0938, -0.6592,\n",
       "         8.0781,  0.5771,  1.6709,  1.0430,  1.8721,  0.1394, -1.4746,  1.5469,\n",
       "         6.2031,  1.6035,  4.2734,  0.1158, -0.3057,  1.4463,  1.7266,  1.1553,\n",
       "         1.3643,  1.1328,  1.1387,  0.1415,  1.4766,  1.5195,  0.3533,  1.6240,\n",
       "         0.7944,  1.8320,  1.1074, -0.5303,  0.3271,  1.8438,  2.0098, -0.7231,\n",
       "         2.1641,  2.3984,  2.0332,  1.6699,  1.4258,  1.8115, -0.3203,  2.0918,\n",
       "         1.3564,  2.9355,  1.6191,  3.2969,  0.2903,  1.5498,  1.9902, -0.8530,\n",
       "         1.7578,  1.7451,  2.1406, -1.0986,  0.0757,  0.8169,  1.7695,  2.0547,\n",
       "         1.9258,  1.1289,  2.4648,  0.4570, -0.0269,  1.6221,  1.1660,  0.8901,\n",
       "         0.8076,  1.6533,  1.1787,  2.1055, -0.5259, -0.7295,  2.3301,  0.3372,\n",
       "         1.1885, -0.2803,  0.8252,  1.6719,  0.0523, -0.5654, -0.1072,  1.8574,\n",
       "         1.6689,  3.9590, -0.8008,  1.1357,  0.0825,  1.2334, -1.6406,  1.3389,\n",
       "         2.6406,  0.4846,  2.5645, -1.1494,  1.7090,  0.8066,  1.4033, -1.1240,\n",
       "        -0.2170, -1.3867,  0.5107,  0.0294,  0.9463, -0.9526, -0.1909,  0.5400,\n",
       "         0.5186,  0.8105, -0.2651,  0.8662,  7.6992,  1.3662,  1.2207, -0.1392,\n",
       "         4.0273, -1.1699, -0.1771,  0.4043, -1.2148, -1.1309,  0.3628,  1.5488,\n",
       "         0.3076,  1.9648,  1.3555, -1.7559, -0.0911,  2.1211, -0.0945,  0.7095,\n",
       "        -0.7275, -0.0976,  2.5176,  1.7881,  1.6611,  1.5566,  2.4609,  1.2344,\n",
       "        -0.3948,  0.8354, -0.4873,  1.9482,  0.5693,  1.5205,  3.6309,  1.9609,\n",
       "         3.0430, -0.0908,  1.4180,  0.0715,  2.9434,  1.5293,  1.5537,  1.8740,\n",
       "         1.9043,  2.3848, -1.5127,  0.1031, -0.8257,  5.5547, -0.6436,  1.6299,\n",
       "        -1.8838,  2.0957, -1.6367, -1.5312, -1.4502,  1.6006,  1.3223,  0.9180,\n",
       "         9.1406,  2.0137,  1.6758,  0.5811, -1.6631,  2.4531,  1.8711, 11.4141,\n",
       "         2.6035,  0.4282,  0.2690,  1.1211,  0.5742,  1.5176,  2.8262,  1.8115,\n",
       "         0.4519,  0.3955, -1.2070,  0.2070,  1.2080,  0.9775,  0.5479, -1.5049,\n",
       "        -1.1406, -0.2007,  0.4492,  0.6870,  0.7568, -0.5127,  2.1680,  0.4629,\n",
       "         0.2065,  2.1191,  0.3394,  0.2188,  0.0269, -0.7090,  1.4775,  1.5088,\n",
       "         1.7207,  1.9189, -1.2568, -0.4043,  0.6865,  2.0684,  3.0547,  0.6909,\n",
       "         1.2275,  1.4365,  0.1199,  1.2480,  6.3008,  1.0957,  4.5391, -0.3101,\n",
       "         1.5996,  1.4902, -0.1909,  0.9204,  1.2637, -0.7358,  4.9492,  4.7461,\n",
       "        -1.6914, -2.0020], dtype=torch.float16, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].T[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_logits = outputs.float()\n",
    "idx = json_data['idx']\n",
    "target_classes_o = torch.cat([t[\"labels\"][J] for t, (_, J) in zip(targets, json_data['indices'])])\n",
    "target_classes = torch.full(\n",
    "            src_logits.shape[:2], json_data['num_classes'], dtype=torch.int64, device=src_logits.device)\n",
    "target_classes[idx] = target_classes_o\n",
    "\n",
    "loss_ce = F.cross_entropy(src_logits.transpose(1, 2), src_logits.transpose(1, 2), json_data['weight'])\n",
    "losses = {\"loss_ce\": loss_ce}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(99.3829, grad_fn=<DivBackward1>)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_260685/67097343.py:7: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  log_softmax = F.log_softmax(input)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m target \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39mlong()\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Gather the log softmax values corresponding to the target classes\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m log_probs \u001b[38;5;241m=\u001b[39m \u001b[43mlog_softmax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "input=src_logits.transpose(1, 2)[0].T[0]\n",
    "target=src_logits.transpose(1, 2)[0].T[0]\n",
    "weight= json_data['weight']\n",
    "\n",
    "\n",
    "# Apply log softmax to the input along the class dimension\n",
    "log_softmax = F.log_softmax(input)\n",
    "\n",
    "# Convert target to long type if not already\n",
    "target = target.long()\n",
    "\n",
    "# Gather the log softmax values corresponding to the target classes\n",
    "log_probs = log_softmax.gather(1, target.unsqueeze(1)).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Index tensor must have the same number of dimensions as input tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlog_softmax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Index tensor must have the same number of dimensions as input tensor"
     ]
    }
   ],
   "source": [
    "log_softmax.gather(0, target.unsqueeze(1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
